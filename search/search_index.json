{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Internals of Delta Lake {{ delta.version }} \u00b6 Welcome to The Internals of Delta Lake online book! I'm Jacek Laskowski , a Seasoned IT Professional specializing in Apache Spark , Delta Lake , Apache Kafka and Kafka Streams . I'm very excited to have you here and hope you will enjoy exploring the internals of Delta Lake as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let me introduce you to Delta Lake .","title":"Welcome"},{"location":"#the-internals-of-delta-lake-deltaversion","text":"Welcome to The Internals of Delta Lake online book! I'm Jacek Laskowski , a Seasoned IT Professional specializing in Apache Spark , Delta Lake , Apache Kafka and Kafka Streams . I'm very excited to have you here and hope you will enjoy exploring the internals of Delta Lake as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let me introduce you to Delta Lake .","title":"The Internals of Delta Lake {{ delta.version }}"},{"location":"Action/","text":"= Action Action is an < > of < > of a change to (the state of) a Delta table. Action can be converted ( serialized ) to < > format for...FIXME [[logSchema]] Action object defines logSchema that is a schema ( StructType ) based on the < > case class. [source, scala] \u00b6 import org.apache.spark.sql.delta.actions.Action.logSchema scala> logSchema.printTreeString root |-- txn: struct (nullable = true) | |-- appId: string (nullable = true) | |-- version: long (nullable = false) | |-- lastUpdated: long (nullable = true) |-- add: struct (nullable = true) | |-- path: string (nullable = true) | |-- partitionValues: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) | |-- size: long (nullable = false) | |-- modificationTime: long (nullable = false) | |-- dataChange: boolean (nullable = false) | |-- stats: string (nullable = true) | |-- tags: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) |-- remove: struct (nullable = true) | |-- path: string (nullable = true) | |-- deletionTimestamp: long (nullable = true) | |-- dataChange: boolean (nullable = false) |-- metaData: struct (nullable = true) | |-- id: string (nullable = true) | |-- name: string (nullable = true) | |-- description: string (nullable = true) | |-- format: struct (nullable = true) | | |-- provider: string (nullable = true) | | |-- options: map (nullable = true) | | | |-- key: string | | | |-- value: string (valueContainsNull = true) | |-- schemaString: string (nullable = true) | |-- partitionColumns: array (nullable = true) | | |-- element: string (containsNull = true) | |-- configuration: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) | |-- createdTime: long (nullable = true) |-- protocol: struct (nullable = true) | |-- minReaderVersion: integer (nullable = false) | |-- minWriterVersion: integer (nullable = false) |-- commitInfo: struct (nullable = true) | |-- version: long (nullable = true) | |-- timestamp: timestamp (nullable = true) | |-- userId: string (nullable = true) | |-- userName: string (nullable = true) | |-- operation: string (nullable = true) | |-- operationParameters: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) | |-- job: struct (nullable = true) | | |-- jobId: string (nullable = true) | | |-- jobName: string (nullable = true) | | |-- runId: string (nullable = true) | | |-- jobOwnerId: string (nullable = true) | | |-- triggerType: string (nullable = true) | |-- notebook: struct (nullable = true) | | |-- notebookId: string (nullable = true) | |-- clusterId: string (nullable = true) | |-- readVersion: long (nullable = true) | |-- isolationLevel: string (nullable = true) | |-- isBlindAppend: boolean (nullable = true) [[contract]] .Action Contract (Abstract Methods Only) [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | wrap a| [[wrap]] [source, scala] \u00b6 wrap: SingleAction \u00b6 Wraps the action into a < > for serialization Used when: Snapshot is created (and initializes the < > for the < > of a delta table) Action is requested to < > |=== [[implementations]] [[extensions]] .Actions (Direct Implementations and Extensions Only) [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Action | Description | < > | [[CommitInfo]] | < > | [[FileAction]] | < > | [[Metadata]] | < > | [[Protocol]] | < > | [[SetTransaction]] |=== NOTE: Action is a Scala sealed trait which means that all the < > are in the same compilation unit (a single file). == [[json]] Serializing to JSON Format -- json Method [source, scala] \u00b6 json: String \u00b6 json simply serializes ( converts ) the < > to JSON format. NOTE: json uses https://github.com/FasterXML/jackson[Jackson ] library (with https://github.com/FasterXML/jackson-module-scala[jackson-module-scala ]) as the JSON processor. [NOTE] \u00b6 json is used when: OptimisticTransactionImpl is requested to < > * ConvertToDeltaCommand is requested to < > \u00b6 == [[fromJson]] Deserializing Action (From JSON Format) -- fromJson Utility [source, scala] \u00b6 fromJson( json: String): Action fromJson ...FIXME [NOTE] \u00b6 fromJson is used when: DeltaHistoryManager utility is requested for the < > DeltaLog is requested for the < > * OptimisticTransactionImpl is requested to < > \u00b6","title":"Action"},{"location":"Action/#source-scala","text":"import org.apache.spark.sql.delta.actions.Action.logSchema scala> logSchema.printTreeString root |-- txn: struct (nullable = true) | |-- appId: string (nullable = true) | |-- version: long (nullable = false) | |-- lastUpdated: long (nullable = true) |-- add: struct (nullable = true) | |-- path: string (nullable = true) | |-- partitionValues: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) | |-- size: long (nullable = false) | |-- modificationTime: long (nullable = false) | |-- dataChange: boolean (nullable = false) | |-- stats: string (nullable = true) | |-- tags: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) |-- remove: struct (nullable = true) | |-- path: string (nullable = true) | |-- deletionTimestamp: long (nullable = true) | |-- dataChange: boolean (nullable = false) |-- metaData: struct (nullable = true) | |-- id: string (nullable = true) | |-- name: string (nullable = true) | |-- description: string (nullable = true) | |-- format: struct (nullable = true) | | |-- provider: string (nullable = true) | | |-- options: map (nullable = true) | | | |-- key: string | | | |-- value: string (valueContainsNull = true) | |-- schemaString: string (nullable = true) | |-- partitionColumns: array (nullable = true) | | |-- element: string (containsNull = true) | |-- configuration: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) | |-- createdTime: long (nullable = true) |-- protocol: struct (nullable = true) | |-- minReaderVersion: integer (nullable = false) | |-- minWriterVersion: integer (nullable = false) |-- commitInfo: struct (nullable = true) | |-- version: long (nullable = true) | |-- timestamp: timestamp (nullable = true) | |-- userId: string (nullable = true) | |-- userName: string (nullable = true) | |-- operation: string (nullable = true) | |-- operationParameters: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) | |-- job: struct (nullable = true) | | |-- jobId: string (nullable = true) | | |-- jobName: string (nullable = true) | | |-- runId: string (nullable = true) | | |-- jobOwnerId: string (nullable = true) | | |-- triggerType: string (nullable = true) | |-- notebook: struct (nullable = true) | | |-- notebookId: string (nullable = true) | |-- clusterId: string (nullable = true) | |-- readVersion: long (nullable = true) | |-- isolationLevel: string (nullable = true) | |-- isBlindAppend: boolean (nullable = true) [[contract]] .Action Contract (Abstract Methods Only) [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | wrap a| [[wrap]]","title":"[source, scala]"},{"location":"Action/#source-scala_1","text":"","title":"[source, scala]"},{"location":"Action/#wrap-singleaction","text":"Wraps the action into a < > for serialization Used when: Snapshot is created (and initializes the < > for the < > of a delta table) Action is requested to < > |=== [[implementations]] [[extensions]] .Actions (Direct Implementations and Extensions Only) [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Action | Description | < > | [[CommitInfo]] | < > | [[FileAction]] | < > | [[Metadata]] | < > | [[Protocol]] | < > | [[SetTransaction]] |=== NOTE: Action is a Scala sealed trait which means that all the < > are in the same compilation unit (a single file). == [[json]] Serializing to JSON Format -- json Method","title":"wrap: SingleAction"},{"location":"Action/#source-scala_2","text":"","title":"[source, scala]"},{"location":"Action/#json-string","text":"json simply serializes ( converts ) the < > to JSON format. NOTE: json uses https://github.com/FasterXML/jackson[Jackson ] library (with https://github.com/FasterXML/jackson-module-scala[jackson-module-scala ]) as the JSON processor.","title":"json: String"},{"location":"Action/#note","text":"json is used when: OptimisticTransactionImpl is requested to < >","title":"[NOTE]"},{"location":"Action/#converttodeltacommand-is-requested-to","text":"== [[fromJson]] Deserializing Action (From JSON Format) -- fromJson Utility","title":"* ConvertToDeltaCommand is requested to &lt;&gt;"},{"location":"Action/#source-scala_3","text":"fromJson( json: String): Action fromJson ...FIXME","title":"[source, scala]"},{"location":"Action/#note_1","text":"fromJson is used when: DeltaHistoryManager utility is requested for the < > DeltaLog is requested for the < >","title":"[NOTE]"},{"location":"Action/#optimistictransactionimpl-is-requested-to","text":"","title":"* OptimisticTransactionImpl is requested to &lt;&gt;"},{"location":"AddFile/","text":"= AddFile AddFile is a < > to denote a < > added to a < >. AddFile is < > when: < > is executed (for < >) DelayedCommitProtocol is requested to < > (for < >) == [[creating-instance]] Creating AddFile Instance AddFile takes the following to be created: [[path]] Path [[partitionValues]] Partition values ( Map[String, String] ) [[size]] Size (in bytes) [[modificationTime]] Modification time [[dataChange]] dataChange flag [[stats]] Stats (default: null ) [[tags]] Tags ( Map[String, String] ) (default: null ) == [[wrap]] wrap Method [source, scala] \u00b6 wrap: SingleAction \u00b6 NOTE: wrap is part of the < > contract to wrap the action into a < > for serialization. wrap simply creates a new < > with the add field set to this AddFile . == [[remove]] Creating RemoveFile Instance With Current Timestamp -- remove Method [source, scala] \u00b6 remove: RemoveFile \u00b6 remove < > with the current timestamp and dataChange flag enabled. [NOTE] \u00b6 remove is used when: < > is executed < > is executed (with Overwrite mode) * DeltaSink is requested to < > (with Complete mode) \u00b6 == [[removeWithTimestamp]] Creating RemoveFile Instance For Given Timestamp -- removeWithTimestamp Method [source, scala] \u00b6 removeWithTimestamp( timestamp: Long = System.currentTimeMillis(), dataChange: Boolean = true): RemoveFile removeWithTimestamp creates a < > action for the < >, and the given timestamp and dataChange flag. [NOTE] \u00b6 removeWithTimestamp is used when: AddFile is requested to < > * < > and < > are executed \u00b6","title":"AddFile"},{"location":"AddFile/#source-scala","text":"","title":"[source, scala]"},{"location":"AddFile/#wrap-singleaction","text":"NOTE: wrap is part of the < > contract to wrap the action into a < > for serialization. wrap simply creates a new < > with the add field set to this AddFile . == [[remove]] Creating RemoveFile Instance With Current Timestamp -- remove Method","title":"wrap: SingleAction"},{"location":"AddFile/#source-scala_1","text":"","title":"[source, scala]"},{"location":"AddFile/#remove-removefile","text":"remove < > with the current timestamp and dataChange flag enabled.","title":"remove: RemoveFile"},{"location":"AddFile/#note","text":"remove is used when: < > is executed < > is executed (with Overwrite mode)","title":"[NOTE]"},{"location":"AddFile/#deltasink-is-requested-to-with-complete-mode","text":"== [[removeWithTimestamp]] Creating RemoveFile Instance For Given Timestamp -- removeWithTimestamp Method","title":"* DeltaSink is requested to &lt;&gt; (with Complete mode)"},{"location":"AddFile/#source-scala_2","text":"removeWithTimestamp( timestamp: Long = System.currentTimeMillis(), dataChange: Boolean = true): RemoveFile removeWithTimestamp creates a < > action for the < >, and the given timestamp and dataChange flag.","title":"[source, scala]"},{"location":"AddFile/#note_1","text":"removeWithTimestamp is used when: AddFile is requested to < >","title":"[NOTE]"},{"location":"AddFile/#and-are-executed","text":"","title":"* &lt;&gt; and &lt;&gt; are executed"},{"location":"CachedDS/","text":"= CachedDS -- Cached Delta State CachedDS (of A s) is < > exclusively when < > is requested to < >. NOTE: CachedDS is an internal class of < > and has access to its internals. == [[creating-instance]] Creating CachedDS Instance CachedDS takes the following to be created: [[ds]] Dataset[A] [[name]] Name == [[getDS]] getDS Method [source, scala] \u00b6 getDS: Dataset[A] \u00b6 getDS ...FIXME [NOTE] \u00b6 getDS is used when: Snapshot is requested to < > * DeltaSourceSnapshot is requested to < > \u00b6 == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | dsCache a| [[dsCache]] ( Option[Dataset[A]] ) Used when...FIXME | rddCache a| [[rddCache]] ( Option[RDD[InternalRow]] ) Used when...FIXME |===","title":"CachedDS"},{"location":"CachedDS/#source-scala","text":"","title":"[source, scala]"},{"location":"CachedDS/#getds-dataseta","text":"getDS ...FIXME","title":"getDS: Dataset[A]"},{"location":"CachedDS/#note","text":"getDS is used when: Snapshot is requested to < >","title":"[NOTE]"},{"location":"CachedDS/#deltasourcesnapshot-is-requested-to","text":"== [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | dsCache a| [[dsCache]] ( Option[Dataset[A]] ) Used when...FIXME | rddCache a| [[rddCache]] ( Option[RDD[InternalRow]] ) Used when...FIXME |===","title":"* DeltaSourceSnapshot is requested to &lt;&gt;"},{"location":"Checkpoints/","text":"= Checkpoints Checkpoints is an < > of < > that can < > the current state of a delta table (represented by the < >). [[contract]] .Checkpoints Contract (Abstract Methods Only) [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | logPath a| [[logPath]] [source, scala] \u00b6 logPath: Path \u00b6 Used when...FIXME | dataPath a| [[dataPath]] [source, scala] \u00b6 dataPath: Path \u00b6 Used when...FIXME | snapshot a| [[snapshot]] [source, scala] \u00b6 snapshot: Snapshot \u00b6 Used when...FIXME | store a| [[store]] [source, scala] \u00b6 store: LogStore \u00b6 Used when...FIXME | metadata a| [[metadata]] [source, scala] \u00b6 metadata: Metadata \u00b6 < > of (the current state of) the < > Used when...FIXME | doLogCleanup a| [[doLogCleanup]] [source, scala] \u00b6 doLogCleanup(): Unit \u00b6 Used when...FIXME |=== [[implementations]][[self]] NOTE: < > is the default and only known Checkpoints in Delta Lake. == [[LAST_CHECKPOINT]][[_last_checkpoint]] _last_checkpoint Metadata File Checkpoints uses _last_checkpoint metadata file (under the < >) for the following: < > < > == [[checkpoint]] Checkpointing -- checkpoint Method [source, scala] \u00b6 checkpoint(): Unit checkpoint( snapshotToCheckpoint: Snapshot): CheckpointMetaData checkpoint ...FIXME NOTE: checkpoint is used when...FIXME == [[lastCheckpoint]] lastCheckpoint Method [source, scala] \u00b6 lastCheckpoint: Option[CheckpointMetaData] \u00b6 lastCheckpoint simply < > (with 0 tries ). NOTE: lastCheckpoint is used when...FIXME == [[manuallyLoadCheckpoint]] manuallyLoadCheckpoint Method [source, scala] \u00b6 manuallyLoadCheckpoint(cv: CheckpointInstance): CheckpointMetaData \u00b6 manuallyLoadCheckpoint ...FIXME NOTE: manuallyLoadCheckpoint is used when...FIXME == [[findLastCompleteCheckpoint]] findLastCompleteCheckpoint Method [source, scala] \u00b6 findLastCompleteCheckpoint(cv: CheckpointInstance): Option[CheckpointInstance] \u00b6 findLastCompleteCheckpoint ...FIXME NOTE: findLastCompleteCheckpoint is used when...FIXME == [[getLatestCompleteCheckpointFromList]] getLatestCompleteCheckpointFromList Method [source, scala] \u00b6 getLatestCompleteCheckpointFromList( instances: Array[CheckpointInstance], notLaterThan: CheckpointInstance): Option[CheckpointInstance] getLatestCompleteCheckpointFromList ...FIXME NOTE: getLatestCompleteCheckpointFromList is used when...FIXME == [[writeCheckpoint]] writeCheckpoint Utility [source, scala] \u00b6 writeCheckpoint( spark: SparkSession, deltaLog: DeltaLog, snapshot: Snapshot): CheckpointMetaData writeCheckpoint ...FIXME NOTE: writeCheckpoint is used when Checkpoints is requested to < >. == [[loadMetadataFromFile]] loadMetadataFromFile Internal Method [source, scala] \u00b6 loadMetadataFromFile(tries: Int): Option[CheckpointMetaData] \u00b6 loadMetadataFromFile ...FIXME NOTE: loadMetadataFromFile is used when...FIXME","title":"Checkpoints"},{"location":"Checkpoints/#source-scala","text":"","title":"[source, scala]"},{"location":"Checkpoints/#logpath-path","text":"Used when...FIXME | dataPath a| [[dataPath]]","title":"logPath: Path"},{"location":"Checkpoints/#source-scala_1","text":"","title":"[source, scala]"},{"location":"Checkpoints/#datapath-path","text":"Used when...FIXME | snapshot a| [[snapshot]]","title":"dataPath: Path"},{"location":"Checkpoints/#source-scala_2","text":"","title":"[source, scala]"},{"location":"Checkpoints/#snapshot-snapshot","text":"Used when...FIXME | store a| [[store]]","title":"snapshot: Snapshot"},{"location":"Checkpoints/#source-scala_3","text":"","title":"[source, scala]"},{"location":"Checkpoints/#store-logstore","text":"Used when...FIXME | metadata a| [[metadata]]","title":"store: LogStore"},{"location":"Checkpoints/#source-scala_4","text":"","title":"[source, scala]"},{"location":"Checkpoints/#metadata-metadata","text":"< > of (the current state of) the < > Used when...FIXME | doLogCleanup a| [[doLogCleanup]]","title":"metadata: Metadata"},{"location":"Checkpoints/#source-scala_5","text":"","title":"[source, scala]"},{"location":"Checkpoints/#dologcleanup-unit","text":"Used when...FIXME |=== [[implementations]][[self]] NOTE: < > is the default and only known Checkpoints in Delta Lake. == [[LAST_CHECKPOINT]][[_last_checkpoint]] _last_checkpoint Metadata File Checkpoints uses _last_checkpoint metadata file (under the < >) for the following: < > < > == [[checkpoint]] Checkpointing -- checkpoint Method","title":"doLogCleanup(): Unit"},{"location":"Checkpoints/#source-scala_6","text":"checkpoint(): Unit checkpoint( snapshotToCheckpoint: Snapshot): CheckpointMetaData checkpoint ...FIXME NOTE: checkpoint is used when...FIXME == [[lastCheckpoint]] lastCheckpoint Method","title":"[source, scala]"},{"location":"Checkpoints/#source-scala_7","text":"","title":"[source, scala]"},{"location":"Checkpoints/#lastcheckpoint-optioncheckpointmetadata","text":"lastCheckpoint simply < > (with 0 tries ). NOTE: lastCheckpoint is used when...FIXME == [[manuallyLoadCheckpoint]] manuallyLoadCheckpoint Method","title":"lastCheckpoint: Option[CheckpointMetaData]"},{"location":"Checkpoints/#source-scala_8","text":"","title":"[source, scala]"},{"location":"Checkpoints/#manuallyloadcheckpointcv-checkpointinstance-checkpointmetadata","text":"manuallyLoadCheckpoint ...FIXME NOTE: manuallyLoadCheckpoint is used when...FIXME == [[findLastCompleteCheckpoint]] findLastCompleteCheckpoint Method","title":"manuallyLoadCheckpoint(cv: CheckpointInstance): CheckpointMetaData"},{"location":"Checkpoints/#source-scala_9","text":"","title":"[source, scala]"},{"location":"Checkpoints/#findlastcompletecheckpointcv-checkpointinstance-optioncheckpointinstance","text":"findLastCompleteCheckpoint ...FIXME NOTE: findLastCompleteCheckpoint is used when...FIXME == [[getLatestCompleteCheckpointFromList]] getLatestCompleteCheckpointFromList Method","title":"findLastCompleteCheckpoint(cv: CheckpointInstance): Option[CheckpointInstance]"},{"location":"Checkpoints/#source-scala_10","text":"getLatestCompleteCheckpointFromList( instances: Array[CheckpointInstance], notLaterThan: CheckpointInstance): Option[CheckpointInstance] getLatestCompleteCheckpointFromList ...FIXME NOTE: getLatestCompleteCheckpointFromList is used when...FIXME == [[writeCheckpoint]] writeCheckpoint Utility","title":"[source, scala]"},{"location":"Checkpoints/#source-scala_11","text":"writeCheckpoint( spark: SparkSession, deltaLog: DeltaLog, snapshot: Snapshot): CheckpointMetaData writeCheckpoint ...FIXME NOTE: writeCheckpoint is used when Checkpoints is requested to < >. == [[loadMetadataFromFile]] loadMetadataFromFile Internal Method","title":"[source, scala]"},{"location":"Checkpoints/#source-scala_12","text":"","title":"[source, scala]"},{"location":"Checkpoints/#loadmetadatafromfiletries-int-optioncheckpointmetadata","text":"loadMetadataFromFile ...FIXME NOTE: loadMetadataFromFile is used when...FIXME","title":"loadMetadataFromFile(tries: Int): Option[CheckpointMetaData]"},{"location":"CommitInfo/","text":"= CommitInfo CommitInfo is an Action.md[] with the following: [[version]] Version [[timestamp]] Timestamp [[userId]] User ID [[userName]] User Name [[operation]] Operation.md#name[Name of the operation] [[operationParameters]] Operation.md#parameters[Parameters of the operation] [[job]] JobInfo [[notebook]] NotebookInfo [[clusterId]] Cluster ID [[readVersion]] Read Version [[isolationLevel]] Isolation Level [[isBlindAppend]] isBlindAppend flag (to indicate whether a commit has blindly appended without caring about existing files) [[operationMetrics]] Metrics of the operation [[userMetadata]] User metadata CommitInfo is created (using < > utility) when: OptimisticTransactionImpl is requested to OptimisticTransactionImpl.md#commit[commit] ConvertToDeltaCommand command is requested to ConvertToDeltaCommand.md#streamWrite[streamWrite] (when executed) CommitInfo is used in OptimisticTransactionImpl.md#commitInfo[OptimisticTransactionImpl] and CommitStats. CommitInfo is added ( logged ) to a Delta log only for DeltaSQLConf.md#commitInfo.enabled[spark.databricks.delta.commitInfo.enabled] configuration enabled. == [[apply]] apply Utility [source,scala] \u00b6 apply( time: Long, operation: String, operationParameters: Map[String, String], commandContext: Map[String, String], readVersion: Option[Long], isolationLevel: Option[String], isBlindAppend: Option[Boolean], operationMetrics: Option[Map[String, String]], userMetadata: Option[String]): CommitInfo apply creates a CommitInfo (for the given arguments and based on the given commandContext for the user ID, user name, job, notebook, cluster). NOTE: commandContext is always empty, but could be customized using ConvertToDeltaCommand.md#ConvertToDeltaCommandBase[ConvertToDeltaCommandBase]. apply is used when: OptimisticTransactionImpl is requested to OptimisticTransactionImpl.md#commit[commit] ConvertToDeltaCommand command is requested to ConvertToDeltaCommand.md#streamWrite[streamWrite] (when executed)","title":"CommitInfo"},{"location":"CommitInfo/#sourcescala","text":"apply( time: Long, operation: String, operationParameters: Map[String, String], commandContext: Map[String, String], readVersion: Option[Long], isolationLevel: Option[String], isBlindAppend: Option[Boolean], operationMetrics: Option[Map[String, String]], userMetadata: Option[String]): CommitInfo apply creates a CommitInfo (for the given arguments and based on the given commandContext for the user ID, user name, job, notebook, cluster). NOTE: commandContext is always empty, but could be customized using ConvertToDeltaCommand.md#ConvertToDeltaCommandBase[ConvertToDeltaCommandBase]. apply is used when: OptimisticTransactionImpl is requested to OptimisticTransactionImpl.md#commit[commit] ConvertToDeltaCommand command is requested to ConvertToDeltaCommand.md#streamWrite[streamWrite] (when executed)","title":"[source,scala]"},{"location":"DelayedCommitProtocol/","text":"DelayedCommitProtocol \u00b6 DelayedCommitProtocol is used to model a distributed write that is orchestrated by the Spark driver with the write itself happening on executors. DelayedCommitProtocol is a concrete FileCommitProtocol (Spark Core) to write out a result of a structured query to a < > and return a < >. FileCommitProtocol (Spark Core) allows to track a write job (with a write task per partition) and inform the driver when all the write tasks finished successfully (and were < >) to consider the write job < >. TaskCommitMessage (Spark Core) allows to \"transfer\" the files added (written out) on the executors to the driver for the < >. TIP: Read up on https://books.japila.pl/apache-spark-internals/apache-spark-internals/2.4.4/spark-internal-io-FileCommitProtocol.html[FileCommitProtocol ] in https://books.japila.pl/apache-spark-internals[The Internals Of Apache Spark] online book. DelayedCommitProtocol is < > exclusively when TransactionalWrite is requested for a < > to < > to the < >. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.delta.files.DelayedCommitProtocol logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.files.DelayedCommitProtocol=ALL Refer to Logging . \u00b6 == [[creating-instance]] Creating DelayedCommitProtocol Instance DelayedCommitProtocol takes the following to be created: [[jobId]] Job ID (seems always < >) [[path]] Directory (to write files to) [[randomPrefixLength]] Optional length of a random prefix (seems always < >) DelayedCommitProtocol initializes the < >. == [[setupTask]] setupTask Method [source, scala] \u00b6 setupTask( taskContext: TaskAttemptContext): Unit NOTE: setupTask is part of the FileCommitProtocol contract to set up a task for a writing job. setupTask simply initializes the < > internal registry to be empty. == [[newTaskTempFile]] newTaskTempFile Method [source, scala] \u00b6 newTaskTempFile( taskContext: TaskAttemptContext, dir: Option[String], ext: String): String NOTE: newTaskTempFile is part of the FileCommitProtocol contract to inform the committer to add a new file. newTaskTempFile < > for the given TaskAttemptContext and ext . newTaskTempFile tries to < > with the given dir or falls back to an empty partitionValues . NOTE: The given dir defines a partition directory if the streaming query (and hence the write) is partitioned. newTaskTempFile builds a path (based on the given randomPrefixLength and the dir , or uses the file name directly). NOTE: FIXME When would the optional dir and the < > be defined? newTaskTempFile adds the partition values and the relative path to the < > internal registry. In the end, newTaskTempFile returns the absolute path of the (relative) path in the < >. == [[commitTask]] Committing Task (After Successful Write) -- commitTask Method [source, scala] \u00b6 commitTask( taskContext: TaskAttemptContext): TaskCommitMessage NOTE: commitTask is part of the FileCommitProtocol contract to commit a task after the writes succeed. commitTask simply creates a TaskCommitMessage with an < > for every < > if there were any. Otherwise, the TaskCommitMessage is empty. NOTE: A file is added (to < > internal registry) when DelayedCommitProtocol is requested for a < >. == [[commitJob]] Committing Spark Job (After Successful Write) -- commitJob Method [source, scala] \u00b6 commitJob( jobContext: JobContext, taskCommits: Seq[TaskCommitMessage]): Unit NOTE: commitJob is part of the FileCommitProtocol contract to commit a job after the writes succeed. commitJob simply adds the < > (from the given taskCommits from every < >) to the < > internal registry. == [[parsePartitions]] parsePartitions Method [source, scala] \u00b6 parsePartitions( dir: String): Map[String, String] parsePartitions ...FIXME NOTE: parsePartitions is used exclusively when DelayedCommitProtocol is requested to < >. == [[setupJob]] setupJob Method [source, scala] \u00b6 setupJob( jobContext: JobContext): Unit NOTE: setupJob is part of the FileCommitProtocol contract to set up a Spark job. setupJob does nothing. == [[abortJob]] abortJob Method [source, scala] \u00b6 abortJob( jobContext: JobContext): Unit NOTE: abortJob is part of the FileCommitProtocol contract to abort a Spark job. abortJob does nothing. == [[getFileName]] getFileName Method [source, scala] \u00b6 getFileName( taskContext: TaskAttemptContext, ext: String): String getFileName takes the task ID from the given TaskAttemptContext (for the split part below). getFileName generates a random UUID (for the uuid part below). In the end, getFileName returns a file name of the format: part-[split]%05d-[uuid][ext] NOTE: getFileName is used exclusively when DelayedCommitProtocol is requested to < >. == [[addedFiles]] addedFiles Internal Registry [source, scala] \u00b6 addedFiles: ArrayBuffer[(Map[String, String], String)] \u00b6 addedFiles tracks the files < > (that runs on an executor). addedFiles is initialized (as an empty collection) in < >. NOTE: addedFiles is used when DelayedCommitProtocol is requested to < > (on an executor and create a TaskCommitMessage with the files added while a task was writing out a partition of a streaming query). == [[addedStatuses]] addedStatuses Internal Registry [source, scala] \u00b6 addedStatuses = new ArrayBuffer[AddFile] \u00b6 addedStatuses is the files that were added by < > (on executors) once all they finish successfully and the < > (on a driver). [NOTE] \u00b6 addedStatuses is used when: DelayedCommitProtocol is requested to < > (on a driver) * TransactionalWrite is requested to < > \u00b6","title":"DelayedCommitProtocol"},{"location":"DelayedCommitProtocol/#delayedcommitprotocol","text":"DelayedCommitProtocol is used to model a distributed write that is orchestrated by the Spark driver with the write itself happening on executors. DelayedCommitProtocol is a concrete FileCommitProtocol (Spark Core) to write out a result of a structured query to a < > and return a < >. FileCommitProtocol (Spark Core) allows to track a write job (with a write task per partition) and inform the driver when all the write tasks finished successfully (and were < >) to consider the write job < >. TaskCommitMessage (Spark Core) allows to \"transfer\" the files added (written out) on the executors to the driver for the < >. TIP: Read up on https://books.japila.pl/apache-spark-internals/apache-spark-internals/2.4.4/spark-internal-io-FileCommitProtocol.html[FileCommitProtocol ] in https://books.japila.pl/apache-spark-internals[The Internals Of Apache Spark] online book. DelayedCommitProtocol is < > exclusively when TransactionalWrite is requested for a < > to < > to the < >. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.delta.files.DelayedCommitProtocol logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.files.DelayedCommitProtocol=ALL","title":"DelayedCommitProtocol"},{"location":"DelayedCommitProtocol/#refer-to-logging","text":"== [[creating-instance]] Creating DelayedCommitProtocol Instance DelayedCommitProtocol takes the following to be created: [[jobId]] Job ID (seems always < >) [[path]] Directory (to write files to) [[randomPrefixLength]] Optional length of a random prefix (seems always < >) DelayedCommitProtocol initializes the < >. == [[setupTask]] setupTask Method","title":"Refer to Logging."},{"location":"DelayedCommitProtocol/#source-scala","text":"setupTask( taskContext: TaskAttemptContext): Unit NOTE: setupTask is part of the FileCommitProtocol contract to set up a task for a writing job. setupTask simply initializes the < > internal registry to be empty. == [[newTaskTempFile]] newTaskTempFile Method","title":"[source, scala]"},{"location":"DelayedCommitProtocol/#source-scala_1","text":"newTaskTempFile( taskContext: TaskAttemptContext, dir: Option[String], ext: String): String NOTE: newTaskTempFile is part of the FileCommitProtocol contract to inform the committer to add a new file. newTaskTempFile < > for the given TaskAttemptContext and ext . newTaskTempFile tries to < > with the given dir or falls back to an empty partitionValues . NOTE: The given dir defines a partition directory if the streaming query (and hence the write) is partitioned. newTaskTempFile builds a path (based on the given randomPrefixLength and the dir , or uses the file name directly). NOTE: FIXME When would the optional dir and the < > be defined? newTaskTempFile adds the partition values and the relative path to the < > internal registry. In the end, newTaskTempFile returns the absolute path of the (relative) path in the < >. == [[commitTask]] Committing Task (After Successful Write) -- commitTask Method","title":"[source, scala]"},{"location":"DelayedCommitProtocol/#source-scala_2","text":"commitTask( taskContext: TaskAttemptContext): TaskCommitMessage NOTE: commitTask is part of the FileCommitProtocol contract to commit a task after the writes succeed. commitTask simply creates a TaskCommitMessage with an < > for every < > if there were any. Otherwise, the TaskCommitMessage is empty. NOTE: A file is added (to < > internal registry) when DelayedCommitProtocol is requested for a < >. == [[commitJob]] Committing Spark Job (After Successful Write) -- commitJob Method","title":"[source, scala]"},{"location":"DelayedCommitProtocol/#source-scala_3","text":"commitJob( jobContext: JobContext, taskCommits: Seq[TaskCommitMessage]): Unit NOTE: commitJob is part of the FileCommitProtocol contract to commit a job after the writes succeed. commitJob simply adds the < > (from the given taskCommits from every < >) to the < > internal registry. == [[parsePartitions]] parsePartitions Method","title":"[source, scala]"},{"location":"DelayedCommitProtocol/#source-scala_4","text":"parsePartitions( dir: String): Map[String, String] parsePartitions ...FIXME NOTE: parsePartitions is used exclusively when DelayedCommitProtocol is requested to < >. == [[setupJob]] setupJob Method","title":"[source, scala]"},{"location":"DelayedCommitProtocol/#source-scala_5","text":"setupJob( jobContext: JobContext): Unit NOTE: setupJob is part of the FileCommitProtocol contract to set up a Spark job. setupJob does nothing. == [[abortJob]] abortJob Method","title":"[source, scala]"},{"location":"DelayedCommitProtocol/#source-scala_6","text":"abortJob( jobContext: JobContext): Unit NOTE: abortJob is part of the FileCommitProtocol contract to abort a Spark job. abortJob does nothing. == [[getFileName]] getFileName Method","title":"[source, scala]"},{"location":"DelayedCommitProtocol/#source-scala_7","text":"getFileName( taskContext: TaskAttemptContext, ext: String): String getFileName takes the task ID from the given TaskAttemptContext (for the split part below). getFileName generates a random UUID (for the uuid part below). In the end, getFileName returns a file name of the format: part-[split]%05d-[uuid][ext] NOTE: getFileName is used exclusively when DelayedCommitProtocol is requested to < >. == [[addedFiles]] addedFiles Internal Registry","title":"[source, scala]"},{"location":"DelayedCommitProtocol/#source-scala_8","text":"","title":"[source, scala]"},{"location":"DelayedCommitProtocol/#addedfiles-arraybuffermapstring-string-string","text":"addedFiles tracks the files < > (that runs on an executor). addedFiles is initialized (as an empty collection) in < >. NOTE: addedFiles is used when DelayedCommitProtocol is requested to < > (on an executor and create a TaskCommitMessage with the files added while a task was writing out a partition of a streaming query). == [[addedStatuses]] addedStatuses Internal Registry","title":"addedFiles: ArrayBuffer[(Map[String, String], String)]"},{"location":"DelayedCommitProtocol/#source-scala_9","text":"","title":"[source, scala]"},{"location":"DelayedCommitProtocol/#addedstatuses-new-arraybufferaddfile","text":"addedStatuses is the files that were added by < > (on executors) once all they finish successfully and the < > (on a driver).","title":"addedStatuses = new ArrayBuffer[AddFile]"},{"location":"DelayedCommitProtocol/#note","text":"addedStatuses is used when: DelayedCommitProtocol is requested to < > (on a driver)","title":"[NOTE]"},{"location":"DelayedCommitProtocol/#transactionalwrite-is-requested-to","text":"","title":"* TransactionalWrite is requested to &lt;&gt;"},{"location":"DeltaAnalysis/","text":"DeltaAnalysis Logical Resolution Rule \u00b6 DeltaAnalysis is a logical resolution rule (Spark SQL's Rule[LogicalPlan] ) for INSERT INTO and INSERT OVERWRITE SQL commands (and DeleteFromTable, UpdateTable, MergeIntoTable). == [[creating-instance]] Creating Instance DeltaAnalysis takes the following to be created: [[session]] SparkSession [[conf]] SQLConf DeltaAnalysis is created when DeltaSparkSessionExtension is requested to DeltaSparkSessionExtension.md#apply[register Delta SQL support]. == [[apply]] Executing Rule [source, scala] \u00b6 apply( plan: LogicalPlan): LogicalPlan apply...FIXME apply is part of the Rule (Spark SQL) abstraction.","title":"DeltaAnalysis"},{"location":"DeltaAnalysis/#deltaanalysis-logical-resolution-rule","text":"DeltaAnalysis is a logical resolution rule (Spark SQL's Rule[LogicalPlan] ) for INSERT INTO and INSERT OVERWRITE SQL commands (and DeleteFromTable, UpdateTable, MergeIntoTable). == [[creating-instance]] Creating Instance DeltaAnalysis takes the following to be created: [[session]] SparkSession [[conf]] SQLConf DeltaAnalysis is created when DeltaSparkSessionExtension is requested to DeltaSparkSessionExtension.md#apply[register Delta SQL support]. == [[apply]] Executing Rule","title":"DeltaAnalysis Logical Resolution Rule"},{"location":"DeltaAnalysis/#source-scala","text":"apply( plan: LogicalPlan): LogicalPlan apply...FIXME apply is part of the Rule (Spark SQL) abstraction.","title":"[source, scala]"},{"location":"DeltaCatalog/","text":"DeltaCatalog \u00b6 DeltaCatalog is an extension of Spark SQL (using DelegatingCatalogExtension and StagingTableCatalog ). DeltaCatalog is registered using spark.sql.catalog.spark_catalog configuration property (while creating a SparkSession in a Spark application). == [[alterTable]] Altering Table [source,scala] \u00b6 alterTable( ident: Identifier, changes: TableChange*): Table alterTable...FIXME alterTable is part of the TableCatalog (Spark SQL 3.0.0) abstraction. Creating Table \u00b6 createTable ( ident : Identifier , schema : StructType , partitions : Array [ Transform ], properties : util.Map [ String , String ]) : Table createTable ...FIXME createTable is part of the TableCatalog ( Spark SQL ) abstraction. == [[loadTable]] Loading Table [source,scala] \u00b6 loadTable( ident: Identifier): Table loadTable...FIXME loadTable is part of the TableCatalog (Spark SQL 3.0.0) abstraction. Creating Delta Table \u00b6 createDeltaTable ( ident : Identifier , schema : StructType , partitions : Array [ Transform ], properties : util.Map [ String , String ], sourceQuery : Option [ LogicalPlan ], operation : TableCreationModes.CreationMode ) : Table createDeltaTable ...FIXME createDeltaTable is used when: DeltaCatalog is requested to createTable StagedDeltaTableV2 is requested to commitStagedChanges","title":"DeltaCatalog"},{"location":"DeltaCatalog/#deltacatalog","text":"DeltaCatalog is an extension of Spark SQL (using DelegatingCatalogExtension and StagingTableCatalog ). DeltaCatalog is registered using spark.sql.catalog.spark_catalog configuration property (while creating a SparkSession in a Spark application). == [[alterTable]] Altering Table","title":"DeltaCatalog"},{"location":"DeltaCatalog/#sourcescala","text":"alterTable( ident: Identifier, changes: TableChange*): Table alterTable...FIXME alterTable is part of the TableCatalog (Spark SQL 3.0.0) abstraction.","title":"[source,scala]"},{"location":"DeltaCatalog/#creating-table","text":"createTable ( ident : Identifier , schema : StructType , partitions : Array [ Transform ], properties : util.Map [ String , String ]) : Table createTable ...FIXME createTable is part of the TableCatalog ( Spark SQL ) abstraction. == [[loadTable]] Loading Table","title":" Creating Table"},{"location":"DeltaCatalog/#sourcescala_1","text":"loadTable( ident: Identifier): Table loadTable...FIXME loadTable is part of the TableCatalog (Spark SQL 3.0.0) abstraction.","title":"[source,scala]"},{"location":"DeltaCatalog/#creating-delta-table","text":"createDeltaTable ( ident : Identifier , schema : StructType , partitions : Array [ Transform ], properties : util.Map [ String , String ], sourceQuery : Option [ LogicalPlan ], operation : TableCreationModes.CreationMode ) : Table createDeltaTable ...FIXME createDeltaTable is used when: DeltaCatalog is requested to createTable StagedDeltaTableV2 is requested to commitStagedChanges","title":" Creating Delta Table"},{"location":"DeltaConfig/","text":"= DeltaConfig -- Configuration Property Of Delta Table (Metadata) [[T]] DeltaConfig (of type T ) represents a < > of a delta table with values (of type T ) in an < >. DeltaConfig can be < >. == [[creating-instance]] Creating DeltaConfig Instance DeltaConfig takes the following to be created: [[key]] Key [[defaultValue]] Default value [[fromString]] Conversion function (from text representation of the DeltaConfig to the < >, i.e. String => T ) [[validationFunction]] Validation function (that guards from incorrect values, i.e. T => Boolean ) [[helpMessage]] Help message [[minimumProtocolVersion]] (optional) Minimum version of < > supported DeltaConfig initializes the < >. == [[fromMetaData]] Reading Configuration Property From Metadata -- fromMetaData Method [source, scala] \u00b6 fromMetaData( metadata: Metadata): T fromMetaData looks up the < > in the < > of the given < >. If not found, fromMetaData gives the < >. In the end, fromMetaData converts the text representation to the proper type using < > conversion function. [NOTE] \u00b6 fromMetaData is used when: DeltaLog is requested for < > and < > table properties, and to < > MetadataCleanup is requested for the < > and the < > * Snapshot is requested for the < > \u00b6","title":"DeltaConfig"},{"location":"DeltaConfig/#source-scala","text":"fromMetaData( metadata: Metadata): T fromMetaData looks up the < > in the < > of the given < >. If not found, fromMetaData gives the < >. In the end, fromMetaData converts the text representation to the proper type using < > conversion function.","title":"[source, scala]"},{"location":"DeltaConfig/#note","text":"fromMetaData is used when: DeltaLog is requested for < > and < > table properties, and to < > MetadataCleanup is requested for the < > and the < >","title":"[NOTE]"},{"location":"DeltaConfig/#snapshot-is-requested-for-the","text":"","title":"* Snapshot is requested for the &lt;&gt;"},{"location":"DeltaConfigs/","text":"= DeltaConfigs DeltaConfigs defines < > (aka table properties ). Table properties can be assigned a value using ALTER TABLE SQL command: ALTER TABLE <table_name> SET TBLPROPERTIES (<key>=<value>) [[sqlConfPrefix]][[spark.databricks.delta.properties.defaults]] DeltaConfigs uses spark.databricks.delta.properties.defaults prefix for < >. [[configuration-properties]] .Reservoir Configuration Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Key | Description | appendOnly a| [[appendOnly]][[IS_APPEND_ONLY]] Whether a delta table is append-only ( true ) or not ( false ). When enabled, a table allows appends only and no updates or deletes. Default: false | autoOptimize a| [[autoOptimize]][[AUTO_OPTIMIZE]] Whether this delta table will automagically optimize the layout of files during writes. Default: false | checkpointInterval a| [[checkpointInterval]][[CHECKPOINT_INTERVAL]] How often to checkpoint the state of a delta table Default: 10 | checkpointRetentionDuration a| [[checkpointRetentionDuration]][[CHECKPOINT_RETENTION_DURATION]] How long to keep checkpoint files around before deleting them Default: interval 2 days The most recent checkpoint is never deleted. It is acceptable to keep checkpoint files beyond this duration until the next calendar day. | compatibility.symlinkFormatManifest.enabled a| [[compatibility.symlinkFormatManifest]][[SYMLINK_FORMAT_MANIFEST_ENABLED]] Whether to register the < > post-commit hook while < > or not Default: false | dataSkippingNumIndexedCols a| [[dataSkippingNumIndexedCols]][[DATA_SKIPPING_NUM_INDEXED_COLS]] The number of columns to collect stats on for data skipping. -1 means collecting stats for all columns. Default: 32 | deletedFileRetentionDuration a| [[deletedFileRetentionDuration]][[TOMBSTONE_RETENTION]] How long to keep logically deleted data files around before deleting them physically (to prevent failures in stale readers after compactions or partition overwrites) Default: interval 1 week | enableExpiredLogCleanup a| [[enableExpiredLogCleanup]][[ENABLE_EXPIRED_LOG_CLEANUP]] Whether to clean up expired log files and checkpoints Default: true | enableFullRetentionRollback a| [[enableFullRetentionRollback]][[ENABLE_FULL_RETENTION_ROLLBACK]] When enabled (default), a delta table can be rolled back to any point within < >. When disabled, the table can be rolled back < > only. Default: true | logRetentionDuration a| [[logRetentionDuration]][[LOG_RETENTION]] How long to keep obsolete logs around before deleting them. Delta can keep logs beyond the duration until the next calendar day to avoid constantly creating checkpoints. Default: interval 30 days | randomizeFilePrefixes a| [[randomizeFilePrefixes]][[RANDOMIZE_FILE_PREFIXES]] Whether to use a random prefix in a file path instead of partition information (may be required for very high volume S3 calls to better be partitioned across S3 servers) Default: false | randomPrefixLength a| [[randomPrefixLength]][[RANDOM_PREFIX_LENGTH]] The length of the random prefix in a file path for < > Default: 2 | sampleRetentionDuration a| [[sampleRetentionDuration]][[SAMPLE_RETENTION]] How long to keep delta sample files around before deleting them Default: interval 7 days |=== == [[mergeGlobalConfigs]] mergeGlobalConfigs Utility [source, scala] \u00b6 mergeGlobalConfigs( sqlConfs: SQLConf, tableConf: Map[String, String], protocol: Protocol): Map[String, String] mergeGlobalConfigs finds all < >-prefixed configuration properties among the < >. NOTE: mergeGlobalConfigs is used when OptimisticTransactionImpl is requested for the OptimisticTransactionImpl.md#snapshotMetadata[metadata], to OptimisticTransactionImpl.md#updateMetadata[update the metadata], and OptimisticTransactionImpl.md#prepareCommit[prepare a commit] (for new delta tables). == [[verifyProtocolVersionRequirements]] verifyProtocolVersionRequirements Utility [source, scala] \u00b6 verifyProtocolVersionRequirements( configurations: Map[String, String], current: Protocol): Unit verifyProtocolVersionRequirements...FIXME verifyProtocolVersionRequirements is used when...FIXME == [[buildConfig]] Creating DeltaConfig Instance -- buildConfig Internal Utility [source, scala] \u00b6 buildConfig T : DeltaConfig[T] buildConfig creates a DeltaConfig.md[DeltaConfig] for the given key (with delta prefix) and adds it to the < > internal registry. NOTE: buildConfig is used to define all of the < >. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | entries a| [[entries]] [source, scala] \u00b6 HashMap[String, DeltaConfig[_]] \u00b6 |===","title":"DeltaConfigs"},{"location":"DeltaConfigs/#source-scala","text":"mergeGlobalConfigs( sqlConfs: SQLConf, tableConf: Map[String, String], protocol: Protocol): Map[String, String] mergeGlobalConfigs finds all < >-prefixed configuration properties among the < >. NOTE: mergeGlobalConfigs is used when OptimisticTransactionImpl is requested for the OptimisticTransactionImpl.md#snapshotMetadata[metadata], to OptimisticTransactionImpl.md#updateMetadata[update the metadata], and OptimisticTransactionImpl.md#prepareCommit[prepare a commit] (for new delta tables). == [[verifyProtocolVersionRequirements]] verifyProtocolVersionRequirements Utility","title":"[source, scala]"},{"location":"DeltaConfigs/#source-scala_1","text":"verifyProtocolVersionRequirements( configurations: Map[String, String], current: Protocol): Unit verifyProtocolVersionRequirements...FIXME verifyProtocolVersionRequirements is used when...FIXME == [[buildConfig]] Creating DeltaConfig Instance -- buildConfig Internal Utility","title":"[source, scala]"},{"location":"DeltaConfigs/#source-scala_2","text":"buildConfig T : DeltaConfig[T] buildConfig creates a DeltaConfig.md[DeltaConfig] for the given key (with delta prefix) and adds it to the < > internal registry. NOTE: buildConfig is used to define all of the < >. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | entries a| [[entries]]","title":"[source, scala]"},{"location":"DeltaConfigs/#source-scala_3","text":"","title":"[source, scala]"},{"location":"DeltaConfigs/#hashmapstring-deltaconfig_","text":"|===","title":"HashMap[String, DeltaConfig[_]]"},{"location":"DeltaConvert/","text":"DeltaConvert Utility \u00b6 DeltaConvert utility is used exclusively for < >. DeltaConvert utility can be used directly or indirectly via < > utility. import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) // CONVERT TO DELTA only supports parquet tables // TableIdentifier should be parquet.`users` import org.apache.spark.sql.catalyst.TableIdentifier val table = TableIdentifier(table = \"users\", database = Some(\"parquet\")) import org.apache.spark.sql.types.{StringType, StructField, StructType} val partitionSchema: Option[StructType] = Some( new StructType().add(StructField(\"country\", StringType))) val deltaPath: Option[String] = None // Use web UI to monitor execution, e.g. http://localhost:4040 import io.delta.tables.execution.DeltaConvert DeltaConvert.executeConvert(spark, table, partitionSchema, deltaPath) DeltaConvert utility is a concrete DeltaConvertBase . Importing Parquet Table Into Delta Lake (Converting Parquet Table To Delta Format) \u00b6 executeConvert ( spark : SparkSession , tableIdentifier : TableIdentifier , partitionSchema : Option [ StructType ], deltaPath : Option [ String ]) : DeltaTable executeConvert creates a new ConvertToDeltaCommand and executes it. In the end, executeConvert creates a DeltaTable . Note executeConvert can convert a Spark table (to Delta) that is registered in a metastore. executeConvert is used when DeltaTable utility is requested to convert a parquet table to delta format (DeltaTable.convertToDelta) .","title":"DeltaConvert"},{"location":"DeltaConvert/#deltaconvert-utility","text":"DeltaConvert utility is used exclusively for < >. DeltaConvert utility can be used directly or indirectly via < > utility. import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) // CONVERT TO DELTA only supports parquet tables // TableIdentifier should be parquet.`users` import org.apache.spark.sql.catalyst.TableIdentifier val table = TableIdentifier(table = \"users\", database = Some(\"parquet\")) import org.apache.spark.sql.types.{StringType, StructField, StructType} val partitionSchema: Option[StructType] = Some( new StructType().add(StructField(\"country\", StringType))) val deltaPath: Option[String] = None // Use web UI to monitor execution, e.g. http://localhost:4040 import io.delta.tables.execution.DeltaConvert DeltaConvert.executeConvert(spark, table, partitionSchema, deltaPath) DeltaConvert utility is a concrete DeltaConvertBase .","title":"DeltaConvert Utility"},{"location":"DeltaConvert/#importing-parquet-table-into-delta-lake-converting-parquet-table-to-delta-format","text":"executeConvert ( spark : SparkSession , tableIdentifier : TableIdentifier , partitionSchema : Option [ StructType ], deltaPath : Option [ String ]) : DeltaTable executeConvert creates a new ConvertToDeltaCommand and executes it. In the end, executeConvert creates a DeltaTable . Note executeConvert can convert a Spark table (to Delta) that is registered in a metastore. executeConvert is used when DeltaTable utility is requested to convert a parquet table to delta format (DeltaTable.convertToDelta) .","title":" Importing Parquet Table Into Delta Lake (Converting Parquet Table To Delta Format)"},{"location":"DeltaDataSource/","text":"DeltaDataSource \u00b6 DeltaDataSource is a < > and acts as the entry point to all features provided by delta data source. DeltaDataSource is a < >. DeltaDataSource is a < > for a streaming sink for streaming queries (Structured Streaming). == [[delta-format]][[DataSourceRegister]] DataSourceRegister for delta alias DeltaDataSource is a DataSourceRegister and registers itself to be available using delta alias. .Reading From Delta Table [source, scala] assert(spark.isInstanceOf[org.apache.spark.sql.SparkSession]) spark.read.format(\"delta\") spark.readStream.format(\"delta\") .Writing To Delta Table [source, scala] assert(df.isInstanceOf[org.apache.spark.sql.Dataset[_]]) df.write.format(\"delta\") df.writeStream.format(\"delta\") TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-DataSourceRegister.html[DataSourceRegister ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. DeltaDataSource is registered using META-INF/services/org.apache.spark.sql.sources.DataSourceRegister : [source, scala] \u00b6 org.apache.spark.sql.delta.sources.DeltaDataSource \u00b6 RelationProvider - Creating Insertable HadoopFsRelation For Batch Queries \u00b6 DeltaDataSource is a RelationProvider for reading ( loading ) data from a delta table in a structured query. Tip Read up on RelationProvider in The Internals of Spark SQL online book. createRelation ( sqlContext : SQLContext , parameters : Map [ String , String ]) : BaseRelation createRelation ...FIXME In the end, createRelation requests the DeltaLog for an insertable HadoopFsRelation . == [[CreatableRelationProvider]][[CreatableRelationProvider-createRelation]] CreatableRelationProvider DeltaDataSource is a CreatableRelationProvider for writing out the result of a structured query. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-CreatableRelationProvider.html[CreatableRelationProvider ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. == [[StreamSourceProvider]][[createSource]] Creating Streaming Source (Structured Streaming) -- createSource Method DeltaDataSource is a StreamSourceProvider . TIP: Read up on https://jaceklaskowski.gitbooks.io/spark-structured-streaming/spark-sql-streaming-StreamSourceProvider.html[StreamSourceProvider ] in https://bit.ly/spark-structured-streaming[The Internals of Spark Structured Streaming] online book. == [[StreamSinkProvider]][[createSink]] Creating Streaming Sink (Structured Streaming) -- createSink Method DeltaDataSource is a StreamSinkProvider for a streaming sink for Structured Streaming. TIP: Read up on https://jaceklaskowski.gitbooks.io/spark-structured-streaming/spark-sql-streaming-StreamSinkProvider.html[StreamSinkProvider ] in https://bit.ly/spark-structured-streaming[The Internals of Spark Structured Streaming] online book. DeltaDataSource supports Append and Complete output modes only. In the end, DeltaDataSource creates a < >. TIP: Consult the demo < >. == [[sourceSchema]] sourceSchema Method [source, scala] \u00b6 sourceSchema( sqlContext: SQLContext, schema: Option[StructType], providerName: String, parameters: Map[String, String]): (String, StructType) NOTE: sourceSchema is part of the StreamSourceProvider contract (Spark Structured Streaming) for the name and schema of the streaming source. sourceSchema ...FIXME == [[getTimeTravelVersion]] getTimeTravelVersion Internal Method [source, scala] \u00b6 getTimeTravelVersion( parameters: Map[String, String]): Option[DeltaTimeTravelSpec] getTimeTravelVersion ...FIXME NOTE: getTimeTravelVersion is used exclusively when DeltaDataSource is requested to < >. == [[extractDeltaPath]] extractDeltaPath Utility [source, scala] \u00b6 extractDeltaPath( dataset: Dataset[_]): Option[String] extractDeltaPath ...FIXME NOTE: extractDeltaPath does not seem to be used whatsoever. == [[parsePathIdentifier]] parsePathIdentifier Utility [source, scala] \u00b6 parsePathIdentifier( spark: SparkSession, userPath: String): (Path, Seq[(String, String)], Option[DeltaTimeTravelSpec]) parsePathIdentifier...FIXME parsePathIdentifier is used when DeltaTableV2 is requested for the DeltaTableV2.md#rootPath[rootPath, partitionFilters, and timeTravelByPath] (for a non-catalog table). == [[getTable]] Loading Table [source,scala] \u00b6 getTable( schema: StructType, partitioning: Array[Transform], properties: java.util.Map[String, String]): Table getTable...FIXME getTable is part of the TableProvider (Spark SQL 3.0.0) abstraction.","title":"DeltaDataSource"},{"location":"DeltaDataSource/#deltadatasource","text":"DeltaDataSource is a < > and acts as the entry point to all features provided by delta data source. DeltaDataSource is a < >. DeltaDataSource is a < > for a streaming sink for streaming queries (Structured Streaming). == [[delta-format]][[DataSourceRegister]] DataSourceRegister for delta alias DeltaDataSource is a DataSourceRegister and registers itself to be available using delta alias. .Reading From Delta Table [source, scala] assert(spark.isInstanceOf[org.apache.spark.sql.SparkSession]) spark.read.format(\"delta\") spark.readStream.format(\"delta\") .Writing To Delta Table [source, scala] assert(df.isInstanceOf[org.apache.spark.sql.Dataset[_]]) df.write.format(\"delta\") df.writeStream.format(\"delta\") TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-DataSourceRegister.html[DataSourceRegister ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. DeltaDataSource is registered using META-INF/services/org.apache.spark.sql.sources.DataSourceRegister :","title":"DeltaDataSource"},{"location":"DeltaDataSource/#source-scala","text":"","title":"[source, scala]"},{"location":"DeltaDataSource/#orgapachesparksqldeltasourcesdeltadatasource","text":"","title":"org.apache.spark.sql.delta.sources.DeltaDataSource"},{"location":"DeltaDataSource/#relationprovider-creating-insertable-hadoopfsrelation-for-batch-queries","text":"DeltaDataSource is a RelationProvider for reading ( loading ) data from a delta table in a structured query. Tip Read up on RelationProvider in The Internals of Spark SQL online book. createRelation ( sqlContext : SQLContext , parameters : Map [ String , String ]) : BaseRelation createRelation ...FIXME In the end, createRelation requests the DeltaLog for an insertable HadoopFsRelation . == [[CreatableRelationProvider]][[CreatableRelationProvider-createRelation]] CreatableRelationProvider DeltaDataSource is a CreatableRelationProvider for writing out the result of a structured query. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-CreatableRelationProvider.html[CreatableRelationProvider ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. == [[StreamSourceProvider]][[createSource]] Creating Streaming Source (Structured Streaming) -- createSource Method DeltaDataSource is a StreamSourceProvider . TIP: Read up on https://jaceklaskowski.gitbooks.io/spark-structured-streaming/spark-sql-streaming-StreamSourceProvider.html[StreamSourceProvider ] in https://bit.ly/spark-structured-streaming[The Internals of Spark Structured Streaming] online book. == [[StreamSinkProvider]][[createSink]] Creating Streaming Sink (Structured Streaming) -- createSink Method DeltaDataSource is a StreamSinkProvider for a streaming sink for Structured Streaming. TIP: Read up on https://jaceklaskowski.gitbooks.io/spark-structured-streaming/spark-sql-streaming-StreamSinkProvider.html[StreamSinkProvider ] in https://bit.ly/spark-structured-streaming[The Internals of Spark Structured Streaming] online book. DeltaDataSource supports Append and Complete output modes only. In the end, DeltaDataSource creates a < >. TIP: Consult the demo < >. == [[sourceSchema]] sourceSchema Method","title":" RelationProvider - Creating Insertable HadoopFsRelation For Batch Queries"},{"location":"DeltaDataSource/#source-scala_1","text":"sourceSchema( sqlContext: SQLContext, schema: Option[StructType], providerName: String, parameters: Map[String, String]): (String, StructType) NOTE: sourceSchema is part of the StreamSourceProvider contract (Spark Structured Streaming) for the name and schema of the streaming source. sourceSchema ...FIXME == [[getTimeTravelVersion]] getTimeTravelVersion Internal Method","title":"[source, scala]"},{"location":"DeltaDataSource/#source-scala_2","text":"getTimeTravelVersion( parameters: Map[String, String]): Option[DeltaTimeTravelSpec] getTimeTravelVersion ...FIXME NOTE: getTimeTravelVersion is used exclusively when DeltaDataSource is requested to < >. == [[extractDeltaPath]] extractDeltaPath Utility","title":"[source, scala]"},{"location":"DeltaDataSource/#source-scala_3","text":"extractDeltaPath( dataset: Dataset[_]): Option[String] extractDeltaPath ...FIXME NOTE: extractDeltaPath does not seem to be used whatsoever. == [[parsePathIdentifier]] parsePathIdentifier Utility","title":"[source, scala]"},{"location":"DeltaDataSource/#source-scala_4","text":"parsePathIdentifier( spark: SparkSession, userPath: String): (Path, Seq[(String, String)], Option[DeltaTimeTravelSpec]) parsePathIdentifier...FIXME parsePathIdentifier is used when DeltaTableV2 is requested for the DeltaTableV2.md#rootPath[rootPath, partitionFilters, and timeTravelByPath] (for a non-catalog table). == [[getTable]] Loading Table","title":"[source, scala]"},{"location":"DeltaDataSource/#sourcescala","text":"getTable( schema: StructType, partitioning: Array[Transform], properties: java.util.Map[String, String]): Table getTable...FIXME getTable is part of the TableProvider (Spark SQL 3.0.0) abstraction.","title":"[source,scala]"},{"location":"DeltaErrors/","text":"= DeltaErrors Utility DeltaErrors utility is...FIXME == [[postCommitHookFailedException]] Reporting Post-Commit Hook Failure (RuntimeException) -- postCommitHookFailedException Method [source, scala] \u00b6 postCommitHookFailedException( failedHook: PostCommitHook, failedOnCommitVersion: Long, extraErrorMessage: String, error: Throwable): Throwable postCommitHookFailedException ...FIXME NOTE: postCommitHookFailedException is used when...FIXME","title":"DeltaErrors"},{"location":"DeltaErrors/#source-scala","text":"postCommitHookFailedException( failedHook: PostCommitHook, failedOnCommitVersion: Long, extraErrorMessage: String, error: Throwable): Throwable postCommitHookFailedException ...FIXME NOTE: postCommitHookFailedException is used when...FIXME","title":"[source, scala]"},{"location":"DeltaFileFormat/","text":"= [[DeltaFileFormat]] DeltaFileFormat Contract -- Spark FileFormat Of Delta Table DeltaFileFormat is the < > of < > that can < >. [[contract]] .DeltaFileFormat Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | fileFormat a| [[fileFormat]] [source, scala] \u00b6 fileFormat: FileFormat \u00b6 Spark SQL's FileFormat of a delta table Default: ParquetFileFormat Used when: DeltaLog is requested for a < > (in batch queries) and < > DeltaCommand is requested for a < > TransactionalWrite is requested to < > |=== [[implementations]] NOTE: < > is the only known DeltaFileFormat .","title":"DeltaFileFormat"},{"location":"DeltaFileFormat/#source-scala","text":"","title":"[source, scala]"},{"location":"DeltaFileFormat/#fileformat-fileformat","text":"Spark SQL's FileFormat of a delta table Default: ParquetFileFormat Used when: DeltaLog is requested for a < > (in batch queries) and < > DeltaCommand is requested for a < > TransactionalWrite is requested to < > |=== [[implementations]] NOTE: < > is the only known DeltaFileFormat .","title":"fileFormat: FileFormat"},{"location":"DeltaFileOperations/","text":"= DeltaFileOperations Utilities :navtitle: DeltaFileOperations == [[utilities]] Utilities === [[recursiveListDirs]] recursiveListDirs [source, scala] \u00b6 recursiveListDirs( spark: SparkSession, subDirs: Seq[String], hadoopConf: Broadcast[SerializableConfiguration], hiddenFileNameFilter: String => Boolean = defaultHiddenFileFilter, fileListingParallelism: Option[Int] = None): Dataset[SerializableFileStatus] recursiveListDirs...FIXME recursiveListDirs is used when: ManualListingFileManifest (of ConvertToDeltaCommandBase) is requested to doList VacuumCommand utility is used to VacuumCommand.md#gc[gc] === [[tryDeleteNonRecursive]] tryDeleteNonRecursive [source,scala] \u00b6 tryDeleteNonRecursive( fs: FileSystem, path: Path, tries: Int = 3): Boolean tryDeleteNonRecursive...FIXME tryDeleteNonRecursive is used when VacuumCommandImpl is requested to VacuumCommandImpl.md#delete[delete] == [[internal-methods]] Internal Methods === [[recurseDirectories]] recurseDirectories [source,scala] \u00b6 recurseDirectories( logStore: LogStore, filesAndDirs: Iterator[SerializableFileStatus], hiddenFileNameFilter: String => Boolean): Iterator[SerializableFileStatus] recurseDirectories...FIXME recurseDirectories is used when DeltaFileOperations is requested to < > and < >. === [[listUsingLogStore]] listUsingLogStore [source,scala] \u00b6 listUsingLogStore( logStore: LogStore, subDirs: Iterator[String], recurse: Boolean, hiddenFileNameFilter: String => Boolean): Iterator[SerializableFileStatus] listUsingLogStore...FIXME listUsingLogStore is used when DeltaFileOperations is requested to < > and < >. === [[isThrottlingError]] isThrottlingError [source,scala] \u00b6 isThrottlingError( t: Throwable): Boolean isThrottlingError returns true when the Throwable contains slow down . isThrottlingError is used when DeltaFileOperations is requested to < > and < >.","title":"DeltaFileOperations"},{"location":"DeltaFileOperations/#source-scala","text":"recursiveListDirs( spark: SparkSession, subDirs: Seq[String], hadoopConf: Broadcast[SerializableConfiguration], hiddenFileNameFilter: String => Boolean = defaultHiddenFileFilter, fileListingParallelism: Option[Int] = None): Dataset[SerializableFileStatus] recursiveListDirs...FIXME recursiveListDirs is used when: ManualListingFileManifest (of ConvertToDeltaCommandBase) is requested to doList VacuumCommand utility is used to VacuumCommand.md#gc[gc] === [[tryDeleteNonRecursive]] tryDeleteNonRecursive","title":"[source, scala]"},{"location":"DeltaFileOperations/#sourcescala","text":"tryDeleteNonRecursive( fs: FileSystem, path: Path, tries: Int = 3): Boolean tryDeleteNonRecursive...FIXME tryDeleteNonRecursive is used when VacuumCommandImpl is requested to VacuumCommandImpl.md#delete[delete] == [[internal-methods]] Internal Methods === [[recurseDirectories]] recurseDirectories","title":"[source,scala]"},{"location":"DeltaFileOperations/#sourcescala_1","text":"recurseDirectories( logStore: LogStore, filesAndDirs: Iterator[SerializableFileStatus], hiddenFileNameFilter: String => Boolean): Iterator[SerializableFileStatus] recurseDirectories...FIXME recurseDirectories is used when DeltaFileOperations is requested to < > and < >. === [[listUsingLogStore]] listUsingLogStore","title":"[source,scala]"},{"location":"DeltaFileOperations/#sourcescala_2","text":"listUsingLogStore( logStore: LogStore, subDirs: Iterator[String], recurse: Boolean, hiddenFileNameFilter: String => Boolean): Iterator[SerializableFileStatus] listUsingLogStore...FIXME listUsingLogStore is used when DeltaFileOperations is requested to < > and < >. === [[isThrottlingError]] isThrottlingError","title":"[source,scala]"},{"location":"DeltaFileOperations/#sourcescala_3","text":"isThrottlingError( t: Throwable): Boolean isThrottlingError returns true when the Throwable contains slow down . isThrottlingError is used when DeltaFileOperations is requested to < > and < >.","title":"[source,scala]"},{"location":"DeltaGenerateCommandBase/","text":"= DeltaGenerateCommandBase DeltaGenerateCommandBase is an extension of the RunnableCommand contract (from Spark SQL) for < > that < >. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan-RunnableCommand.html[RunnableCommand ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. [[implementations]] NOTE: < > is the default and only known DeltaGenerateCommandBase in Delta Lake. == [[getPath]] Getting Path Of Delta Table From Table Identifier -- getPath Method [source, scala] \u00b6 getPath( spark: SparkSession, tableId: TableIdentifier): Path getPath ...FIXME NOTE: getPath is used when DeltaGenerateCommand is requested to < >.","title":"DeltaGenerateCommandBase"},{"location":"DeltaGenerateCommandBase/#source-scala","text":"getPath( spark: SparkSession, tableId: TableIdentifier): Path getPath ...FIXME NOTE: getPath is used when DeltaGenerateCommand is requested to < >.","title":"[source, scala]"},{"location":"DeltaHistoryManager/","text":"= DeltaHistoryManager DeltaHistoryManager is...FIXME == [[getHistory]] getHistory Method [source, scala] \u00b6 getHistory( limitOpt: Option[Int]): Seq[CommitInfo] getHistory( start: Long, end: Option[Long]): Seq[CommitInfo] getHistory ...FIXME NOTE: getHistory is used when...FIXME == [[getActiveCommitAtTime]] getActiveCommitAtTime Method [source, scala] \u00b6 getActiveCommitAtTime( timestamp: Timestamp, canReturnLastCommit: Boolean, mustBeRecreatable: Boolean = true): Commit getActiveCommitAtTime ...FIXME NOTE: getActiveCommitAtTime is used exclusively when DeltaTableUtils utility is requested to < >. == [[checkVersionExists]] checkVersionExists Method [source, scala] \u00b6 checkVersionExists(version: Long): Unit \u00b6 checkVersionExists ...FIXME NOTE: checkVersionExists is used when...FIXME == [[parallelSearch]] parallelSearch Internal Method [source, scala] \u00b6 parallelSearch( time: Long, start: Long, end: Long): Commit parallelSearch ...FIXME NOTE: parallelSearch is used exclusively when DeltaHistoryManager is requested to < >. == [[parallelSearch0]] parallelSearch0 Internal Utility [source, scala] \u00b6 parallelSearch0( spark: SparkSession, conf: SerializableConfiguration, logPath: String, time: Long, start: Long, end: Long, step: Long): Commit parallelSearch0 ...FIXME NOTE: parallelSearch0 is used exclusively when DeltaHistoryManager is requested to < >. == [[getCommitInfo]] CommitInfo Of Delta File -- getCommitInfo Internal Utility [source, scala] \u00b6 getCommitInfo( logStore: LogStore, basePath: Path, version: Long): CommitInfo getCommitInfo ...FIXME NOTE: getCommitInfo is used when...FIXME","title":"DeltaHistoryManager"},{"location":"DeltaHistoryManager/#source-scala","text":"getHistory( limitOpt: Option[Int]): Seq[CommitInfo] getHistory( start: Long, end: Option[Long]): Seq[CommitInfo] getHistory ...FIXME NOTE: getHistory is used when...FIXME == [[getActiveCommitAtTime]] getActiveCommitAtTime Method","title":"[source, scala]"},{"location":"DeltaHistoryManager/#source-scala_1","text":"getActiveCommitAtTime( timestamp: Timestamp, canReturnLastCommit: Boolean, mustBeRecreatable: Boolean = true): Commit getActiveCommitAtTime ...FIXME NOTE: getActiveCommitAtTime is used exclusively when DeltaTableUtils utility is requested to < >. == [[checkVersionExists]] checkVersionExists Method","title":"[source, scala]"},{"location":"DeltaHistoryManager/#source-scala_2","text":"","title":"[source, scala]"},{"location":"DeltaHistoryManager/#checkversionexistsversion-long-unit","text":"checkVersionExists ...FIXME NOTE: checkVersionExists is used when...FIXME == [[parallelSearch]] parallelSearch Internal Method","title":"checkVersionExists(version: Long): Unit"},{"location":"DeltaHistoryManager/#source-scala_3","text":"parallelSearch( time: Long, start: Long, end: Long): Commit parallelSearch ...FIXME NOTE: parallelSearch is used exclusively when DeltaHistoryManager is requested to < >. == [[parallelSearch0]] parallelSearch0 Internal Utility","title":"[source, scala]"},{"location":"DeltaHistoryManager/#source-scala_4","text":"parallelSearch0( spark: SparkSession, conf: SerializableConfiguration, logPath: String, time: Long, start: Long, end: Long, step: Long): Commit parallelSearch0 ...FIXME NOTE: parallelSearch0 is used exclusively when DeltaHistoryManager is requested to < >. == [[getCommitInfo]] CommitInfo Of Delta File -- getCommitInfo Internal Utility","title":"[source, scala]"},{"location":"DeltaHistoryManager/#source-scala_5","text":"getCommitInfo( logStore: LogStore, basePath: Path, version: Long): CommitInfo getCommitInfo ...FIXME NOTE: getCommitInfo is used when...FIXME","title":"[source, scala]"},{"location":"DeltaInvariantCheckerExec/","text":"= [[DeltaInvariantCheckerExec]] DeltaInvariantCheckerExec Unary Physical Operator DeltaInvariantCheckerExec is...FIXME","title":"DeltaInvariantCheckerExec"},{"location":"DeltaLog/","text":"DeltaLog \u00b6 DeltaLog is a transaction log ( change log ) of changes to the state of a delta table . DeltaLog uses _delta_log directory for (the files of) the transaction log of a delta table (that is given when DeltaLog.forTable utility is used to create an instance). DeltaLog is created (indirectly via DeltaLog.apply utility) when DeltaLog.forTable utility is used. import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) val dataPath = \"/tmp/delta/t1\" import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, dataPath) import org.apache.hadoop.fs.Path val expected = new Path(s\"file:$dataPath/_delta_log/_last_checkpoint\") assert(deltaLog.LAST_CHECKPOINT == expected) A common idiom (if not the only way) to know the current version of the delta table is to request the DeltaLog for the current state (snapshot) and then for the version . import org.apache.spark.sql.delta.DeltaLog assert(deltaLog.isInstanceOf[DeltaLog]) val deltaVersion = deltaLog.snapshot.version scala> println(deltaVersion) 5 When created, DeltaLog does the following: Creates the < > based on < > configuration property (default: < >) Initializes the < > < > when there is no < > (e.g. the version of the < > is -1 ) In other words, the version of (the DeltaLog of) a delta table is at version 0 at the very minimum. assert ( deltaLog . snapshot . version >= 0 ) DeltaLog is a LogStoreProvider . == [[FileFormats]] FileFormats DeltaLog defines two FileFormats : [[CHECKPOINT_FILE_FORMAT]] ParquetFileFormat for indices of delta files [[COMMIT_FILE_FORMAT]] JsonFileFormat for indices of checkpoint files The FileFormats are used to create < > for < > that in turn used them for < >. == [[_delta_log]] _delta_log Directory DeltaLog uses _delta_log metadata directory under the < > directory (that is specified using < > utility). The _delta_log directory is resolved (in the < > utility) using the application-wide Hadoop https://hadoop.apache.org/docs/current2/api/org/apache/hadoop/conf/Configuration.html[Configuration ]. [NOTE] \u00b6 < > utility uses the given SparkSession to create an Hadoop Configuration instance. [source, scala] \u00b6 spark.sessionState.newHadoopConf() \u00b6 ==== Once resolved and turned into a qualified path, the _delta_log directory of the delta table (under the < > directory) is < > for later reuse. == [[store]] LogStore DeltaLog uses an < > for...FIXME == [[deltaLogCache]] Transaction Logs (DeltaLogs) per Fully-Qualified Path -- deltaLogCache Internal Registry [source, scala] \u00b6 deltaLogCache: Cache[Path, DeltaLog] \u00b6 NOTE: deltaLogCache is part of DeltaLog Scala object which makes it an application-wide cache. Once used, deltaLogCache will only be one until the application that uses it stops. deltaLogCache is a registry of DeltaLogs by their fully-qualified <<_delta_log, _delta_log>> directories. A new instance of DeltaLog is added when < > utility is used and the instance hasn't been created before for a path. deltaLogCache is invalidated: For a delta table using < > utility (and < > when the cached reference is no longer < >) For all delta tables using < > utility == [[creating-instance]] Creating DeltaLog Instance DeltaLog takes the following to be created: [[logPath]] Log directory (Hadoop https://hadoop.apache.org/docs/r2.6.5/api/org/apache/hadoop/fs/Path.html[Path ]) [[dataPath]] Data directory (Hadoop https://hadoop.apache.org/docs/r2.6.5/api/org/apache/hadoop/fs/Path.html[Path ]) [[clock]] Clock DeltaLog initializes the < >. == [[apply]] Looking Up Or Creating DeltaLog Instance -- apply Utility [source, scala] \u00b6 apply( spark: SparkSession, rawPath: Path, clock: Clock = new SystemClock): DeltaLog NOTE: rawPath is a Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path ] to the <<_delta_log, _delta_log>> directory at the root of the data of a delta table. apply ...FIXME NOTE: apply is used when DeltaLog is requested to < >. Executing Single-Threaded Operation in New Transaction \u00b6 withNewTransaction [ T ]( thunk : OptimisticTransaction => T ) : T withNewTransaction starts a new transaction (that is active for the whole thread) and executes the given thunk block. In the end, withNewTransaction makes the transaction no longer active . withNewTransaction is used when: DeleteCommand , MergeIntoCommand , UpdateCommand , and WriteIntoDelta commands are executed DeltaSink is requested to add a streaming micro-batch Starting New Transaction \u00b6 startTransaction () : OptimisticTransaction startTransaction < > and creates a new OptimisticTransaction.md[] (for this DeltaLog). NOTE: startTransaction is a subset of < >. startTransaction is used when: DeltaLog is requested to < > AlterDeltaTableCommand is requested to AlterDeltaTableCommand.md#startTransaction[startTransaction] ConvertToDeltaCommandBase is ConvertToDeltaCommand.md#run[executed] CreateDeltaTableCommand is CreateDeltaTableCommand.md#run[executed] == [[assertRemovable]] Throwing UnsupportedOperationException For appendOnly Table Property Enabled -- assertRemovable Method [source, scala] \u00b6 assertRemovable(): Unit \u00b6 assertRemovable throws an UnsupportedOperationException for the < > table property (< > the < >) enabled ( true ): This table is configured to only allow appends. If you would like to permit updates or deletes, use 'ALTER TABLE <table_name> SET TBLPROPERTIES (appendOnly=false)'. NOTE: assertRemovable is used when...FIXME == [[metadata]] metadata Method [source, scala] \u00b6 metadata: Metadata \u00b6 NOTE: metadata is part of the < > to...FIXME. metadata requests the < > for the < > or creates a new < > (if the < > is not initialized). == [[forTable]] Creating DeltaLog Instance -- forTable Utility [source, scala] \u00b6 forTable( spark: SparkSession, dataPath: File): DeltaLog forTable( spark: SparkSession, dataPath: File, clock: Clock): DeltaLog forTable( spark: SparkSession, dataPath: Path): DeltaLog forTable( spark: SparkSession, dataPath: Path, clock: Clock): DeltaLog forTable( spark: SparkSession, dataPath: String): DeltaLog forTable( spark: SparkSession, dataPath: String, clock: Clock): DeltaLog forTable creates a < > with _delta_log directory (in the given dataPath directory). [NOTE] \u00b6 forTable is used when: < > utility is used to create a < > < >, < >, < > are requested to run DeltaDataSource is requested to < >, < >, and create a relation (as < > and < >) < > utility is used DeltaTableIdentifier is requested to getDeltaLog * < > is created \u00b6 == [[update]] update Method [source, scala] \u00b6 update( stalenessAcceptable: Boolean = false): Snapshot update branches off based on a combination of flags: the given stalenessAcceptable and < > flags. For the stalenessAcceptable not acceptable (default) and the < >, update simply acquires the < > lock and < > (with isAsync flag off). For all other cases, update ...FIXME [NOTE] \u00b6 update is used when: DeltaHistoryManager is requested to < >, < >, and < > DeltaLog is < > (with no < > created), and requested to < > and < > OptimisticTransactionImpl is requested to < > and < > ConvertToDeltaCommand is requested to < > and < > VacuumCommand utility is used to < > TahoeLogFileIndex is requested for the < > * DeltaDataSource is requested for a < > \u00b6 == [[snapshot]] Current State Snapshot -- snapshot Method [source, scala] \u00b6 snapshot: Snapshot \u00b6 snapshot returns the < >. [NOTE] \u00b6 snapshot is used when: < > is created Checkpoints is requested to < > DeltaLog is requested for the < >, to < >, < >, < > OptimisticTransactionImpl is requested to < > < >, < >, < >, < >, < > are executed DeltaCommand is requested to < > TahoeFileIndex is requested for the < >, < > TahoeLogFileIndex is requested for the < > DeltaDataSource is requested for the < > * < > is created and requested for the < >, < > \u00b6 == [[currentSnapshot]] Current State Snapshot -- currentSnapshot Internal Registry [source, scala] \u00b6 currentSnapshot: Snapshot \u00b6 currentSnapshot is a < > based on the < > if available or a new Snapshot instance (with version being -1 ). NOTE: For a new Snapshot instance (with version being -1 ) DeltaLog immediately < >. Internally, currentSnapshot ...FIXME NOTE: currentSnapshot is available using < > method. NOTE: currentSnapshot is used when DeltaLog is requested to < >, < >, < >, and < . Creating Insertable HadoopFsRelation For Batch Queries \u00b6 createRelation ( partitionFilters : Seq [ Expression ] = Nil , timeTravel : Option [ DeltaTimeTravelSpec ] = None ) : BaseRelation createRelation ...FIXME createRelation creates a TahoeLogFileIndex for the data path , the given partitionFilters and a version (if defined). createRelation ...FIXME In the end, createRelation creates a HadoopFsRelation for the TahoeLogFileIndex and...FIXME. The HadoopFsRelation is also an < >. createRelation is used when DeltaDataSource is requested for a relation as a CreatableRelationProvider and a RelationProvider (for batch queries). insert Method \u00b6 insert ( data : DataFrame , overwrite : Boolean ) : Unit insert ...FIXME insert is part of the InsertableRelation abstraction. == [[getSnapshotAt]] Retrieving State Of Delta Table At Given Version -- getSnapshotAt Method [source, scala] \u00b6 getSnapshotAt( version: Long, commitTimestamp: Option[Long] = None, lastCheckpointHint: Option[CheckpointInstance] = None): Snapshot getSnapshotAt ...FIXME [NOTE] \u00b6 getSnapshotAt is used when: DeltaLog is requested for a < >, and to < > DeltaSource is requested to < > * TahoeLogFileIndex is requested for < > \u00b6 == [[tryUpdate]] tryUpdate Method [source, scala] \u00b6 tryUpdate( isAsync: Boolean = false): Snapshot tryUpdate ...FIXME NOTE: tryUpdate is used exclusively when DeltaLog is requested to < >. == [[ensureLogDirectoryExist]] ensureLogDirectoryExist Method [source, scala] \u00b6 ensureLogDirectoryExist(): Unit \u00b6 ensureLogDirectoryExist ...FIXME NOTE: ensureLogDirectoryExist is used when...FIXME == [[protocolWrite]] protocolWrite Method [source, scala] \u00b6 protocolWrite( protocol: Protocol, logUpgradeMessage: Boolean = true): Unit protocolWrite ...FIXME NOTE: protocolWrite is used when...FIXME == [[checkpointInterval]] checkpointInterval Method [source, scala] \u00b6 checkpointInterval: Int \u00b6 checkpointInterval gives the value of < > table property (< > the < >). NOTE: checkpointInterval is used when...FIXME == [[getChanges]] Changes (Actions) Of Delta Version And Later -- getChanges Method [source, scala] \u00b6 getChanges( startVersion: Long): Iterator[(Long, Seq[Action])] getChanges gives all < > ( changes ) per delta log file for the given startVersion of a delta table and later. [source,scala] \u00b6 val dataPath = \"/tmp/delta/users\" import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, dataPath) assert(deltaLog.isInstanceOf[DeltaLog]) val changesPerVersion = deltaLog.getChanges(startVersion = 0) Internally, getChanges requests the < > for < > that are lexicographically greater or equal to the < > for the given startVersion (in the < >) and leaves only < > (e.g. files with numbers only as file name and .json file extension). For every delta file, getChanges requests the < > to < > (every line is an < >), and then < >. NOTE: getChanges is used when DeltaSource is requested for the < >. == [[createDataFrame]] Creating DataFrame For Given AddFiles -- createDataFrame Method [source, scala] \u00b6 createDataFrame( snapshot: Snapshot, addFiles: Seq[AddFile], isStreaming: Boolean = false, actionTypeOpt: Option[String] = None): DataFrame createDataFrame takes the action name to build the result DataFrame for from the actionTypeOpt (if defined), or uses the following per isStreaming flag: streaming when isStreaming flag is enabled ( true ) batch when isStreaming flag is disabled ( false ) createDataFrame creates a new < > (for the action type, and the given < > and < >). createDataFrame creates a HadoopFsRelation with the TahoeBatchFileIndex and the other properties based on the given < > (and its < >). TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-BaseRelation-HadoopFsRelation.html[HadoopFsRelation ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. In the end, createDataFrame creates a DataFrame with a logical query plan with a LogicalRelation over the HadoopFsRelation . TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan-LogicalRelation.html[LogicalRelation ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. [NOTE] \u00b6 createDataFrame is used when: < > is executed * DeltaSource is requested for a < > \u00b6 == [[lockInterruptibly]] Acquiring Interruptible Lock on Log -- lockInterruptibly Method [source, scala] \u00b6 lockInterruptibly T : T \u00b6 lockInterruptibly ...FIXME NOTE: lockInterruptibly is used when...FIXME == [[minFileRetentionTimestamp]] minFileRetentionTimestamp Method [source, scala] \u00b6 minFileRetentionTimestamp: Long \u00b6 minFileRetentionTimestamp is the timestamp that is < > before the current time (per the < >). [NOTE] \u00b6 minFileRetentionTimestamp is used when: DeltaLog is requested for the < >, to < >, and to < > * VacuumCommand is requested for < > \u00b6 == [[tombstoneRetentionMillis]] tombstoneRetentionMillis Method [source, scala] \u00b6 tombstoneRetentionMillis: Long \u00b6 tombstoneRetentionMillis gives the value of < > table property (< > the < >). [NOTE] \u00b6 tombstoneRetentionMillis is used when: DeltaLog is requested for < > * VacuumCommand is requested for < > \u00b6 == [[updateInternal]] updateInternal Internal Method [source, scala] \u00b6 updateInternal( isAsync: Boolean): Snapshot updateInternal ...FIXME NOTE: updateInternal is used when DeltaLog is requested to < > (directly or via < >). == [[invalidateCache]] Invalidating Cached DeltaLog Instance By Path -- invalidateCache Utility [source, scala] \u00b6 invalidateCache( spark: SparkSession, dataPath: Path): Unit invalidateCache ...FIXME NOTE: invalidateCache is a public API and does not seem to be used at all. == [[clearCache]] Removing (Clearing) All Cached DeltaLog Instances -- clearCache Utility [source, scala] \u00b6 clearCache(): Unit \u00b6 clearCache ...FIXME NOTE: clearCache is a public API and is used exclusively in tests. == [[upgradeProtocol]] upgradeProtocol Method [source, scala] \u00b6 upgradeProtocol( newVersion: Protocol = Protocol()): Unit upgradeProtocol ...FIXME NOTE: upgradeProtocol seems to be used exclusively in tests. == [[protocolRead]] protocolRead Method [source, scala] \u00b6 protocolRead( protocol: Protocol): Unit protocolRead ...FIXME [NOTE] \u00b6 protocolRead is used when: OptimisticTransactionImpl is requested to < > < > is created * DeltaSource is requested to < > \u00b6 == [[isValid]] isValid Method [source, scala] \u00b6 isValid(): Boolean \u00b6 isValid ...FIXME NOTE: isValid is used when DeltaLog utility is used to < >. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | deltaLogLock a| [[deltaLogLock]] Lock Used when...FIXME |=== Logging \u00b6 Enable ALL logging level for org.apache.spark.sql.delta.DeltaLog logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.DeltaLog=ALL Refer to Logging .","title":"DeltaLog"},{"location":"DeltaLog/#deltalog","text":"DeltaLog is a transaction log ( change log ) of changes to the state of a delta table . DeltaLog uses _delta_log directory for (the files of) the transaction log of a delta table (that is given when DeltaLog.forTable utility is used to create an instance). DeltaLog is created (indirectly via DeltaLog.apply utility) when DeltaLog.forTable utility is used. import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) val dataPath = \"/tmp/delta/t1\" import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, dataPath) import org.apache.hadoop.fs.Path val expected = new Path(s\"file:$dataPath/_delta_log/_last_checkpoint\") assert(deltaLog.LAST_CHECKPOINT == expected) A common idiom (if not the only way) to know the current version of the delta table is to request the DeltaLog for the current state (snapshot) and then for the version . import org.apache.spark.sql.delta.DeltaLog assert(deltaLog.isInstanceOf[DeltaLog]) val deltaVersion = deltaLog.snapshot.version scala> println(deltaVersion) 5 When created, DeltaLog does the following: Creates the < > based on < > configuration property (default: < >) Initializes the < > < > when there is no < > (e.g. the version of the < > is -1 ) In other words, the version of (the DeltaLog of) a delta table is at version 0 at the very minimum. assert ( deltaLog . snapshot . version >= 0 ) DeltaLog is a LogStoreProvider . == [[FileFormats]] FileFormats DeltaLog defines two FileFormats : [[CHECKPOINT_FILE_FORMAT]] ParquetFileFormat for indices of delta files [[COMMIT_FILE_FORMAT]] JsonFileFormat for indices of checkpoint files The FileFormats are used to create < > for < > that in turn used them for < >. == [[_delta_log]] _delta_log Directory DeltaLog uses _delta_log metadata directory under the < > directory (that is specified using < > utility). The _delta_log directory is resolved (in the < > utility) using the application-wide Hadoop https://hadoop.apache.org/docs/current2/api/org/apache/hadoop/conf/Configuration.html[Configuration ].","title":"DeltaLog"},{"location":"DeltaLog/#note","text":"< > utility uses the given SparkSession to create an Hadoop Configuration instance.","title":"[NOTE]"},{"location":"DeltaLog/#source-scala","text":"","title":"[source, scala]"},{"location":"DeltaLog/#sparksessionstatenewhadoopconf","text":"==== Once resolved and turned into a qualified path, the _delta_log directory of the delta table (under the < > directory) is < > for later reuse. == [[store]] LogStore DeltaLog uses an < > for...FIXME == [[deltaLogCache]] Transaction Logs (DeltaLogs) per Fully-Qualified Path -- deltaLogCache Internal Registry","title":"spark.sessionState.newHadoopConf()"},{"location":"DeltaLog/#source-scala_1","text":"","title":"[source, scala]"},{"location":"DeltaLog/#deltalogcache-cachepath-deltalog","text":"NOTE: deltaLogCache is part of DeltaLog Scala object which makes it an application-wide cache. Once used, deltaLogCache will only be one until the application that uses it stops. deltaLogCache is a registry of DeltaLogs by their fully-qualified <<_delta_log, _delta_log>> directories. A new instance of DeltaLog is added when < > utility is used and the instance hasn't been created before for a path. deltaLogCache is invalidated: For a delta table using < > utility (and < > when the cached reference is no longer < >) For all delta tables using < > utility == [[creating-instance]] Creating DeltaLog Instance DeltaLog takes the following to be created: [[logPath]] Log directory (Hadoop https://hadoop.apache.org/docs/r2.6.5/api/org/apache/hadoop/fs/Path.html[Path ]) [[dataPath]] Data directory (Hadoop https://hadoop.apache.org/docs/r2.6.5/api/org/apache/hadoop/fs/Path.html[Path ]) [[clock]] Clock DeltaLog initializes the < >. == [[apply]] Looking Up Or Creating DeltaLog Instance -- apply Utility","title":"deltaLogCache: Cache[Path, DeltaLog]"},{"location":"DeltaLog/#source-scala_2","text":"apply( spark: SparkSession, rawPath: Path, clock: Clock = new SystemClock): DeltaLog NOTE: rawPath is a Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path ] to the <<_delta_log, _delta_log>> directory at the root of the data of a delta table. apply ...FIXME NOTE: apply is used when DeltaLog is requested to < >.","title":"[source, scala]"},{"location":"DeltaLog/#executing-single-threaded-operation-in-new-transaction","text":"withNewTransaction [ T ]( thunk : OptimisticTransaction => T ) : T withNewTransaction starts a new transaction (that is active for the whole thread) and executes the given thunk block. In the end, withNewTransaction makes the transaction no longer active . withNewTransaction is used when: DeleteCommand , MergeIntoCommand , UpdateCommand , and WriteIntoDelta commands are executed DeltaSink is requested to add a streaming micro-batch","title":" Executing Single-Threaded Operation in New Transaction"},{"location":"DeltaLog/#starting-new-transaction","text":"startTransaction () : OptimisticTransaction startTransaction < > and creates a new OptimisticTransaction.md[] (for this DeltaLog). NOTE: startTransaction is a subset of < >. startTransaction is used when: DeltaLog is requested to < > AlterDeltaTableCommand is requested to AlterDeltaTableCommand.md#startTransaction[startTransaction] ConvertToDeltaCommandBase is ConvertToDeltaCommand.md#run[executed] CreateDeltaTableCommand is CreateDeltaTableCommand.md#run[executed] == [[assertRemovable]] Throwing UnsupportedOperationException For appendOnly Table Property Enabled -- assertRemovable Method","title":" Starting New Transaction"},{"location":"DeltaLog/#source-scala_3","text":"","title":"[source, scala]"},{"location":"DeltaLog/#assertremovable-unit","text":"assertRemovable throws an UnsupportedOperationException for the < > table property (< > the < >) enabled ( true ): This table is configured to only allow appends. If you would like to permit updates or deletes, use 'ALTER TABLE <table_name> SET TBLPROPERTIES (appendOnly=false)'. NOTE: assertRemovable is used when...FIXME == [[metadata]] metadata Method","title":"assertRemovable(): Unit"},{"location":"DeltaLog/#source-scala_4","text":"","title":"[source, scala]"},{"location":"DeltaLog/#metadata-metadata","text":"NOTE: metadata is part of the < > to...FIXME. metadata requests the < > for the < > or creates a new < > (if the < > is not initialized). == [[forTable]] Creating DeltaLog Instance -- forTable Utility","title":"metadata: Metadata"},{"location":"DeltaLog/#source-scala_5","text":"forTable( spark: SparkSession, dataPath: File): DeltaLog forTable( spark: SparkSession, dataPath: File, clock: Clock): DeltaLog forTable( spark: SparkSession, dataPath: Path): DeltaLog forTable( spark: SparkSession, dataPath: Path, clock: Clock): DeltaLog forTable( spark: SparkSession, dataPath: String): DeltaLog forTable( spark: SparkSession, dataPath: String, clock: Clock): DeltaLog forTable creates a < > with _delta_log directory (in the given dataPath directory).","title":"[source, scala]"},{"location":"DeltaLog/#note_1","text":"forTable is used when: < > utility is used to create a < > < >, < >, < > are requested to run DeltaDataSource is requested to < >, < >, and create a relation (as < > and < >) < > utility is used DeltaTableIdentifier is requested to getDeltaLog","title":"[NOTE]"},{"location":"DeltaLog/#is-created","text":"== [[update]] update Method","title":"* &lt;&gt; is created"},{"location":"DeltaLog/#source-scala_6","text":"update( stalenessAcceptable: Boolean = false): Snapshot update branches off based on a combination of flags: the given stalenessAcceptable and < > flags. For the stalenessAcceptable not acceptable (default) and the < >, update simply acquires the < > lock and < > (with isAsync flag off). For all other cases, update ...FIXME","title":"[source, scala]"},{"location":"DeltaLog/#note_2","text":"update is used when: DeltaHistoryManager is requested to < >, < >, and < > DeltaLog is < > (with no < > created), and requested to < > and < > OptimisticTransactionImpl is requested to < > and < > ConvertToDeltaCommand is requested to < > and < > VacuumCommand utility is used to < > TahoeLogFileIndex is requested for the < >","title":"[NOTE]"},{"location":"DeltaLog/#deltadatasource-is-requested-for-a","text":"== [[snapshot]] Current State Snapshot -- snapshot Method","title":"* DeltaDataSource is requested for a &lt;&gt;"},{"location":"DeltaLog/#source-scala_7","text":"","title":"[source, scala]"},{"location":"DeltaLog/#snapshot-snapshot","text":"snapshot returns the < >.","title":"snapshot: Snapshot"},{"location":"DeltaLog/#note_3","text":"snapshot is used when: < > is created Checkpoints is requested to < > DeltaLog is requested for the < >, to < >, < >, < > OptimisticTransactionImpl is requested to < > < >, < >, < >, < >, < > are executed DeltaCommand is requested to < > TahoeFileIndex is requested for the < >, < > TahoeLogFileIndex is requested for the < > DeltaDataSource is requested for the < >","title":"[NOTE]"},{"location":"DeltaLog/#is-created-and-requested-for-the","text":"== [[currentSnapshot]] Current State Snapshot -- currentSnapshot Internal Registry","title":"* &lt;&gt; is created and requested for the &lt;&gt;, &lt;&gt;"},{"location":"DeltaLog/#source-scala_8","text":"","title":"[source, scala]"},{"location":"DeltaLog/#currentsnapshot-snapshot","text":"currentSnapshot is a < > based on the < > if available or a new Snapshot instance (with version being -1 ). NOTE: For a new Snapshot instance (with version being -1 ) DeltaLog immediately < >. Internally, currentSnapshot ...FIXME NOTE: currentSnapshot is available using < > method. NOTE: currentSnapshot is used when DeltaLog is requested to < >, < >, < >, and < .","title":"currentSnapshot: Snapshot"},{"location":"DeltaLog/#creating-insertable-hadoopfsrelation-for-batch-queries","text":"createRelation ( partitionFilters : Seq [ Expression ] = Nil , timeTravel : Option [ DeltaTimeTravelSpec ] = None ) : BaseRelation createRelation ...FIXME createRelation creates a TahoeLogFileIndex for the data path , the given partitionFilters and a version (if defined). createRelation ...FIXME In the end, createRelation creates a HadoopFsRelation for the TahoeLogFileIndex and...FIXME. The HadoopFsRelation is also an < >. createRelation is used when DeltaDataSource is requested for a relation as a CreatableRelationProvider and a RelationProvider (for batch queries).","title":" Creating Insertable HadoopFsRelation For Batch Queries"},{"location":"DeltaLog/#insert-method","text":"insert ( data : DataFrame , overwrite : Boolean ) : Unit insert ...FIXME insert is part of the InsertableRelation abstraction. == [[getSnapshotAt]] Retrieving State Of Delta Table At Given Version -- getSnapshotAt Method","title":" insert Method"},{"location":"DeltaLog/#source-scala_9","text":"getSnapshotAt( version: Long, commitTimestamp: Option[Long] = None, lastCheckpointHint: Option[CheckpointInstance] = None): Snapshot getSnapshotAt ...FIXME","title":"[source, scala]"},{"location":"DeltaLog/#note_4","text":"getSnapshotAt is used when: DeltaLog is requested for a < >, and to < > DeltaSource is requested to < >","title":"[NOTE]"},{"location":"DeltaLog/#tahoelogfileindex-is-requested-for","text":"== [[tryUpdate]] tryUpdate Method","title":"* TahoeLogFileIndex is requested for &lt;&gt;"},{"location":"DeltaLog/#source-scala_10","text":"tryUpdate( isAsync: Boolean = false): Snapshot tryUpdate ...FIXME NOTE: tryUpdate is used exclusively when DeltaLog is requested to < >. == [[ensureLogDirectoryExist]] ensureLogDirectoryExist Method","title":"[source, scala]"},{"location":"DeltaLog/#source-scala_11","text":"","title":"[source, scala]"},{"location":"DeltaLog/#ensurelogdirectoryexist-unit","text":"ensureLogDirectoryExist ...FIXME NOTE: ensureLogDirectoryExist is used when...FIXME == [[protocolWrite]] protocolWrite Method","title":"ensureLogDirectoryExist(): Unit"},{"location":"DeltaLog/#source-scala_12","text":"protocolWrite( protocol: Protocol, logUpgradeMessage: Boolean = true): Unit protocolWrite ...FIXME NOTE: protocolWrite is used when...FIXME == [[checkpointInterval]] checkpointInterval Method","title":"[source, scala]"},{"location":"DeltaLog/#source-scala_13","text":"","title":"[source, scala]"},{"location":"DeltaLog/#checkpointinterval-int","text":"checkpointInterval gives the value of < > table property (< > the < >). NOTE: checkpointInterval is used when...FIXME == [[getChanges]] Changes (Actions) Of Delta Version And Later -- getChanges Method","title":"checkpointInterval: Int"},{"location":"DeltaLog/#source-scala_14","text":"getChanges( startVersion: Long): Iterator[(Long, Seq[Action])] getChanges gives all < > ( changes ) per delta log file for the given startVersion of a delta table and later.","title":"[source, scala]"},{"location":"DeltaLog/#sourcescala","text":"val dataPath = \"/tmp/delta/users\" import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, dataPath) assert(deltaLog.isInstanceOf[DeltaLog]) val changesPerVersion = deltaLog.getChanges(startVersion = 0) Internally, getChanges requests the < > for < > that are lexicographically greater or equal to the < > for the given startVersion (in the < >) and leaves only < > (e.g. files with numbers only as file name and .json file extension). For every delta file, getChanges requests the < > to < > (every line is an < >), and then < >. NOTE: getChanges is used when DeltaSource is requested for the < >. == [[createDataFrame]] Creating DataFrame For Given AddFiles -- createDataFrame Method","title":"[source,scala]"},{"location":"DeltaLog/#source-scala_15","text":"createDataFrame( snapshot: Snapshot, addFiles: Seq[AddFile], isStreaming: Boolean = false, actionTypeOpt: Option[String] = None): DataFrame createDataFrame takes the action name to build the result DataFrame for from the actionTypeOpt (if defined), or uses the following per isStreaming flag: streaming when isStreaming flag is enabled ( true ) batch when isStreaming flag is disabled ( false ) createDataFrame creates a new < > (for the action type, and the given < > and < >). createDataFrame creates a HadoopFsRelation with the TahoeBatchFileIndex and the other properties based on the given < > (and its < >). TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-BaseRelation-HadoopFsRelation.html[HadoopFsRelation ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. In the end, createDataFrame creates a DataFrame with a logical query plan with a LogicalRelation over the HadoopFsRelation . TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan-LogicalRelation.html[LogicalRelation ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book.","title":"[source, scala]"},{"location":"DeltaLog/#note_5","text":"createDataFrame is used when: < > is executed","title":"[NOTE]"},{"location":"DeltaLog/#deltasource-is-requested-for-a","text":"== [[lockInterruptibly]] Acquiring Interruptible Lock on Log -- lockInterruptibly Method","title":"* DeltaSource is requested for a &lt;&gt;"},{"location":"DeltaLog/#source-scala_16","text":"","title":"[source, scala]"},{"location":"DeltaLog/#lockinterruptiblyt-t","text":"lockInterruptibly ...FIXME NOTE: lockInterruptibly is used when...FIXME == [[minFileRetentionTimestamp]] minFileRetentionTimestamp Method","title":"lockInterruptiblyT: T"},{"location":"DeltaLog/#source-scala_17","text":"","title":"[source, scala]"},{"location":"DeltaLog/#minfileretentiontimestamp-long","text":"minFileRetentionTimestamp is the timestamp that is < > before the current time (per the < >).","title":"minFileRetentionTimestamp: Long"},{"location":"DeltaLog/#note_6","text":"minFileRetentionTimestamp is used when: DeltaLog is requested for the < >, to < >, and to < >","title":"[NOTE]"},{"location":"DeltaLog/#vacuumcommand-is-requested-for","text":"== [[tombstoneRetentionMillis]] tombstoneRetentionMillis Method","title":"* VacuumCommand is requested for &lt;&gt;"},{"location":"DeltaLog/#source-scala_18","text":"","title":"[source, scala]"},{"location":"DeltaLog/#tombstoneretentionmillis-long","text":"tombstoneRetentionMillis gives the value of < > table property (< > the < >).","title":"tombstoneRetentionMillis: Long"},{"location":"DeltaLog/#note_7","text":"tombstoneRetentionMillis is used when: DeltaLog is requested for < >","title":"[NOTE]"},{"location":"DeltaLog/#vacuumcommand-is-requested-for_1","text":"== [[updateInternal]] updateInternal Internal Method","title":"* VacuumCommand is requested for &lt;&gt;"},{"location":"DeltaLog/#source-scala_19","text":"updateInternal( isAsync: Boolean): Snapshot updateInternal ...FIXME NOTE: updateInternal is used when DeltaLog is requested to < > (directly or via < >). == [[invalidateCache]] Invalidating Cached DeltaLog Instance By Path -- invalidateCache Utility","title":"[source, scala]"},{"location":"DeltaLog/#source-scala_20","text":"invalidateCache( spark: SparkSession, dataPath: Path): Unit invalidateCache ...FIXME NOTE: invalidateCache is a public API and does not seem to be used at all. == [[clearCache]] Removing (Clearing) All Cached DeltaLog Instances -- clearCache Utility","title":"[source, scala]"},{"location":"DeltaLog/#source-scala_21","text":"","title":"[source, scala]"},{"location":"DeltaLog/#clearcache-unit","text":"clearCache ...FIXME NOTE: clearCache is a public API and is used exclusively in tests. == [[upgradeProtocol]] upgradeProtocol Method","title":"clearCache(): Unit"},{"location":"DeltaLog/#source-scala_22","text":"upgradeProtocol( newVersion: Protocol = Protocol()): Unit upgradeProtocol ...FIXME NOTE: upgradeProtocol seems to be used exclusively in tests. == [[protocolRead]] protocolRead Method","title":"[source, scala]"},{"location":"DeltaLog/#source-scala_23","text":"protocolRead( protocol: Protocol): Unit protocolRead ...FIXME","title":"[source, scala]"},{"location":"DeltaLog/#note_8","text":"protocolRead is used when: OptimisticTransactionImpl is requested to < > < > is created","title":"[NOTE]"},{"location":"DeltaLog/#deltasource-is-requested-to","text":"== [[isValid]] isValid Method","title":"* DeltaSource is requested to &lt;&gt;"},{"location":"DeltaLog/#source-scala_24","text":"","title":"[source, scala]"},{"location":"DeltaLog/#isvalid-boolean","text":"isValid ...FIXME NOTE: isValid is used when DeltaLog utility is used to < >. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | deltaLogLock a| [[deltaLogLock]] Lock Used when...FIXME |===","title":"isValid(): Boolean"},{"location":"DeltaLog/#logging","text":"Enable ALL logging level for org.apache.spark.sql.delta.DeltaLog logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.DeltaLog=ALL Refer to Logging .","title":"Logging"},{"location":"DeltaLogFileIndex/","text":"= DeltaLogFileIndex [[creating-instance]] DeltaLogFileIndex is a FileIndex (Spark SQL) that takes the following to be created: [[format]] One of the supported < > [[files]] Array[FileStatus] (Hadoop) DeltaLogFileIndex is < > (directly or indirectly using < > utility) when DeltaLog is requested for the < >, to < > and < >. == [[apply]] apply Utility [source, scala] \u00b6 apply( format: FileFormat, fs: FileSystem, paths: Seq[Path]): DeltaLogFileIndex apply simply creates a new DeltaLogFileIndex . NOTE: apply is used when DeltaLog is requested for the < >, to < > and < >.","title":"DeltaLogFileIndex"},{"location":"DeltaLogFileIndex/#source-scala","text":"apply( format: FileFormat, fs: FileSystem, paths: Seq[Path]): DeltaLogFileIndex apply simply creates a new DeltaLogFileIndex . NOTE: apply is used when DeltaLog is requested for the < >, to < > and < >.","title":"[source, scala]"},{"location":"DeltaOptions/","text":"= DeltaOptions [[DeltaWriteOptionsImpl]][[DeltaWriteOptions]][[DeltaReadOptions]] DeltaOptions (aka DeltaWriteOptionsImpl , DeltaWriteOptions ) is the options for the < >. The < > can be defined using option method of DataFrameReader , DataFrameWriter , DataStreamReader , and DataStreamWriter . DeltaOptions is used to create < > command, < >, and < >. DeltaOptions can be < >. == [[options]][[validOptionKeys]] Options === [[checkpointLocation]] checkpointLocation === [[DATA_CHANGE_OPTION]][[dataChange]] dataChange === [[EXCLUDE_REGEX_OPTION]][[excludeRegex]] excludeRegex === [[IGNORE_CHANGES_OPTION]][[ignoreChanges]] ignoreChanges === [[IGNORE_DELETES_OPTION]][[ignoreDeletes]] ignoreDeletes === [[IGNORE_FILE_DELETION_OPTION]][[ignoreFileDeletion]] ignoreFileDeletion === [[MAX_BYTES_PER_TRIGGER_OPTION]][[maxBytesPerTrigger]] maxBytesPerTrigger === [[MAX_FILES_PER_TRIGGER_OPTION]][[maxFilesPerTrigger]][[MAX_FILES_PER_TRIGGER_OPTION_DEFAULT]] maxFilesPerTrigger Maximum number of files (< >) that < > will < > ( read ) in a streaming micro-batch ( trigger ) Default: 1000 Must be at least 1 === [[MERGE_SCHEMA_OPTION]][[mergeSchema]][[canMergeSchema]] mergeSchema Enables schema migration (e.g. allows automatic schema merging during a write operation for < > and < >) Equivalent SQL Session configuration: < > === [[OPTIMIZE_WRITE_OPTION]][[optimizeWrite]] optimizeWrite === [[OVERWRITE_SCHEMA_OPTION]][[overwriteSchema]] overwriteSchema === [[path]] path (required) Directory on a Hadoop DFS-compliant file system with an optional < > identifier. Default: (undefined) Can also be specified using load method of DataFrameReader and DataStreamReader . === [[queryName]] queryName === [[REPLACE_WHERE_OPTION]][[replaceWhere]] replaceWhere === [[timestampAsOf]] timestampAsOf < > using a timestamp of a table Mutually exclusive with < > option and the time travel identifier of the < > option. === [[USER_METADATA_OPTION]][[userMetadata]] userMetadata Defines a CommitInfo.md#userMetadata[user-defined commit metadata] Take precedence over DeltaSQLConf.md#commitInfo.userMetadata[spark.databricks.delta.commitInfo.userMetadata] Available by inspecting CommitInfo.md[]s using delta-sql.md#DESCRIBE-HISTORY[DESCRIBE HISTORY] or DeltaTable.md#history[DeltaTable.history]. === [[versionAsOf]] versionAsOf < > using a version of a table Mutually exclusive with < > option and the time travel identifier of the < > option. Used exclusively when DeltaDataSource is requested for a < > == [[creating-instance]] Creating Instance DeltaOptions takes the following to be created: Options ( Map[String, String] or CaseInsensitiveMap[String] ) [[sqlConf]] SQLConf DeltaOptions is created when: DeltaLog is requested to < > (for < > as a < > and a < >) DeltaDataSource is requested to < > (to create a < > for Structured Streaming), < > (to create a < > for Structured Streaming), and < > == [[verifyOptions]] verifyOptions Utility [source, scala] \u00b6 verifyOptions( options: CaseInsensitiveMap[String]): Unit verifyOptions...FIXME verifyOptions is used when: DeltaOptions is < > DeltaDataSource is requested to < >","title":"DeltaOptions"},{"location":"DeltaOptions/#source-scala","text":"verifyOptions( options: CaseInsensitiveMap[String]): Unit verifyOptions...FIXME verifyOptions is used when: DeltaOptions is < > DeltaDataSource is requested to < >","title":"[source, scala]"},{"location":"DeltaSQLConf/","text":"DeltaSQLConf \u2014 spark.databricks.delta Configuration Properties \u00b6 DeltaSQLConf contains spark.databricks.delta -prefixed configuration properties to configure behaviour of Delta Lake. alterLocation.bypassSchemaCheck \u00b6 spark.databricks.delta.alterLocation.bypassSchemaCheck enables Alter Table Set Location on Delta to go through even if the Delta table in the new location has a different schema from the original Delta table Default: false checkLatestSchemaOnRead \u00b6 spark.databricks.delta.checkLatestSchemaOnRead (internal) enables a check that ensures that users won't read corrupt data if the source schema changes in an incompatible way. Default: true In Delta, we always try to give users the latest version of their data without having to call REFRESH TABLE or redefine their DataFrames when used in the context of streaming. There is a possibility that the schema of the latest version of the table may be incompatible with the schema at the time of DataFrame creation. checkpoint.partSize \u00b6 spark.databricks.delta.checkpoint.partSize (internal) is the limit at which we will start parallelizing the checkpoint. We will attempt to write maximum of this many actions per checkpoint. Default: 5000000 commitInfo.enabled \u00b6 spark.databricks.delta.commitInfo.enabled controls whether to log commit information into a Delta log . Default: true commitInfo.userMetadata \u00b6 spark.databricks.delta.commitInfo.userMetadata is an arbitrary user-defined metadata to include in CommitInfo (requires spark.databricks.delta.commitInfo.enabled ). Default: (empty) commitValidation.enabled \u00b6 spark.databricks.delta.commitValidation.enabled (internal) controls whether to perform validation checks before commit or not Default: true convert.metadataCheck.enabled \u00b6 spark.databricks.delta.convert.metadataCheck.enabled enables validation during convert to delta, if there is a difference between the catalog table's properties and the Delta table's configuration, we should error. If disabled, merge the two configurations with the same semantics as update and merge Default: true dummyFileManager.numOfFiles \u00b6 spark.databricks.delta.dummyFileManager.numOfFiles (internal) controls how many dummy files to write in DummyFileManager Default: 3 dummyFileManager.prefix \u00b6 spark.databricks.delta.dummyFileManager.prefix (internal) is the file prefix to use in DummyFileManager Default: .s3-optimization- history.maxKeysPerList \u00b6 spark.databricks.delta.history.maxKeysPerList (internal) controls how many commits to list when performing a parallel search. The default is the maximum keys returned by S3 per list call. Azure can return 5000, therefore we choose 1000. Default: 1000 history.metricsEnabled \u00b6 spark.databricks.delta.history.metricsEnabled enables Metrics reporting in Describe History. CommitInfo will now record the Operation Metrics. Default: true Used when: OptimisticTransactionImpl is requested to getOperationMetrics ConvertToDeltaCommand is requested to streamWrite SQLMetricsReporting is requested to registerSQLMetrics TransactionalWrite is requested to writeFiles import.batchSize.schemaInference \u00b6 spark.databricks.delta.import.batchSize.schemaInference (internal) is the number of files per batch for schema inference during import . Default: 1000000 import.batchSize.statsCollection \u00b6 spark.databricks.delta.import.batchSize.statsCollection (internal) is the number of files per batch for stats collection during import . Default: 50000 maxSnapshotLineageLength \u00b6 spark.databricks.delta.maxSnapshotLineageLength (internal) is the maximum lineage length of a Snapshot before Delta forces to build a Snapshot from scratch Default: 50 merge.maxInsertCount \u00b6 spark.databricks.delta.merge.maxInsertCount (internal) is the maximum row count of inserts in each MERGE execution Default: 10000L merge.optimizeInsertOnlyMerge.enabled \u00b6 spark.databricks.delta.merge.optimizeInsertOnlyMerge.enabled (internal) controls merge without any matched clause (i.e., insert-only merge) will be optimized by avoiding rewriting old files and just inserting new files Default: true merge.optimizeMatchedOnlyMerge.enabled \u00b6 spark.databricks.delta.merge.optimizeMatchedOnlyMerge.enabled (internal) controls merge without 'when not matched' clause will be optimized to use a right outer join instead of a full outer join Default: true merge.repartitionBeforeWrite.enabled \u00b6 spark.databricks.delta.merge.repartitionBeforeWrite.enabled (internal) controls merge will repartition the output by the table's partition columns before writing the files Default: false partitionColumnValidity.enabled \u00b6 spark.databricks.delta.partitionColumnValidity.enabled (internal) enables validation of the partition column names (just like the data columns) Default: true retentionDurationCheck.enabled \u00b6 spark.databricks.delta.retentionDurationCheck.enabled adds a check preventing users from running vacuum with a very short retention period, which may end up corrupting a Delta log. Default: true sampling.enabled \u00b6 spark.databricks.delta.sampling.enabled (internal) enables sample-based estimation Default: false schema.autoMerge.enabled \u00b6 spark.databricks.delta.schema.autoMerge.enabled enables schema merging on appends and overwrites. Default: false Equivalent DataFrame option: mergeSchema snapshotIsolation.enabled \u00b6 spark.databricks.delta.snapshotIsolation.enabled (internal) controls whether queries on Delta tables are guaranteed to have snapshot isolation Default: true snapshotPartitions \u00b6 spark.databricks.delta.snapshotPartitions (internal) is the number of partitions to use for state reconstruction (when building a snapshot of a Delta table). Default: 50 stalenessLimit \u00b6 spark.databricks.delta.stalenessLimit (in millis) allows you to query the last loaded state of the Delta table without blocking on a table update. You can use this configuration to reduce the latency on queries when up-to-date results are not a requirement. Table updates will be scheduled on a separate scheduler pool in a FIFO queue, and will share cluster resources fairly with your query. If a table hasn't updated past this time limit, we will block on a synchronous state update before running the query. Default: 0 (no tables can be stale) state.corruptionIsFatal \u00b6 spark.databricks.delta.state.corruptionIsFatal (internal) throws a fatal error when the recreated Delta State doesn't match committed checksum file Default: true stateReconstructionValidation.enabled \u00b6 spark.databricks.delta.stateReconstructionValidation.enabled (internal) controls whether to perform validation checks on the reconstructed state Default: true stats.collect \u00b6 spark.databricks.delta.stats.collect (internal) enables statistics to be collected while writing files into a Delta table Default: true stats.limitPushdown.enabled \u00b6 spark.databricks.delta.stats.limitPushdown.enabled (internal) enables using the limit clause and file statistics to prune files before they are collected to the driver Default: true stats.localCache.maxNumFiles \u00b6 spark.databricks.delta.stats.localCache.maxNumFiles (internal) is the maximum number of files for a table to be considered a delta small table . Some metadata operations (such as using data skipping) are optimized for small tables using driver local caching and local execution. Default: 2000 stats.skipping \u00b6 spark.databricks.delta.stats.skipping (internal) enables statistics used for skipping Default: true timeTravel.resolveOnIdentifier.enabled \u00b6 spark.databricks.delta.timeTravel.resolveOnIdentifier.enabled (internal) controls whether to resolve patterns as @v123 in identifiers as time travel nodes. Default: true","title":"Configuration Properties"},{"location":"DeltaSQLConf/#deltasqlconf-sparkdatabricksdelta-configuration-properties","text":"DeltaSQLConf contains spark.databricks.delta -prefixed configuration properties to configure behaviour of Delta Lake.","title":"DeltaSQLConf &mdash; spark.databricks.delta Configuration Properties"},{"location":"DeltaSQLConf/#alterlocationbypassschemacheck","text":"spark.databricks.delta.alterLocation.bypassSchemaCheck enables Alter Table Set Location on Delta to go through even if the Delta table in the new location has a different schema from the original Delta table Default: false","title":" alterLocation.bypassSchemaCheck"},{"location":"DeltaSQLConf/#checklatestschemaonread","text":"spark.databricks.delta.checkLatestSchemaOnRead (internal) enables a check that ensures that users won't read corrupt data if the source schema changes in an incompatible way. Default: true In Delta, we always try to give users the latest version of their data without having to call REFRESH TABLE or redefine their DataFrames when used in the context of streaming. There is a possibility that the schema of the latest version of the table may be incompatible with the schema at the time of DataFrame creation.","title":" checkLatestSchemaOnRead"},{"location":"DeltaSQLConf/#checkpointpartsize","text":"spark.databricks.delta.checkpoint.partSize (internal) is the limit at which we will start parallelizing the checkpoint. We will attempt to write maximum of this many actions per checkpoint. Default: 5000000","title":" checkpoint.partSize"},{"location":"DeltaSQLConf/#commitinfoenabled","text":"spark.databricks.delta.commitInfo.enabled controls whether to log commit information into a Delta log . Default: true","title":" commitInfo.enabled"},{"location":"DeltaSQLConf/#commitinfousermetadata","text":"spark.databricks.delta.commitInfo.userMetadata is an arbitrary user-defined metadata to include in CommitInfo (requires spark.databricks.delta.commitInfo.enabled ). Default: (empty)","title":" commitInfo.userMetadata"},{"location":"DeltaSQLConf/#commitvalidationenabled","text":"spark.databricks.delta.commitValidation.enabled (internal) controls whether to perform validation checks before commit or not Default: true","title":" commitValidation.enabled"},{"location":"DeltaSQLConf/#convertmetadatacheckenabled","text":"spark.databricks.delta.convert.metadataCheck.enabled enables validation during convert to delta, if there is a difference between the catalog table's properties and the Delta table's configuration, we should error. If disabled, merge the two configurations with the same semantics as update and merge Default: true","title":" convert.metadataCheck.enabled"},{"location":"DeltaSQLConf/#dummyfilemanagernumoffiles","text":"spark.databricks.delta.dummyFileManager.numOfFiles (internal) controls how many dummy files to write in DummyFileManager Default: 3","title":" dummyFileManager.numOfFiles"},{"location":"DeltaSQLConf/#dummyfilemanagerprefix","text":"spark.databricks.delta.dummyFileManager.prefix (internal) is the file prefix to use in DummyFileManager Default: .s3-optimization-","title":" dummyFileManager.prefix"},{"location":"DeltaSQLConf/#historymaxkeysperlist","text":"spark.databricks.delta.history.maxKeysPerList (internal) controls how many commits to list when performing a parallel search. The default is the maximum keys returned by S3 per list call. Azure can return 5000, therefore we choose 1000. Default: 1000","title":" history.maxKeysPerList"},{"location":"DeltaSQLConf/#historymetricsenabled","text":"spark.databricks.delta.history.metricsEnabled enables Metrics reporting in Describe History. CommitInfo will now record the Operation Metrics. Default: true Used when: OptimisticTransactionImpl is requested to getOperationMetrics ConvertToDeltaCommand is requested to streamWrite SQLMetricsReporting is requested to registerSQLMetrics TransactionalWrite is requested to writeFiles","title":" history.metricsEnabled"},{"location":"DeltaSQLConf/#importbatchsizeschemainference","text":"spark.databricks.delta.import.batchSize.schemaInference (internal) is the number of files per batch for schema inference during import . Default: 1000000","title":" import.batchSize.schemaInference"},{"location":"DeltaSQLConf/#importbatchsizestatscollection","text":"spark.databricks.delta.import.batchSize.statsCollection (internal) is the number of files per batch for stats collection during import . Default: 50000","title":" import.batchSize.statsCollection"},{"location":"DeltaSQLConf/#maxsnapshotlineagelength","text":"spark.databricks.delta.maxSnapshotLineageLength (internal) is the maximum lineage length of a Snapshot before Delta forces to build a Snapshot from scratch Default: 50","title":" maxSnapshotLineageLength"},{"location":"DeltaSQLConf/#mergemaxinsertcount","text":"spark.databricks.delta.merge.maxInsertCount (internal) is the maximum row count of inserts in each MERGE execution Default: 10000L","title":" merge.maxInsertCount"},{"location":"DeltaSQLConf/#mergeoptimizeinsertonlymergeenabled","text":"spark.databricks.delta.merge.optimizeInsertOnlyMerge.enabled (internal) controls merge without any matched clause (i.e., insert-only merge) will be optimized by avoiding rewriting old files and just inserting new files Default: true","title":" merge.optimizeInsertOnlyMerge.enabled"},{"location":"DeltaSQLConf/#mergeoptimizematchedonlymergeenabled","text":"spark.databricks.delta.merge.optimizeMatchedOnlyMerge.enabled (internal) controls merge without 'when not matched' clause will be optimized to use a right outer join instead of a full outer join Default: true","title":" merge.optimizeMatchedOnlyMerge.enabled"},{"location":"DeltaSQLConf/#mergerepartitionbeforewriteenabled","text":"spark.databricks.delta.merge.repartitionBeforeWrite.enabled (internal) controls merge will repartition the output by the table's partition columns before writing the files Default: false","title":" merge.repartitionBeforeWrite.enabled"},{"location":"DeltaSQLConf/#partitioncolumnvalidityenabled","text":"spark.databricks.delta.partitionColumnValidity.enabled (internal) enables validation of the partition column names (just like the data columns) Default: true","title":" partitionColumnValidity.enabled"},{"location":"DeltaSQLConf/#retentiondurationcheckenabled","text":"spark.databricks.delta.retentionDurationCheck.enabled adds a check preventing users from running vacuum with a very short retention period, which may end up corrupting a Delta log. Default: true","title":" retentionDurationCheck.enabled"},{"location":"DeltaSQLConf/#samplingenabled","text":"spark.databricks.delta.sampling.enabled (internal) enables sample-based estimation Default: false","title":" sampling.enabled"},{"location":"DeltaSQLConf/#schemaautomergeenabled","text":"spark.databricks.delta.schema.autoMerge.enabled enables schema merging on appends and overwrites. Default: false Equivalent DataFrame option: mergeSchema","title":" schema.autoMerge.enabled"},{"location":"DeltaSQLConf/#snapshotisolationenabled","text":"spark.databricks.delta.snapshotIsolation.enabled (internal) controls whether queries on Delta tables are guaranteed to have snapshot isolation Default: true","title":" snapshotIsolation.enabled"},{"location":"DeltaSQLConf/#snapshotpartitions","text":"spark.databricks.delta.snapshotPartitions (internal) is the number of partitions to use for state reconstruction (when building a snapshot of a Delta table). Default: 50","title":" snapshotPartitions"},{"location":"DeltaSQLConf/#stalenesslimit","text":"spark.databricks.delta.stalenessLimit (in millis) allows you to query the last loaded state of the Delta table without blocking on a table update. You can use this configuration to reduce the latency on queries when up-to-date results are not a requirement. Table updates will be scheduled on a separate scheduler pool in a FIFO queue, and will share cluster resources fairly with your query. If a table hasn't updated past this time limit, we will block on a synchronous state update before running the query. Default: 0 (no tables can be stale)","title":" stalenessLimit"},{"location":"DeltaSQLConf/#statecorruptionisfatal","text":"spark.databricks.delta.state.corruptionIsFatal (internal) throws a fatal error when the recreated Delta State doesn't match committed checksum file Default: true","title":" state.corruptionIsFatal"},{"location":"DeltaSQLConf/#statereconstructionvalidationenabled","text":"spark.databricks.delta.stateReconstructionValidation.enabled (internal) controls whether to perform validation checks on the reconstructed state Default: true","title":" stateReconstructionValidation.enabled"},{"location":"DeltaSQLConf/#statscollect","text":"spark.databricks.delta.stats.collect (internal) enables statistics to be collected while writing files into a Delta table Default: true","title":" stats.collect"},{"location":"DeltaSQLConf/#statslimitpushdownenabled","text":"spark.databricks.delta.stats.limitPushdown.enabled (internal) enables using the limit clause and file statistics to prune files before they are collected to the driver Default: true","title":" stats.limitPushdown.enabled"},{"location":"DeltaSQLConf/#statslocalcachemaxnumfiles","text":"spark.databricks.delta.stats.localCache.maxNumFiles (internal) is the maximum number of files for a table to be considered a delta small table . Some metadata operations (such as using data skipping) are optimized for small tables using driver local caching and local execution. Default: 2000","title":" stats.localCache.maxNumFiles"},{"location":"DeltaSQLConf/#statsskipping","text":"spark.databricks.delta.stats.skipping (internal) enables statistics used for skipping Default: true","title":" stats.skipping"},{"location":"DeltaSQLConf/#timetravelresolveonidentifierenabled","text":"spark.databricks.delta.timeTravel.resolveOnIdentifier.enabled (internal) controls whether to resolve patterns as @v123 in identifiers as time travel nodes. Default: true","title":" timeTravel.resolveOnIdentifier.enabled"},{"location":"DeltaSink/","text":"= DeltaSink DeltaSink is the sink of < > for streaming queries in Spark Structured Streaming. TIP: Read up on https://jaceklaskowski.gitbooks.io/spark-structured-streaming/spark-sql-streaming-Sink.html[Streaming Sink] in https://bit.ly/spark-structured-streaming[The Internals of Spark Structured Streaming] online book. DeltaSink is < > exclusively when DeltaDataSource is requested for a < > (Structured Streaming). [[toString]] DeltaSink uses the following text representation (with the < >): DeltaSink[path] [[ImplicitMetadataOperation]] DeltaSink is an < > of a < >. == [[creating-instance]] Creating Instance DeltaSink takes the following to be created: [[sqlContext]] SQLContext [[path]] Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path ] of the delta table (to < > as configured by the < > option) [[partitionColumns]] Names of the partition columns ( Seq[String] ) [[outputMode]] OutputMode [[options]] < > == [[deltaLog]] deltaLog Internal Property [source, scala] \u00b6 deltaLog: DeltaLog \u00b6 deltaLog is a < > that is < > for the < > when DeltaSink is created (when DeltaDataSource is requested for a < >). deltaLog is used exclusively when DeltaSink is requested to < >. == [[addBatch]] Adding Streaming Micro-Batch [source, scala] \u00b6 addBatch( batchId: Long, data: DataFrame): Unit NOTE: addBatch is part of the Sink contract (in Spark Structured Streaming) to add a batch of data to the sink. addBatch requests the < > to < >. addBatch ...FIXME In the end, addBatch requests the OptimisticTransaction to < >.","title":"DeltaSink"},{"location":"DeltaSink/#source-scala","text":"","title":"[source, scala]"},{"location":"DeltaSink/#deltalog-deltalog","text":"deltaLog is a < > that is < > for the < > when DeltaSink is created (when DeltaDataSource is requested for a < >). deltaLog is used exclusively when DeltaSink is requested to < >. == [[addBatch]] Adding Streaming Micro-Batch","title":"deltaLog: DeltaLog"},{"location":"DeltaSink/#source-scala_1","text":"addBatch( batchId: Long, data: DataFrame): Unit NOTE: addBatch is part of the Sink contract (in Spark Structured Streaming) to add a batch of data to the sink. addBatch requests the < > to < >. addBatch ...FIXME In the end, addBatch requests the OptimisticTransaction to < >.","title":"[source, scala]"},{"location":"DeltaSource/","text":"DeltaSource \u2014 Streaming Source of Delta Data Source \u00b6 DeltaSource is the streaming source of < > for streaming queries in Spark Structured Streaming. TIP: Read up on https://jaceklaskowski.gitbooks.io/spark-structured-streaming/spark-sql-streaming-Source.html[Streaming Source] in https://bit.ly/spark-structured-streaming[The Internals of Spark Structured Streaming] online book. DeltaSource is < > when DeltaDataSource is requested for a < >. val q = spark .readStream // Creating a streaming query .format(\"delta\") // Using delta data source .load(\"/tmp/delta/users\") // Over data in a delta table .writeStream .format(\"memory\") .option(\"queryName\", \"demo\") .start import org.apache.spark.sql.execution.streaming.{MicroBatchExecution, StreamingQueryWrapper} val plan = q.asInstanceOf[StreamingQueryWrapper] .streamingQuery .asInstanceOf[MicroBatchExecution] .logicalPlan import org.apache.spark.sql.execution.streaming.StreamingExecutionRelation val relation = plan.collect { case r: StreamingExecutionRelation => r }.head import org.apache.spark.sql.delta.sources.DeltaSource assert(relation.source.asInstanceOf[DeltaSource]) scala> println(relation.source) DeltaSource[file:/tmp/delta/users] [[maxFilesPerTrigger]] DeltaSource uses < > option to limit the number of files to process when requested for the < >. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.delta.sources.DeltaSource logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.sources.DeltaSource=ALL Refer to Logging . \u00b6 == [[creating-instance]] Creating DeltaSource Instance DeltaSource takes the following to be created: [[spark]] SparkSession [[deltaLog]] < > of the delta table to read data (as < >) from [[options]] < > [[filters]] Filter expressions (default: no filters) DeltaSource initializes the < >. == [[getBatch]] Micro-Batch With Data Between Start And End Offsets (Streaming DataFrame) -- getBatch Method [source, scala] \u00b6 getBatch( start: Option[Offset], end: Offset): DataFrame NOTE: getBatch is part of the Source contract ( https://jaceklaskowski.gitbooks.io/spark-structured-streaming/spark-sql-streaming-Source.html[Spark Structured Streaming]) for a streaming DataFrame with data between the start and end offsets. getBatch creates an < > for the < > (aka < >) and the given end offset. getBatch < > as follows...FIXME == [[getOffset]] Latest Available Offset -- getOffset Method [source, scala] \u00b6 getOffset: Option[Offset] \u00b6 NOTE: getOffset is part of the Source abstraction ( https://jaceklaskowski.gitbooks.io/spark-structured-streaming/spark-sql-streaming-Source.html[Spark Structured Streaming]) for the latest available offset of this streaming source. [[getOffset-currentOffset]] getOffset calculates the latest offset (that a streaming query can use for the data of the next micro-batch) based on the < > internal registry. For no < >, getOffset simply < > (based on the latest version of the delta table). When the < > is defined (which means that the DeltaSource is requested for another micro-batch), getOffset takes the < > from < > for the < >. getOffset returns the < > when the last element was not available. With the < > and the < > both available, getOffset creates a new < > for the version, index, and isLast flag from the last indexed < >. [NOTE] \u00b6 isStartingVersion local value is enabled ( true ) when the following holds: Version of the last indexed < > is equal to the < > of the < > * < > flag of the < > is enabled ( true ) \u00b6 In the end, getOffset prints out the following DEBUG message to the logs (using the < > internal registry): previousOffset -> currentOffset: [previousOffset] -> [currentOffset] == [[stop]] Stopping -- stop Method [source, scala] \u00b6 stop(): Unit \u00b6 NOTE: stop is part of the streaming Source contract ( https://jaceklaskowski.gitbooks.io/spark-structured-streaming/spark-sql-streaming-Source.html[Spark Structured Streaming]) to stop this source and free up any resources allocated. stop simply < >. == [[getStartingOffset]] Retrieving Starting Offset -- getStartingOffset Internal Method [source, scala] \u00b6 getStartingOffset(): Option[Offset] \u00b6 getStartingOffset requests the < > for the version of the delta table (by requesting for the < > and then for the < >). getStartingOffset < > from the < > for the version of the delta table, -1L as the fromIndex , and the isStartingVersion flag enabled ( true ). getStartingOffset returns a new < > for the < >, the version and the index of the last file added, and whether the last file added is the last file of its version. getStartingOffset returns None ( offset not available ) when either happens: the version of the delta table is negative (below 0 ) no files were added in the version getStartingOffset throws an AssertionError when the version of the last file added is smaller than the delta table's version: assertion failed: getChangesWithRateLimit returns an invalid version: [v] (expected: >= [version]) NOTE: getStartingOffset is used exclusively when DeltaSource is requested for the < >. == [[getChanges]] getChanges Internal Method [source, scala] \u00b6 getChanges( fromVersion: Long, fromIndex: Long, isStartingVersion: Boolean): Iterator[IndexedFile] getChanges branches per the given isStartingVersion flag (enabled or not): For isStartingVersion flag enabled ( true ), getChanges < > for the given fromVersion followed by < > for the next version after the given fromVersion For isStartingVersion flag disabled ( false ), getChanges simply gives < > for the given fromVersion [NOTE] \u00b6 isStartingVersion flag simply adds < > before < > when enabled ( true ). isStartingVersion flag is enabled when DeltaSource is requested for the following: < > and the start offset is not given or is for the < > * < > with no < > or the < > for the < > \u00b6 In the end, getChanges filters out ( excludes ) indexed < > that are not with the version later than the given fromVersion or the index greater than the given fromIndex . NOTE: getChanges is used when DeltaSource is requested for the < > (when requested for the < >) and < >. === [[getChanges-filterAndIndexDeltaLogs]] filterAndIndexDeltaLogs Internal Method [source, scala] \u00b6 filterAndIndexDeltaLogs( startVersion: Long): Iterator[IndexedFile] filterAndIndexDeltaLogs ...FIXME == [[getChangesWithRateLimit]] Retrieving File Additions (With Rate Limit) -- getChangesWithRateLimit Internal Method [source, scala] \u00b6 getChangesWithRateLimit( fromVersion: Long, fromIndex: Long, isStartingVersion: Boolean): Iterator[IndexedFile] getChangesWithRateLimit < > (as indexed < >) for the given fromVersion , fromIndex , and isStartingVersion flag. getChangesWithRateLimit takes the configured number of AddFiles (up to the < > option (if defined) or < >). NOTE: getChangesWithRateLimit is used when DeltaSource is requested for the < >. == [[getSnapshotAt]] Retrieving State Of Delta Table At Given Version -- getSnapshotAt Internal Method [source, scala] \u00b6 getSnapshotAt( version: Long): Iterator[IndexedFile] getSnapshotAt requests the < > for the < > (as indexed < >). In case the < > hasn't been initialized yet ( null ) or the requested version is different from the < >, getSnapshotAt does the following: . < > . Requests the < > for the < > at the version . Creates a new < > for the state (snapshot) as the current < > . Changes the < > internal registry to the requested version NOTE: getSnapshotAt is used when DeltaSource is requested to < > (with isStartingVersion flag enabled). == [[verifyStreamHygieneAndFilterAddFiles]] verifyStreamHygieneAndFilterAddFiles Internal Method [source, scala] \u00b6 verifyStreamHygieneAndFilterAddFiles( actions: Seq[Action]): Seq[Action] verifyStreamHygieneAndFilterAddFiles ...FIXME NOTE: verifyStreamHygieneAndFilterAddFiles is used when DeltaSource is requested to < >. == [[cleanUpSnapshotResources]] cleanUpSnapshotResources Internal Method [source, scala] \u00b6 cleanUpSnapshotResources(): Unit \u00b6 cleanUpSnapshotResources ...FIXME NOTE: cleanUpSnapshotResources is used when DeltaSource is requested to < >, < > and < >. == [[iteratorLast]] Retrieving Last Element From Iterator -- iteratorLast Internal Method [source, scala] \u00b6 iteratorLast T : Option[T] iteratorLast simply returns the last element in the given Iterator or None . NOTE: iteratorLast is used when DeltaSource is requested to < > and < >. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | initialState a| [[initialState]] < > Initially uninitialized ( null ). Changes (along with the < >) when DeltaSource is requested for the < > (only when the versions are different) Used when DeltaSource is requested for the < > Closed and dereferenced ( null ) when DeltaSource is requested to < > | initialStateVersion a| [[initialStateVersion]] Version of the < > Initially -1L and changes (along with the < >) to the version requested when DeltaSource is requested for the < > (only when the versions are different) Used when DeltaSource is requested to < > (and unpersist the current snapshot) | previousOffset a| [[previousOffset]] Ending < > of the latest < > Starts uninitialized ( null ). Used when DeltaSource is requested for the < >. | tableId a| [[tableId]] Table ID Used when...FIXME |===","title":"DeltaSource"},{"location":"DeltaSource/#deltasource-streaming-source-of-delta-data-source","text":"DeltaSource is the streaming source of < > for streaming queries in Spark Structured Streaming. TIP: Read up on https://jaceklaskowski.gitbooks.io/spark-structured-streaming/spark-sql-streaming-Source.html[Streaming Source] in https://bit.ly/spark-structured-streaming[The Internals of Spark Structured Streaming] online book. DeltaSource is < > when DeltaDataSource is requested for a < >. val q = spark .readStream // Creating a streaming query .format(\"delta\") // Using delta data source .load(\"/tmp/delta/users\") // Over data in a delta table .writeStream .format(\"memory\") .option(\"queryName\", \"demo\") .start import org.apache.spark.sql.execution.streaming.{MicroBatchExecution, StreamingQueryWrapper} val plan = q.asInstanceOf[StreamingQueryWrapper] .streamingQuery .asInstanceOf[MicroBatchExecution] .logicalPlan import org.apache.spark.sql.execution.streaming.StreamingExecutionRelation val relation = plan.collect { case r: StreamingExecutionRelation => r }.head import org.apache.spark.sql.delta.sources.DeltaSource assert(relation.source.asInstanceOf[DeltaSource]) scala> println(relation.source) DeltaSource[file:/tmp/delta/users] [[maxFilesPerTrigger]] DeltaSource uses < > option to limit the number of files to process when requested for the < >. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.delta.sources.DeltaSource logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.sources.DeltaSource=ALL","title":"DeltaSource &mdash; Streaming Source of Delta Data Source"},{"location":"DeltaSource/#refer-to-logging","text":"== [[creating-instance]] Creating DeltaSource Instance DeltaSource takes the following to be created: [[spark]] SparkSession [[deltaLog]] < > of the delta table to read data (as < >) from [[options]] < > [[filters]] Filter expressions (default: no filters) DeltaSource initializes the < >. == [[getBatch]] Micro-Batch With Data Between Start And End Offsets (Streaming DataFrame) -- getBatch Method","title":"Refer to Logging."},{"location":"DeltaSource/#source-scala","text":"getBatch( start: Option[Offset], end: Offset): DataFrame NOTE: getBatch is part of the Source contract ( https://jaceklaskowski.gitbooks.io/spark-structured-streaming/spark-sql-streaming-Source.html[Spark Structured Streaming]) for a streaming DataFrame with data between the start and end offsets. getBatch creates an < > for the < > (aka < >) and the given end offset. getBatch < > as follows...FIXME == [[getOffset]] Latest Available Offset -- getOffset Method","title":"[source, scala]"},{"location":"DeltaSource/#source-scala_1","text":"","title":"[source, scala]"},{"location":"DeltaSource/#getoffset-optionoffset","text":"NOTE: getOffset is part of the Source abstraction ( https://jaceklaskowski.gitbooks.io/spark-structured-streaming/spark-sql-streaming-Source.html[Spark Structured Streaming]) for the latest available offset of this streaming source. [[getOffset-currentOffset]] getOffset calculates the latest offset (that a streaming query can use for the data of the next micro-batch) based on the < > internal registry. For no < >, getOffset simply < > (based on the latest version of the delta table). When the < > is defined (which means that the DeltaSource is requested for another micro-batch), getOffset takes the < > from < > for the < >. getOffset returns the < > when the last element was not available. With the < > and the < > both available, getOffset creates a new < > for the version, index, and isLast flag from the last indexed < >.","title":"getOffset: Option[Offset]"},{"location":"DeltaSource/#note","text":"isStartingVersion local value is enabled ( true ) when the following holds: Version of the last indexed < > is equal to the < > of the < >","title":"[NOTE]"},{"location":"DeltaSource/#flag-of-the-is-enabled-true","text":"In the end, getOffset prints out the following DEBUG message to the logs (using the < > internal registry): previousOffset -> currentOffset: [previousOffset] -> [currentOffset] == [[stop]] Stopping -- stop Method","title":"* &lt;&gt; flag of the &lt;&gt; is enabled (true)"},{"location":"DeltaSource/#source-scala_2","text":"","title":"[source, scala]"},{"location":"DeltaSource/#stop-unit","text":"NOTE: stop is part of the streaming Source contract ( https://jaceklaskowski.gitbooks.io/spark-structured-streaming/spark-sql-streaming-Source.html[Spark Structured Streaming]) to stop this source and free up any resources allocated. stop simply < >. == [[getStartingOffset]] Retrieving Starting Offset -- getStartingOffset Internal Method","title":"stop(): Unit"},{"location":"DeltaSource/#source-scala_3","text":"","title":"[source, scala]"},{"location":"DeltaSource/#getstartingoffset-optionoffset","text":"getStartingOffset requests the < > for the version of the delta table (by requesting for the < > and then for the < >). getStartingOffset < > from the < > for the version of the delta table, -1L as the fromIndex , and the isStartingVersion flag enabled ( true ). getStartingOffset returns a new < > for the < >, the version and the index of the last file added, and whether the last file added is the last file of its version. getStartingOffset returns None ( offset not available ) when either happens: the version of the delta table is negative (below 0 ) no files were added in the version getStartingOffset throws an AssertionError when the version of the last file added is smaller than the delta table's version: assertion failed: getChangesWithRateLimit returns an invalid version: [v] (expected: >= [version]) NOTE: getStartingOffset is used exclusively when DeltaSource is requested for the < >. == [[getChanges]] getChanges Internal Method","title":"getStartingOffset(): Option[Offset]"},{"location":"DeltaSource/#source-scala_4","text":"getChanges( fromVersion: Long, fromIndex: Long, isStartingVersion: Boolean): Iterator[IndexedFile] getChanges branches per the given isStartingVersion flag (enabled or not): For isStartingVersion flag enabled ( true ), getChanges < > for the given fromVersion followed by < > for the next version after the given fromVersion For isStartingVersion flag disabled ( false ), getChanges simply gives < > for the given fromVersion","title":"[source, scala]"},{"location":"DeltaSource/#note_1","text":"isStartingVersion flag simply adds < > before < > when enabled ( true ). isStartingVersion flag is enabled when DeltaSource is requested for the following: < > and the start offset is not given or is for the < >","title":"[NOTE]"},{"location":"DeltaSource/#with-no-or-the-for-the","text":"In the end, getChanges filters out ( excludes ) indexed < > that are not with the version later than the given fromVersion or the index greater than the given fromIndex . NOTE: getChanges is used when DeltaSource is requested for the < > (when requested for the < >) and < >. === [[getChanges-filterAndIndexDeltaLogs]] filterAndIndexDeltaLogs Internal Method","title":"* &lt;&gt; with no &lt;&gt; or the &lt;&gt; for the &lt;&gt;"},{"location":"DeltaSource/#source-scala_5","text":"filterAndIndexDeltaLogs( startVersion: Long): Iterator[IndexedFile] filterAndIndexDeltaLogs ...FIXME == [[getChangesWithRateLimit]] Retrieving File Additions (With Rate Limit) -- getChangesWithRateLimit Internal Method","title":"[source, scala]"},{"location":"DeltaSource/#source-scala_6","text":"getChangesWithRateLimit( fromVersion: Long, fromIndex: Long, isStartingVersion: Boolean): Iterator[IndexedFile] getChangesWithRateLimit < > (as indexed < >) for the given fromVersion , fromIndex , and isStartingVersion flag. getChangesWithRateLimit takes the configured number of AddFiles (up to the < > option (if defined) or < >). NOTE: getChangesWithRateLimit is used when DeltaSource is requested for the < >. == [[getSnapshotAt]] Retrieving State Of Delta Table At Given Version -- getSnapshotAt Internal Method","title":"[source, scala]"},{"location":"DeltaSource/#source-scala_7","text":"getSnapshotAt( version: Long): Iterator[IndexedFile] getSnapshotAt requests the < > for the < > (as indexed < >). In case the < > hasn't been initialized yet ( null ) or the requested version is different from the < >, getSnapshotAt does the following: . < > . Requests the < > for the < > at the version . Creates a new < > for the state (snapshot) as the current < > . Changes the < > internal registry to the requested version NOTE: getSnapshotAt is used when DeltaSource is requested to < > (with isStartingVersion flag enabled). == [[verifyStreamHygieneAndFilterAddFiles]] verifyStreamHygieneAndFilterAddFiles Internal Method","title":"[source, scala]"},{"location":"DeltaSource/#source-scala_8","text":"verifyStreamHygieneAndFilterAddFiles( actions: Seq[Action]): Seq[Action] verifyStreamHygieneAndFilterAddFiles ...FIXME NOTE: verifyStreamHygieneAndFilterAddFiles is used when DeltaSource is requested to < >. == [[cleanUpSnapshotResources]] cleanUpSnapshotResources Internal Method","title":"[source, scala]"},{"location":"DeltaSource/#source-scala_9","text":"","title":"[source, scala]"},{"location":"DeltaSource/#cleanupsnapshotresources-unit","text":"cleanUpSnapshotResources ...FIXME NOTE: cleanUpSnapshotResources is used when DeltaSource is requested to < >, < > and < >. == [[iteratorLast]] Retrieving Last Element From Iterator -- iteratorLast Internal Method","title":"cleanUpSnapshotResources(): Unit"},{"location":"DeltaSource/#source-scala_10","text":"iteratorLast T : Option[T] iteratorLast simply returns the last element in the given Iterator or None . NOTE: iteratorLast is used when DeltaSource is requested to < > and < >. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | initialState a| [[initialState]] < > Initially uninitialized ( null ). Changes (along with the < >) when DeltaSource is requested for the < > (only when the versions are different) Used when DeltaSource is requested for the < > Closed and dereferenced ( null ) when DeltaSource is requested to < > | initialStateVersion a| [[initialStateVersion]] Version of the < > Initially -1L and changes (along with the < >) to the version requested when DeltaSource is requested for the < > (only when the versions are different) Used when DeltaSource is requested to < > (and unpersist the current snapshot) | previousOffset a| [[previousOffset]] Ending < > of the latest < > Starts uninitialized ( null ). Used when DeltaSource is requested for the < >. | tableId a| [[tableId]] Table ID Used when...FIXME |===","title":"[source, scala]"},{"location":"DeltaSourceOffset/","text":"= DeltaSourceOffset -- Streaming Offset Of DeltaSource DeltaSourceOffset is a streaming Offset for DeltaSource.md[DeltaSource]. TIP: Read up on https://jaceklaskowski.gitbooks.io/spark-structured-streaming/spark-sql-streaming-Offset.html[Offset ] in https://bit.ly/spark-structured-streaming[The Internals of Spark Structured Streaming] online book. DeltaSourceOffset is < > (via < > utility) when DeltaSource is requested for the DeltaSource.md#getOffset[latest offset] and a DeltaSource.md#getBatch[batch (for the given starting and ending offsets)]. [[VERSION]] DeltaSourceOffset uses the version 1 . == [[creating-instance]] Creating DeltaSourceOffset Instance DeltaSourceOffset takes the following to be created: [[sourceVersion]] Source Version (always < >) [[reservoirId]] Reservoir ID (aka DeltaSource.md#tableId[Table ID]) [[reservoirVersion]] Reservoir Version [[index]] Index [[isStartingVersion]] isStartingVersion flag == [[apply]] Creating DeltaSourceOffset Instance -- apply Utility [source, scala] \u00b6 apply( reservoirId: String, reservoirVersion: Long, index: Long, isStartingVersion: Boolean): DeltaSourceOffset apply( reservoirId: String, offset: Offset): DeltaSourceOffset apply creates a new DeltaSourceOffset (for the < > and the given arguments) or converts the given Offset to a DeltaSourceOffset . NOTE: apply is used when DeltaSource is requested for the DeltaSource.md#getOffset[latest offset] and a DeltaSource.md#getBatch[batch (for the given starting and ending offsets)]. == [[json]] json Method [source, scala] \u00b6 json: String \u00b6 NOTE: json is part of the Offset contract to serialize an offset to JSON. json ...FIXME == [[validateSourceVersion]] validateSourceVersion Internal Utility [source, scala] \u00b6 validateSourceVersion( json: String): Unit validateSourceVersion ...FIXME NOTE: validateSourceVersion is used when...FIXME","title":"DeltaSourceOffset"},{"location":"DeltaSourceOffset/#source-scala","text":"apply( reservoirId: String, reservoirVersion: Long, index: Long, isStartingVersion: Boolean): DeltaSourceOffset apply( reservoirId: String, offset: Offset): DeltaSourceOffset apply creates a new DeltaSourceOffset (for the < > and the given arguments) or converts the given Offset to a DeltaSourceOffset . NOTE: apply is used when DeltaSource is requested for the DeltaSource.md#getOffset[latest offset] and a DeltaSource.md#getBatch[batch (for the given starting and ending offsets)]. == [[json]] json Method","title":"[source, scala]"},{"location":"DeltaSourceOffset/#source-scala_1","text":"","title":"[source, scala]"},{"location":"DeltaSourceOffset/#json-string","text":"NOTE: json is part of the Offset contract to serialize an offset to JSON. json ...FIXME == [[validateSourceVersion]] validateSourceVersion Internal Utility","title":"json: String"},{"location":"DeltaSourceOffset/#source-scala_2","text":"validateSourceVersion( json: String): Unit validateSourceVersion ...FIXME NOTE: validateSourceVersion is used when...FIXME","title":"[source, scala]"},{"location":"DeltaSourceSnapshot/","text":"= DeltaSourceSnapshot [[SnapshotIterator]][[StateCache]] DeltaSourceSnapshot is a < > with < > DeltaSourceSnapshot is < > when DeltaSource is requested for the < >. [[version]] When < >, DeltaSourceSnapshot requests the < > for the < > that it uses for the < > (a new column and the name of the cached RDD). == [[creating-instance]] Creating DeltaSourceSnapshot Instance DeltaSourceSnapshot takes the following to be created: [[spark]] SparkSession [[snapshot]] < > [[filters]] Filter expressions ( Seq[Expression] ) == [[initialFiles]] Initial Files (Indexed AddFiles) -- initialFiles Method [source, scala] \u00b6 initialFiles: Dataset[IndexedFile] \u00b6 initialFiles requests the < > for < > ( Dataset[AddFile] ) and sorts them by < > and < > in ascending order. initialFiles zips the < > with indices (using RDD.zipWithIndex operator), adds two new columns with the < > and isLast as false , and finally creates a Dataset[IndexedFile] . In the end, initialFiles < > with the following name (with the < > and the < > of the < >) Delta Source Snapshot #[version] - [redactedPath] NOTE: initialFiles is used exclusively when SnapshotIterator is requested for a < >.","title":"DeltaSourceSnapshot"},{"location":"DeltaSourceSnapshot/#source-scala","text":"","title":"[source, scala]"},{"location":"DeltaSourceSnapshot/#initialfiles-datasetindexedfile","text":"initialFiles requests the < > for < > ( Dataset[AddFile] ) and sorts them by < > and < > in ascending order. initialFiles zips the < > with indices (using RDD.zipWithIndex operator), adds two new columns with the < > and isLast as false , and finally creates a Dataset[IndexedFile] . In the end, initialFiles < > with the following name (with the < > and the < > of the < >) Delta Source Snapshot #[version] - [redactedPath] NOTE: initialFiles is used exclusively when SnapshotIterator is requested for a < >.","title":"initialFiles: Dataset[IndexedFile]"},{"location":"DeltaSparkSessionExtension/","text":"DeltaSparkSessionExtension \u00b6 DeltaSparkSessionExtension is used to register ( inject ) the following extensions: Delta SQL support (using DeltaSqlParser ) DeltaAnalysis logical resolution rule DeltaUnsupportedOperationsCheck PreprocessTableUpdate logical resolution rule PreprocessTableMerge logical resolution rule PreprocessTableDelete logical resolution rule DeltaSparkSessionExtension is registered using spark.sql.extensions configuration property (while creating a SparkSession in a Spark application).","title":"DeltaSparkSessionExtension"},{"location":"DeltaSparkSessionExtension/#deltasparksessionextension","text":"DeltaSparkSessionExtension is used to register ( inject ) the following extensions: Delta SQL support (using DeltaSqlParser ) DeltaAnalysis logical resolution rule DeltaUnsupportedOperationsCheck PreprocessTableUpdate logical resolution rule PreprocessTableMerge logical resolution rule PreprocessTableDelete logical resolution rule DeltaSparkSessionExtension is registered using spark.sql.extensions configuration property (while creating a SparkSession in a Spark application).","title":"DeltaSparkSessionExtension"},{"location":"DeltaTable/","text":"DeltaTable \u00b6 DeltaTable is the management interface of a Delta table (with the Delta DML Operations ). DeltaTable instances are created using utilities (e.g. DeltaTable.forName , DeltaTable.convertToDelta ). Utilities (Static Methods) \u00b6 convertToDelta \u00b6 convertToDelta ( spark : SparkSession , identifier : String ) : DeltaTable convertToDelta ( spark : SparkSession , identifier : String , partitionSchema : String ) : DeltaTable convertToDelta ( spark : SparkSession , identifier : String , partitionSchema : StructType ) : DeltaTable convertToDelta converts a parquet table to delta format (and makes the table available in Delta Lake). Note Refer to Demo: Converting Parquet Dataset Into Delta Format for a demo of DeltaTable.convertToDelta . Internally, convertToDelta requests the SparkSession for the SQL parser ( ParserInterface ) that is in turn requested to parse the given table identifier (to get a TableIdentifier ). Tip Read up on ParserInterface in The Internals of Spark SQL online book. In the end, convertToDelta uses the DeltaConvert utility to convert the parquet table to delta format and creates a DeltaTable . forName \u00b6 forName ( sparkSession : SparkSession , tableName : String ) : DeltaTable forName ( tableOrViewName : String ) : DeltaTable forName uses ParserInterface (of the given SparkSession ) to parse the given table name. forName checks whether the given table name is of a Delta table and, if so, creates a DeltaTable with the following: Dataset that represents loading data from the specified table name (using SparkSession.table operator) DeltaLog of the specified table forName throws an AnalysisException when the given table name is for non-Delta table: [deltaTableIdentifier] is not a Delta table. forName is used internally when DeltaConvert utility is used to executeConvert . forPath \u00b6 forPath ( sparkSession : SparkSession , path : String ) : DeltaTable forPath ( path : String ) : DeltaTable forPath creates a DeltaTable instance for data in the given directory ( path ) when the given directory is part of a delta table already (as the root or a child directory). assert(spark.isInstanceOf[org.apache.spark.sql.SparkSession]) val tableId = \"/tmp/delta-table/users\" import io.delta.tables.DeltaTable assert(DeltaTable.isDeltaTable(tableId), s\"$tableId should be a Delta table\") val dt = DeltaTable.forPath(\"delta-table\") forPath throws an AnalysisException when the given path does not belong to a delta table: [deltaTableIdentifier] is not a Delta table. Internally, forPath creates a new DeltaTable with the following: Dataset that represents loading data from the specified path using delta data source DeltaLog for the (transaction log in) the specified path forPath is used internally in DeltaTable.convertToDelta (via DeltaConvert utility). isDeltaTable \u00b6 isDeltaTable ( sparkSession : SparkSession , identifier : String ) : Boolean isDeltaTable ( identifier : String ) : Boolean isDeltaTable checks whether the provided identifier string is a file path that points to the root of a Delta table or one of the subdirectories. Internally, isDeltaTable simply relays to DeltaTableUtils.isDeltaTable utility. Creating Instance \u00b6 DeltaTable takes the following to be created: Table Data ( Dataset[Row] ) DeltaLog DeltaTable is created using DeltaTable.forPath or DeltaTable.forName utilities. Operators \u00b6 alias \u00b6 alias ( alias : String ) : DeltaTable Applies an alias to the DeltaTable (equivalent to as ) as \u00b6 as ( alias : String ) : DeltaTable Applies an alias to the DeltaTable delete \u00b6 delete () : Unit delete ( condition : Column ) : Unit delete ( condition : String ) : Unit Deletes data from the DeltaTable that matches the given condition . generate \u00b6 generate ( mode : String ) : Unit Generates a manifest for the delta table generate executeGenerate with the table ID of the format ++delta. path ++ (where the path is the data directory of the DeltaLog ) and the given mode. history \u00b6 history () : DataFrame history ( limit : Int ) : DataFrame Gets available commits ( history ) of the DeltaTable merge \u00b6 merge ( source : DataFrame , condition : Column ) : DeltaMergeBuilder merge ( source : DataFrame , condition : String ) : DeltaMergeBuilder Creates a DeltaMergeBuilder toDF \u00b6 toDF : Dataset [ Row ] Returns the DataFrame representation of the DeltaTable update \u00b6 update ( condition : Column , set : Map [ String , Column ]) : Unit update ( set : Map [ String , Column ]) : Unit Updates data in the DeltaTable on the rows that match the given condition based on the rules defined by set updateExpr \u00b6 updateExpr ( set : Map [ String , String ]) : Unit updateExpr ( condition : String , set : Map [ String , String ]) : Unit Updates data in the DeltaTable on the rows that match the given condition based on the rules defined by set vacuum \u00b6 vacuum () : DataFrame vacuum ( retentionHours : Double ) : DataFrame Deletes files and directories (recursively) in the DeltaTable that are not needed by the table (and maintains older versions up to the given retention threshold). vacuum executes vacuum command . Demo \u00b6 import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) val path = \"/tmp/delta/t1\" // random data to create a delta table from scratch val data = spark.range(5) data.write.format(\"delta\").save(path) import io.delta.tables.DeltaTable val dt = DeltaTable.forPath(spark, path) val history = dt.history.select('version, 'timestamp, 'operation, 'operationParameters, 'isBlindAppend) scala> history.show(truncate = false) +-------+-------------------+---------+------------------------------------------+-------------+ |version|timestamp |operation|operationParameters |isBlindAppend| +-------+-------------------+---------+------------------------------------------+-------------+ |0 |2019-12-23 22:24:40|WRITE |[mode -> ErrorIfExists, partitionBy -> []]|true | +-------+-------------------+---------+------------------------------------------+-------------+","title":"DeltaTable"},{"location":"DeltaTable/#deltatable","text":"DeltaTable is the management interface of a Delta table (with the Delta DML Operations ). DeltaTable instances are created using utilities (e.g. DeltaTable.forName , DeltaTable.convertToDelta ).","title":"DeltaTable"},{"location":"DeltaTable/#utilities-static-methods","text":"","title":" Utilities (Static Methods)"},{"location":"DeltaTable/#converttodelta","text":"convertToDelta ( spark : SparkSession , identifier : String ) : DeltaTable convertToDelta ( spark : SparkSession , identifier : String , partitionSchema : String ) : DeltaTable convertToDelta ( spark : SparkSession , identifier : String , partitionSchema : StructType ) : DeltaTable convertToDelta converts a parquet table to delta format (and makes the table available in Delta Lake). Note Refer to Demo: Converting Parquet Dataset Into Delta Format for a demo of DeltaTable.convertToDelta . Internally, convertToDelta requests the SparkSession for the SQL parser ( ParserInterface ) that is in turn requested to parse the given table identifier (to get a TableIdentifier ). Tip Read up on ParserInterface in The Internals of Spark SQL online book. In the end, convertToDelta uses the DeltaConvert utility to convert the parquet table to delta format and creates a DeltaTable .","title":" convertToDelta"},{"location":"DeltaTable/#forname","text":"forName ( sparkSession : SparkSession , tableName : String ) : DeltaTable forName ( tableOrViewName : String ) : DeltaTable forName uses ParserInterface (of the given SparkSession ) to parse the given table name. forName checks whether the given table name is of a Delta table and, if so, creates a DeltaTable with the following: Dataset that represents loading data from the specified table name (using SparkSession.table operator) DeltaLog of the specified table forName throws an AnalysisException when the given table name is for non-Delta table: [deltaTableIdentifier] is not a Delta table. forName is used internally when DeltaConvert utility is used to executeConvert .","title":" forName"},{"location":"DeltaTable/#forpath","text":"forPath ( sparkSession : SparkSession , path : String ) : DeltaTable forPath ( path : String ) : DeltaTable forPath creates a DeltaTable instance for data in the given directory ( path ) when the given directory is part of a delta table already (as the root or a child directory). assert(spark.isInstanceOf[org.apache.spark.sql.SparkSession]) val tableId = \"/tmp/delta-table/users\" import io.delta.tables.DeltaTable assert(DeltaTable.isDeltaTable(tableId), s\"$tableId should be a Delta table\") val dt = DeltaTable.forPath(\"delta-table\") forPath throws an AnalysisException when the given path does not belong to a delta table: [deltaTableIdentifier] is not a Delta table. Internally, forPath creates a new DeltaTable with the following: Dataset that represents loading data from the specified path using delta data source DeltaLog for the (transaction log in) the specified path forPath is used internally in DeltaTable.convertToDelta (via DeltaConvert utility).","title":" forPath"},{"location":"DeltaTable/#isdeltatable","text":"isDeltaTable ( sparkSession : SparkSession , identifier : String ) : Boolean isDeltaTable ( identifier : String ) : Boolean isDeltaTable checks whether the provided identifier string is a file path that points to the root of a Delta table or one of the subdirectories. Internally, isDeltaTable simply relays to DeltaTableUtils.isDeltaTable utility.","title":" isDeltaTable"},{"location":"DeltaTable/#creating-instance","text":"DeltaTable takes the following to be created: Table Data ( Dataset[Row] ) DeltaLog DeltaTable is created using DeltaTable.forPath or DeltaTable.forName utilities.","title":"Creating Instance"},{"location":"DeltaTable/#operators","text":"","title":"Operators"},{"location":"DeltaTable/#alias","text":"alias ( alias : String ) : DeltaTable Applies an alias to the DeltaTable (equivalent to as )","title":" alias"},{"location":"DeltaTable/#as","text":"as ( alias : String ) : DeltaTable Applies an alias to the DeltaTable","title":" as"},{"location":"DeltaTable/#delete","text":"delete () : Unit delete ( condition : Column ) : Unit delete ( condition : String ) : Unit Deletes data from the DeltaTable that matches the given condition .","title":" delete"},{"location":"DeltaTable/#generate","text":"generate ( mode : String ) : Unit Generates a manifest for the delta table generate executeGenerate with the table ID of the format ++delta. path ++ (where the path is the data directory of the DeltaLog ) and the given mode.","title":" generate"},{"location":"DeltaTable/#history","text":"history () : DataFrame history ( limit : Int ) : DataFrame Gets available commits ( history ) of the DeltaTable","title":" history"},{"location":"DeltaTable/#merge","text":"merge ( source : DataFrame , condition : Column ) : DeltaMergeBuilder merge ( source : DataFrame , condition : String ) : DeltaMergeBuilder Creates a DeltaMergeBuilder","title":" merge"},{"location":"DeltaTable/#todf","text":"toDF : Dataset [ Row ] Returns the DataFrame representation of the DeltaTable","title":" toDF"},{"location":"DeltaTable/#update","text":"update ( condition : Column , set : Map [ String , Column ]) : Unit update ( set : Map [ String , Column ]) : Unit Updates data in the DeltaTable on the rows that match the given condition based on the rules defined by set","title":" update"},{"location":"DeltaTable/#updateexpr","text":"updateExpr ( set : Map [ String , String ]) : Unit updateExpr ( condition : String , set : Map [ String , String ]) : Unit Updates data in the DeltaTable on the rows that match the given condition based on the rules defined by set","title":" updateExpr"},{"location":"DeltaTable/#vacuum","text":"vacuum () : DataFrame vacuum ( retentionHours : Double ) : DataFrame Deletes files and directories (recursively) in the DeltaTable that are not needed by the table (and maintains older versions up to the given retention threshold). vacuum executes vacuum command .","title":" vacuum"},{"location":"DeltaTable/#demo","text":"import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) val path = \"/tmp/delta/t1\" // random data to create a delta table from scratch val data = spark.range(5) data.write.format(\"delta\").save(path) import io.delta.tables.DeltaTable val dt = DeltaTable.forPath(spark, path) val history = dt.history.select('version, 'timestamp, 'operation, 'operationParameters, 'isBlindAppend) scala> history.show(truncate = false) +-------+-------------------+---------+------------------------------------------+-------------+ |version|timestamp |operation|operationParameters |isBlindAppend| +-------+-------------------+---------+------------------------------------------+-------------+ |0 |2019-12-23 22:24:40|WRITE |[mode -> ErrorIfExists, partitionBy -> []]|true | +-------+-------------------+---------+------------------------------------------+-------------+","title":"Demo"},{"location":"DeltaTableOperations/","text":"= [[DeltaTableOperations]] DeltaTableOperations -- Delta DML Operations [[self]] DeltaTableOperations is the < > of < > of a < > for executing < >, < >, < >, and < > commands. [[implementations]] NOTE: < > is the default and only known DeltaTableOperations in Delta Lake. NOTE: < > and < > operations do not support subqueries (and < > otherwise). == [[executeGenerate]] Executing Generate Command -- executeGenerate Method [source, scala] \u00b6 executeGenerate( tblIdentifier: String, mode: String): Unit executeGenerate requests the SQL parser (of the SparkSession ) to parse the given table identifier, creates a < > and runs it. NOTE: executeGenerate is used in < > operator. == [[executeDelete]] Executing Delete Command -- executeDelete Method [source, scala] \u00b6 executeDelete( condition: Option[Expression]): Unit executeDelete creates a QueryExecution for the Delete logical operator with (the analyzed logical plan of) the < >. executeDelete requests the QueryExecution for the analyzed logical plan that is ( again?! ) a Delete unary node. NOTE: FIXME What's the purpose of all this resolutions? In the end, executeDelete simply creates a < > (for the resolved delete) and < >. executeDelete < > when the condition expression contains subqueries. NOTE: executeDelete is used in < > operator. == [[executeHistory]] executeHistory Method [source, scala] \u00b6 executeHistory( deltaLog: DeltaLog, limit: Option[Int]): DataFrame executeHistory ...FIXME NOTE: executeHistory is used when...FIXME == [[executeUpdate]] executeUpdate Method [source, scala] \u00b6 executeUpdate( set: Map[String, Column], condition: Option[Column]): Unit executeUpdate ...FIXME NOTE: executeUpdate is used when...FIXME == [[executeVacuum]] Executing Vacuum Command -- executeVacuum Method [source, scala] \u00b6 executeVacuum( deltaLog: DeltaLog, retentionHours: Option[Double]): DataFrame executeVacuum simply uses the VacuumCommand utility to < > (with the dryRun flag off) and returns an empty DataFrame . NOTE: executeVacuum returns an empty DataFrame not the one from < >. NOTE: executeVacuum is used exclusively in < > operator. == [[makeUpdateTable]] makeUpdateTable Method [source, scala] \u00b6 makeUpdateTable( target: DeltaTable, onCondition: Option[Column], setColumns: Seq[(String, Column)]): UpdateTable makeUpdateTable ...FIXME NOTE: makeUpdateTable is used when...FIXME == [[subqueryNotSupportedCheck]] subqueryNotSupportedCheck Internal Method [source, scala] \u00b6 subqueryNotSupportedCheck( condition: Option[Expression], op: String): Unit subqueryNotSupportedCheck traverses the condition expression and throws an AnalysisException when it finds a SubqueryExpression : Subqueries are not supported in the [op] (condition = [sql]). NOTE: subqueryNotSupportedCheck is used when DeltaTableOperations is requested to execute < > and < > operations.","title":"DeltaTableOperations"},{"location":"DeltaTableOperations/#source-scala","text":"executeGenerate( tblIdentifier: String, mode: String): Unit executeGenerate requests the SQL parser (of the SparkSession ) to parse the given table identifier, creates a < > and runs it. NOTE: executeGenerate is used in < > operator. == [[executeDelete]] Executing Delete Command -- executeDelete Method","title":"[source, scala]"},{"location":"DeltaTableOperations/#source-scala_1","text":"executeDelete( condition: Option[Expression]): Unit executeDelete creates a QueryExecution for the Delete logical operator with (the analyzed logical plan of) the < >. executeDelete requests the QueryExecution for the analyzed logical plan that is ( again?! ) a Delete unary node. NOTE: FIXME What's the purpose of all this resolutions? In the end, executeDelete simply creates a < > (for the resolved delete) and < >. executeDelete < > when the condition expression contains subqueries. NOTE: executeDelete is used in < > operator. == [[executeHistory]] executeHistory Method","title":"[source, scala]"},{"location":"DeltaTableOperations/#source-scala_2","text":"executeHistory( deltaLog: DeltaLog, limit: Option[Int]): DataFrame executeHistory ...FIXME NOTE: executeHistory is used when...FIXME == [[executeUpdate]] executeUpdate Method","title":"[source, scala]"},{"location":"DeltaTableOperations/#source-scala_3","text":"executeUpdate( set: Map[String, Column], condition: Option[Column]): Unit executeUpdate ...FIXME NOTE: executeUpdate is used when...FIXME == [[executeVacuum]] Executing Vacuum Command -- executeVacuum Method","title":"[source, scala]"},{"location":"DeltaTableOperations/#source-scala_4","text":"executeVacuum( deltaLog: DeltaLog, retentionHours: Option[Double]): DataFrame executeVacuum simply uses the VacuumCommand utility to < > (with the dryRun flag off) and returns an empty DataFrame . NOTE: executeVacuum returns an empty DataFrame not the one from < >. NOTE: executeVacuum is used exclusively in < > operator. == [[makeUpdateTable]] makeUpdateTable Method","title":"[source, scala]"},{"location":"DeltaTableOperations/#source-scala_5","text":"makeUpdateTable( target: DeltaTable, onCondition: Option[Column], setColumns: Seq[(String, Column)]): UpdateTable makeUpdateTable ...FIXME NOTE: makeUpdateTable is used when...FIXME == [[subqueryNotSupportedCheck]] subqueryNotSupportedCheck Internal Method","title":"[source, scala]"},{"location":"DeltaTableOperations/#source-scala_6","text":"subqueryNotSupportedCheck( condition: Option[Expression], op: String): Unit subqueryNotSupportedCheck traverses the condition expression and throws an AnalysisException when it finds a SubqueryExpression : Subqueries are not supported in the [op] (condition = [sql]). NOTE: subqueryNotSupportedCheck is used when DeltaTableOperations is requested to execute < > and < > operations.","title":"[source, scala]"},{"location":"DeltaTableUtils/","text":"= [[DeltaTableUtils]] DeltaTableUtils Utility DeltaTableUtils comes with the following utilities: < > for checking out whether a given directory is part of delta table < > for finding the root directory of a delta table < > == [[isDeltaTable]] isDeltaTable Utility [source, scala] \u00b6 isDeltaTable( spark: SparkSession, path: Path): Boolean isDeltaTable tries to < > for the given path and returns whether it was successful or not. NOTE: isDeltaTable is used when DeltaTable utility is used to < > or < >. == [[findDeltaTableRoot]] findDeltaTableRoot Utility [source, scala] \u00b6 findDeltaTableRoot( spark: SparkSession, path: Path): Option[Path] findDeltaTableRoot traverses the Hadoop DFS-compliant path upwards (to the root directory of the file system) until _delta_log or _samples directories are found, or the root directory is reached. For _delta_log or _samples directories, findDeltaTableRoot returns the parent directory. [NOTE] \u00b6 findDeltaTableRoot is used when: < > is executed DeltaTableUtils utility is used to < > * DeltaDataSource is requested to < > \u00b6 == [[resolveTimeTravelVersion]] resolveTimeTravelVersion Utility [source, scala] \u00b6 resolveTimeTravelVersion( conf: SQLConf, deltaLog: DeltaLog, tt: DeltaTimeTravelSpec): (Long, String) resolveTimeTravelVersion ...FIXME NOTE: resolveTimeTravelVersion is used exclusively when DeltaLog is requested to < >. == [[splitMetadataAndDataPredicates]] splitMetadataAndDataPredicates Utility [source, scala] \u00b6 splitMetadataAndDataPredicates( condition: Expression, partitionColumns: Seq[String], spark: SparkSession): (Seq[Expression], Seq[Expression]) splitMetadataAndDataPredicates ...FIXME [NOTE] \u00b6 splitMetadataAndDataPredicates is used when: PartitionFiltering is requested to PartitionFiltering.md#filesForScan[filesForScan] * DeleteCommand.md[DeleteCommand] and UpdateCommand.md[UpdateCommand] are executed \u00b6 == [[isPredicatePartitionColumnsOnly]] isPredicatePartitionColumnsOnly Utility [source, scala] \u00b6 isPredicatePartitionColumnsOnly( condition: Expression, partitionColumns: Seq[String], spark: SparkSession): Boolean isPredicatePartitionColumnsOnly ...FIXME [NOTE] \u00b6 isPredicatePartitionColumnsOnly is used when...FIXME \u00b6 == [[combineWithCatalogMetadata]] combineWithCatalogMetadata Utility [source, scala] \u00b6 combineWithCatalogMetadata( sparkSession: SparkSession, table: CatalogTable): CatalogTable combineWithCatalogMetadata ...FIXME NOTE: combineWithCatalogMetadata seems unused.","title":"DeltaTableUtils"},{"location":"DeltaTableUtils/#source-scala","text":"isDeltaTable( spark: SparkSession, path: Path): Boolean isDeltaTable tries to < > for the given path and returns whether it was successful or not. NOTE: isDeltaTable is used when DeltaTable utility is used to < > or < >. == [[findDeltaTableRoot]] findDeltaTableRoot Utility","title":"[source, scala]"},{"location":"DeltaTableUtils/#source-scala_1","text":"findDeltaTableRoot( spark: SparkSession, path: Path): Option[Path] findDeltaTableRoot traverses the Hadoop DFS-compliant path upwards (to the root directory of the file system) until _delta_log or _samples directories are found, or the root directory is reached. For _delta_log or _samples directories, findDeltaTableRoot returns the parent directory.","title":"[source, scala]"},{"location":"DeltaTableUtils/#note","text":"findDeltaTableRoot is used when: < > is executed DeltaTableUtils utility is used to < >","title":"[NOTE]"},{"location":"DeltaTableUtils/#deltadatasource-is-requested-to","text":"== [[resolveTimeTravelVersion]] resolveTimeTravelVersion Utility","title":"* DeltaDataSource is requested to &lt;&gt;"},{"location":"DeltaTableUtils/#source-scala_2","text":"resolveTimeTravelVersion( conf: SQLConf, deltaLog: DeltaLog, tt: DeltaTimeTravelSpec): (Long, String) resolveTimeTravelVersion ...FIXME NOTE: resolveTimeTravelVersion is used exclusively when DeltaLog is requested to < >. == [[splitMetadataAndDataPredicates]] splitMetadataAndDataPredicates Utility","title":"[source, scala]"},{"location":"DeltaTableUtils/#source-scala_3","text":"splitMetadataAndDataPredicates( condition: Expression, partitionColumns: Seq[String], spark: SparkSession): (Seq[Expression], Seq[Expression]) splitMetadataAndDataPredicates ...FIXME","title":"[source, scala]"},{"location":"DeltaTableUtils/#note_1","text":"splitMetadataAndDataPredicates is used when: PartitionFiltering is requested to PartitionFiltering.md#filesForScan[filesForScan]","title":"[NOTE]"},{"location":"DeltaTableUtils/#deletecommandmddeletecommand-and-updatecommandmdupdatecommand-are-executed","text":"== [[isPredicatePartitionColumnsOnly]] isPredicatePartitionColumnsOnly Utility","title":"* DeleteCommand.md[DeleteCommand] and UpdateCommand.md[UpdateCommand] are executed"},{"location":"DeltaTableUtils/#source-scala_4","text":"isPredicatePartitionColumnsOnly( condition: Expression, partitionColumns: Seq[String], spark: SparkSession): Boolean isPredicatePartitionColumnsOnly ...FIXME","title":"[source, scala]"},{"location":"DeltaTableUtils/#note_2","text":"","title":"[NOTE]"},{"location":"DeltaTableUtils/#ispredicatepartitioncolumnsonly-is-used-whenfixme","text":"== [[combineWithCatalogMetadata]] combineWithCatalogMetadata Utility","title":"isPredicatePartitionColumnsOnly is used when...FIXME"},{"location":"DeltaTableUtils/#source-scala_5","text":"combineWithCatalogMetadata( sparkSession: SparkSession, table: CatalogTable): CatalogTable combineWithCatalogMetadata ...FIXME NOTE: combineWithCatalogMetadata seems unused.","title":"[source, scala]"},{"location":"DeltaTableV2/","text":"DeltaTableV2 \u00b6 DeltaTableV2 is a logical representation of a writable Delta table. Using the abstractions introduced in Spark SQL 3.0.0, DeltaTableV2 is a Table that SupportsWrite . Creating Instance \u00b6 DeltaTableV2 takes the following to be created: SparkSession Hadoop Path Optional Catalog Metadata ( Option[CatalogTable] ) Optional Table ID ( Option[String] ) Optional Time Travel Specification ( Option[DeltaTimeTravelSpec] ) DeltaTableV2 is created when: DeltaCatalog is requested to load a table DeltaDataSource is requested to load a table or create a table relation Snapshot \u00b6 snapshot : Snapshot DeltaTableV2 has a Snapshot (that is loaded once when first accessed). DeltaTableV2 uses the DeltaLog to load it at a given version (based on the optional timeTravelSpec ) or update to the latest version . snapshot is used when DeltaTableV2 is requested for the schema , partitioning and properties . DeltaTimeTravelSpec \u00b6 timeTravelSpec : Option [ DeltaTimeTravelSpec ] DeltaTableV2 may have a DeltaTimeTravelSpec specified that is either timeTravelOpt or timeTravelByPath . timeTravelSpec is used when DeltaTableV2 is requested for a Snapshot and BaseRelation . Converting to BaseRelation (Insertable HadoopFsRelation) \u00b6 toBaseRelation : BaseRelation toBaseRelation verifyAndCreatePartitionFilters for the Path , the current Snapshot and partitionFilters . In the end, toBaseRelation requests the DeltaLog for an insertable HadoopFsRelation . toBaseRelation is used when: DeltaDataSource is requested to createRelation DeltaRelation utility is used to fromV2Relation","title":"DeltaTableV2"},{"location":"DeltaTableV2/#deltatablev2","text":"DeltaTableV2 is a logical representation of a writable Delta table. Using the abstractions introduced in Spark SQL 3.0.0, DeltaTableV2 is a Table that SupportsWrite .","title":"DeltaTableV2"},{"location":"DeltaTableV2/#creating-instance","text":"DeltaTableV2 takes the following to be created: SparkSession Hadoop Path Optional Catalog Metadata ( Option[CatalogTable] ) Optional Table ID ( Option[String] ) Optional Time Travel Specification ( Option[DeltaTimeTravelSpec] ) DeltaTableV2 is created when: DeltaCatalog is requested to load a table DeltaDataSource is requested to load a table or create a table relation","title":"Creating Instance"},{"location":"DeltaTableV2/#snapshot","text":"snapshot : Snapshot DeltaTableV2 has a Snapshot (that is loaded once when first accessed). DeltaTableV2 uses the DeltaLog to load it at a given version (based on the optional timeTravelSpec ) or update to the latest version . snapshot is used when DeltaTableV2 is requested for the schema , partitioning and properties .","title":" Snapshot"},{"location":"DeltaTableV2/#deltatimetravelspec","text":"timeTravelSpec : Option [ DeltaTimeTravelSpec ] DeltaTableV2 may have a DeltaTimeTravelSpec specified that is either timeTravelOpt or timeTravelByPath . timeTravelSpec is used when DeltaTableV2 is requested for a Snapshot and BaseRelation .","title":" DeltaTimeTravelSpec"},{"location":"DeltaTableV2/#converting-to-baserelation-insertable-hadoopfsrelation","text":"toBaseRelation : BaseRelation toBaseRelation verifyAndCreatePartitionFilters for the Path , the current Snapshot and partitionFilters . In the end, toBaseRelation requests the DeltaLog for an insertable HadoopFsRelation . toBaseRelation is used when: DeltaDataSource is requested to createRelation DeltaRelation utility is used to fromV2Relation","title":" Converting to BaseRelation (Insertable HadoopFsRelation)"},{"location":"DeltaUnsupportedOperationsCheck/","text":"DeltaUnsupportedOperationsCheck \u00b6 DeltaUnsupportedOperationsCheck is a logical check rule that adds helpful error messages when Delta is being used with unsupported Hive operations or if an unsupported operation is executed.","title":"DeltaUnsupportedOperationsCheck"},{"location":"DeltaUnsupportedOperationsCheck/#deltaunsupportedoperationscheck","text":"DeltaUnsupportedOperationsCheck is a logical check rule that adds helpful error messages when Delta is being used with unsupported Hive operations or if an unsupported operation is executed.","title":"DeltaUnsupportedOperationsCheck"},{"location":"FileAction/","text":"= FileAction FileAction is an < > of the < > for < > with the < > and < > flag. [[contract]] .FileAction Contract (Abstract Methods Only) [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | dataChange a| [[dataChange]] [source, scala] \u00b6 dataChange: Boolean \u00b6 Used when...FIXME | path a| [[path]] [source, scala] \u00b6 path: String \u00b6 Used when...FIXME |=== [[implementations]] .FileActions [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | FileAction | Description | < > | [[AddFile]] | RemoveFile | [[RemoveFile]] |=== NOTE: FileAction is a Scala sealed trait which means that all the < > are in the same compilation unit (a single file).","title":"FileAction"},{"location":"FileAction/#source-scala","text":"","title":"[source, scala]"},{"location":"FileAction/#datachange-boolean","text":"Used when...FIXME | path a| [[path]]","title":"dataChange: Boolean"},{"location":"FileAction/#source-scala_1","text":"","title":"[source, scala]"},{"location":"FileAction/#path-string","text":"Used when...FIXME |=== [[implementations]] .FileActions [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | FileAction | Description | < > | [[AddFile]] | RemoveFile | [[RemoveFile]] |=== NOTE: FileAction is a Scala sealed trait which means that all the < > are in the same compilation unit (a single file).","title":"path: String"},{"location":"FileNames/","text":"= FileNames Utility [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | checkpointPrefix a| [[checkpointPrefix]] Creates a Hadoop Path for a file name with a given version : [version][%020d].checkpoint E.g. 00000000000000000005.checkpoint | isCheckpointFile a| [[isCheckpointFile]] | isDeltaFile a| [[isDeltaFile]] |=== == [[deltaFile]] Creating Hadoop Path To Delta File -- deltaFile Utility [source, scala] \u00b6 deltaFile( path: Path, version: Long): Path deltaFile creates a Hadoop Path to a file of the format [version][%020d].json in the path directory, e.g. 00000000000000000001.json . NOTE: deltaFile is used when...FIXME","title":"FileNames"},{"location":"FileNames/#source-scala","text":"deltaFile( path: Path, version: Long): Path deltaFile creates a Hadoop Path to a file of the format [version][%020d].json in the path directory, e.g. 00000000000000000001.json . NOTE: deltaFile is used when...FIXME","title":"[source, scala]"},{"location":"GenerateSymlinkManifest/","text":"GenerateSymlinkManifest (And GenerateSymlinkManifestImpl) \u00b6 [[GenerateSymlinkManifest]] GenerateSymlinkManifest is a concrete < > to generate < > and < > Hive-style manifests for delta tables. NOTE: You can generate a < > Hive-style manifest for delta tables using < > SQL command or < > operator. [[GenerateSymlinkManifestImpl]] GenerateSymlinkManifestImpl is a < > that...FIXME == [[generateFullManifest]] generateFullManifest Method [source, scala] \u00b6 generateFullManifest( spark: SparkSession, deltaLog: DeltaLog): Unit generateFullManifest ...FIXME NOTE: generateFullManifest is used when...FIXME == [[generateIncrementalManifest]] generateIncrementalManifest Method [source, scala] \u00b6 generateIncrementalManifest( spark: SparkSession, deltaLog: DeltaLog, txnReadSnapshot: Snapshot, actions: Seq[Action]): Unit generateIncrementalManifest ...FIXME NOTE: generateIncrementalManifest is used when...FIXME == [[run]] Running Post-Commit Hook -- run Method [source, scala] \u00b6 run( spark: SparkSession, txn: OptimisticTransactionImpl, committedActions: Seq[Action]): Unit NOTE: run is part of the < > to execute a post-commit hook. run simply < > for the < > and < > of the delta table (of the given < >) and the < >. == [[handleError]] Handling Errors -- handleError Method [source, scala] \u00b6 handleError( error: Throwable, version: Long): Unit NOTE: handleError is part of the < > to handle errors while < > handleError ...FIXME","title":"GenerateSymlinkManifest"},{"location":"GenerateSymlinkManifest/#generatesymlinkmanifest-and-generatesymlinkmanifestimpl","text":"[[GenerateSymlinkManifest]] GenerateSymlinkManifest is a concrete < > to generate < > and < > Hive-style manifests for delta tables. NOTE: You can generate a < > Hive-style manifest for delta tables using < > SQL command or < > operator. [[GenerateSymlinkManifestImpl]] GenerateSymlinkManifestImpl is a < > that...FIXME == [[generateFullManifest]] generateFullManifest Method","title":"GenerateSymlinkManifest (And GenerateSymlinkManifestImpl)"},{"location":"GenerateSymlinkManifest/#source-scala","text":"generateFullManifest( spark: SparkSession, deltaLog: DeltaLog): Unit generateFullManifest ...FIXME NOTE: generateFullManifest is used when...FIXME == [[generateIncrementalManifest]] generateIncrementalManifest Method","title":"[source, scala]"},{"location":"GenerateSymlinkManifest/#source-scala_1","text":"generateIncrementalManifest( spark: SparkSession, deltaLog: DeltaLog, txnReadSnapshot: Snapshot, actions: Seq[Action]): Unit generateIncrementalManifest ...FIXME NOTE: generateIncrementalManifest is used when...FIXME == [[run]] Running Post-Commit Hook -- run Method","title":"[source, scala]"},{"location":"GenerateSymlinkManifest/#source-scala_2","text":"run( spark: SparkSession, txn: OptimisticTransactionImpl, committedActions: Seq[Action]): Unit NOTE: run is part of the < > to execute a post-commit hook. run simply < > for the < > and < > of the delta table (of the given < >) and the < >. == [[handleError]] Handling Errors -- handleError Method","title":"[source, scala]"},{"location":"GenerateSymlinkManifest/#source-scala_3","text":"handleError( error: Throwable, version: Long): Unit NOTE: handleError is part of the < > to handle errors while < > handleError ...FIXME","title":"[source, scala]"},{"location":"HDFSLogStore/","text":"= HDFSLogStore HDFSLogStore is...FIXME","title":"HDFSLogStore"},{"location":"HadoopFileSystemLogStore/","text":"= HadoopFileSystemLogStore HadoopFileSystemLogStore is...FIXME","title":"HadoopFileSystemLogStore"},{"location":"ImplicitMetadataOperation/","text":"= [[ImplicitMetadataOperation]] ImplicitMetadataOperation -- Operations Updating Metadata (Schema And Partitioning) ImplicitMetadataOperation is an < > of < > that can < > of a delta table (while writing out a new data to a delta table). ImplicitMetadataOperation operations can update schema by < > and < > schema. [[contract]] .ImplicitMetadataOperation Contract (Abstract Methods Only) [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | canMergeSchema a| [[canMergeSchema]] [source, scala] \u00b6 canMergeSchema: Boolean \u00b6 Used when ImplicitMetadataOperation is requested to < > | canOverwriteSchema a| [[canOverwriteSchema]] [source, scala] \u00b6 canOverwriteSchema: Boolean \u00b6 Used when ImplicitMetadataOperation is requested to < > |=== [[implementations]] .ImplicitMetadataOperations [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | ImplicitMetadataOperation | Description | < > | [[WriteIntoDelta]] Delta command for batch queries (Spark SQL) | < > | [[DeltaSink]] Streaming sink for streaming queries (Spark Structured Streaming) |=== Updating Metadata \u00b6 updateMetadata ( txn : OptimisticTransaction , data : Dataset [ _ ], partitionColumns : Seq [ String ], configuration : Map [ String , String ], isOverwriteMode : Boolean ) : Unit updateMetadata ...FIXME updateMetadata is used when: WriteIntoDelta command is executed DeltaSink is requested to add a streaming micro-batch == [[normalizePartitionColumns]] Normalize Partition Columns -- normalizePartitionColumns Internal Method [source, scala] \u00b6 normalizePartitionColumns( spark: SparkSession, partitionCols: Seq[String], schema: StructType): Seq[String] normalizePartitionColumns ...FIXME NOTE: normalizePartitionColumns is used when ImplicitMetadataOperation is requested to < >.","title":"ImplicitMetadataOperation"},{"location":"ImplicitMetadataOperation/#source-scala","text":"","title":"[source, scala]"},{"location":"ImplicitMetadataOperation/#canmergeschema-boolean","text":"Used when ImplicitMetadataOperation is requested to < > | canOverwriteSchema a| [[canOverwriteSchema]]","title":"canMergeSchema: Boolean"},{"location":"ImplicitMetadataOperation/#source-scala_1","text":"","title":"[source, scala]"},{"location":"ImplicitMetadataOperation/#canoverwriteschema-boolean","text":"Used when ImplicitMetadataOperation is requested to < > |=== [[implementations]] .ImplicitMetadataOperations [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | ImplicitMetadataOperation | Description | < > | [[WriteIntoDelta]] Delta command for batch queries (Spark SQL) | < > | [[DeltaSink]] Streaming sink for streaming queries (Spark Structured Streaming) |===","title":"canOverwriteSchema: Boolean"},{"location":"ImplicitMetadataOperation/#updating-metadata","text":"updateMetadata ( txn : OptimisticTransaction , data : Dataset [ _ ], partitionColumns : Seq [ String ], configuration : Map [ String , String ], isOverwriteMode : Boolean ) : Unit updateMetadata ...FIXME updateMetadata is used when: WriteIntoDelta command is executed DeltaSink is requested to add a streaming micro-batch == [[normalizePartitionColumns]] Normalize Partition Columns -- normalizePartitionColumns Internal Method","title":" Updating Metadata"},{"location":"ImplicitMetadataOperation/#source-scala_2","text":"normalizePartitionColumns( spark: SparkSession, partitionCols: Seq[String], schema: StructType): Seq[String] normalizePartitionColumns ...FIXME NOTE: normalizePartitionColumns is used when ImplicitMetadataOperation is requested to < >.","title":"[source, scala]"},{"location":"InMemoryLogReplay/","text":"= InMemoryLogReplay -- Delta Log Replay InMemoryLogReplay is used at the very last phase of < > (of a < >). InMemoryLogReplay is < > for every partition of the < > dataset ( Dataset[SingleAction] ) that is based on the < > configuration property (default: 50 ). The lifecycle of InMemoryLogReplay is as follows: . < > (with < >) . < > (with all < > of a partition) . < > == [[creating-instance]] Creating InMemoryLogReplay Instance InMemoryLogReplay takes the following to be created: [[minFileRetentionTimestamp]] minFileRetentionTimestamp (that is exactly < >) InMemoryLogReplay initializes the < >. == [[append]] Appending Actions -- append Method [source, scala] \u00b6 append( version: Long, actions: Iterator[Action]): Unit append sets the < > as the given version . append adds < > to respective registries: Every < > is registered in the < > by < > < > is registered as the < > < > is registered as the < > Every < > is registered as follows: ** Added to < > by pathAsUri (with dataChange flag turned off) ** Removed from < > by pathAsUri Every < > is registered as follows: ** Removed from < > by pathAsUri ** Added to < > by pathAsUri (with dataChange flag turned off) < > are ignored append throws an AssertionError when the < > is neither -1 (the default) nor one before the given version : Attempted to replay version [version], but state is at [currentVersion] NOTE: append is used when Snapshot is created (and initializes the < > for the < >). == [[checkpoint]] Current State Of Delta Table -- checkpoint Method [source, scala] \u00b6 checkpoint: Iterator[Action] \u00b6 checkpoint simply builds a sequence ( Iterator[Action] ) of the following (in that order): < > if defined (non- null ) < > if defined (non- null ) < > < > and < > (after the < >) sorted by < > (lexicographically) NOTE: checkpoint is used when Snapshot is created (and initializes the < > for the < >). == [[getTombstones]] getTombstones Internal Method [source, scala] \u00b6 getTombstones: Iterable[FileAction] \u00b6 getTombstones returns < > (from the < >) with their delTimestamp after the < >. NOTE: getTombstones is used when InMemoryLogReplay is requested to < >. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | currentProtocolVersion a| [[currentProtocolVersion]] < > (default: null ) Used when...FIXME | currentVersion a| [[currentVersion]] Version (default: -1 ) Used when...FIXME | currentMetaData a| [[currentMetaData]] < > (default: null ) Used when...FIXME | transactions a| [[transactions]] < > per ID ( HashMap[String, SetTransaction] ) Used when...FIXME | activeFiles a| [[activeFiles]] < > per URI ( HashMap[URI, AddFile] ) Used when...FIXME | tombstones a| [[tombstones]] < > per URI ( HashMap[URI, RemoveFile] ) Used when...FIXME |===","title":"InMemoryLogReplay"},{"location":"InMemoryLogReplay/#source-scala","text":"append( version: Long, actions: Iterator[Action]): Unit append sets the < > as the given version . append adds < > to respective registries: Every < > is registered in the < > by < > < > is registered as the < > < > is registered as the < > Every < > is registered as follows: ** Added to < > by pathAsUri (with dataChange flag turned off) ** Removed from < > by pathAsUri Every < > is registered as follows: ** Removed from < > by pathAsUri ** Added to < > by pathAsUri (with dataChange flag turned off) < > are ignored append throws an AssertionError when the < > is neither -1 (the default) nor one before the given version : Attempted to replay version [version], but state is at [currentVersion] NOTE: append is used when Snapshot is created (and initializes the < > for the < >). == [[checkpoint]] Current State Of Delta Table -- checkpoint Method","title":"[source, scala]"},{"location":"InMemoryLogReplay/#source-scala_1","text":"","title":"[source, scala]"},{"location":"InMemoryLogReplay/#checkpoint-iteratoraction","text":"checkpoint simply builds a sequence ( Iterator[Action] ) of the following (in that order): < > if defined (non- null ) < > if defined (non- null ) < > < > and < > (after the < >) sorted by < > (lexicographically) NOTE: checkpoint is used when Snapshot is created (and initializes the < > for the < >). == [[getTombstones]] getTombstones Internal Method","title":"checkpoint: Iterator[Action]"},{"location":"InMemoryLogReplay/#source-scala_2","text":"","title":"[source, scala]"},{"location":"InMemoryLogReplay/#gettombstones-iterablefileaction","text":"getTombstones returns < > (from the < >) with their delTimestamp after the < >. NOTE: getTombstones is used when InMemoryLogReplay is requested to < >. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | currentProtocolVersion a| [[currentProtocolVersion]] < > (default: null ) Used when...FIXME | currentVersion a| [[currentVersion]] Version (default: -1 ) Used when...FIXME | currentMetaData a| [[currentMetaData]] < > (default: null ) Used when...FIXME | transactions a| [[transactions]] < > per ID ( HashMap[String, SetTransaction] ) Used when...FIXME | activeFiles a| [[activeFiles]] < > per URI ( HashMap[URI, AddFile] ) Used when...FIXME | tombstones a| [[tombstones]] < > per URI ( HashMap[URI, RemoveFile] ) Used when...FIXME |===","title":"getTombstones: Iterable[FileAction]"},{"location":"Invariants/","text":"= [[Invariants]] Invariants Invariants is...FIXME","title":"Invariants"},{"location":"LogStore/","text":"= LogStore LogStore is an < > of < > that can < > and < > actions to a directory (among other things). [[contract]] .LogStore Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | invalidateCache a| [[invalidateCache]] [source, scala] \u00b6 invalidateCache(): Unit \u00b6 Used when...FIXME | isPartialWriteVisible a| [[isPartialWriteVisible]] [source, scala] \u00b6 isPartialWriteVisible(path: Path): Boolean = true \u00b6 Used when...FIXME | listFrom a| [[listFrom]] [source, scala] \u00b6 listFrom( path: Path): Iterator[FileStatus] listFrom( path: String): Iterator[FileStatus] Used when...FIXME | read a| [[read]] [source, scala] \u00b6 read(path: String): Seq[String] read(path: Path): Seq[String] Used when: Checkpoints is requested to < > DeltaHistoryManager utility is requested to < > DeltaLog is requested to < > VerifyChecksum is requested to < > OptimisticTransactionImpl is requested to < > | write a| [[write]] [source, scala] \u00b6 write( path: Path, actions: Iterator[String], overwrite: Boolean = false): Unit write( path: String, actions: Iterator[String]): Unit Writes the actions out to the given path (with or without overwrite as indicated). Used when: Checkpoints is requested to < > ConvertToDeltaCommand is < > (and does < >) OptimisticTransactionImpl is requested to < > |=== [[implementations]] .LogStores (Direct Implementations and Extensions Only) [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | LogStore | Description | < > | [[HDFSLogStore]] | < > | [[HadoopFileSystemLogStore]] |=== == [[resolvePathOnPhysicalStorage]] resolvePathOnPhysicalStorage Method [source, scala] \u00b6 resolvePathOnPhysicalStorage(path: Path): Path \u00b6 resolvePathOnPhysicalStorage ...FIXME NOTE: resolvePathOnPhysicalStorage is used when...FIXME == [[apply]] Creating LogStore -- apply Utility [source, scala] \u00b6 apply( sc: SparkContext): LogStore apply( sparkConf: SparkConf, hadoopConf: Configuration): LogStore apply ...FIXME [NOTE] \u00b6 apply is used when: DeltaHistoryManager is requested to < > and < > * DeltaFileOperations utility is used to < > \u00b6","title":"LogStore"},{"location":"LogStore/#source-scala","text":"","title":"[source, scala]"},{"location":"LogStore/#invalidatecache-unit","text":"Used when...FIXME | isPartialWriteVisible a| [[isPartialWriteVisible]]","title":"invalidateCache(): Unit"},{"location":"LogStore/#source-scala_1","text":"","title":"[source, scala]"},{"location":"LogStore/#ispartialwritevisiblepath-path-boolean-true","text":"Used when...FIXME | listFrom a| [[listFrom]]","title":"isPartialWriteVisible(path: Path): Boolean = true"},{"location":"LogStore/#source-scala_2","text":"listFrom( path: Path): Iterator[FileStatus] listFrom( path: String): Iterator[FileStatus] Used when...FIXME | read a| [[read]]","title":"[source, scala]"},{"location":"LogStore/#source-scala_3","text":"read(path: String): Seq[String] read(path: Path): Seq[String] Used when: Checkpoints is requested to < > DeltaHistoryManager utility is requested to < > DeltaLog is requested to < > VerifyChecksum is requested to < > OptimisticTransactionImpl is requested to < > | write a| [[write]]","title":"[source, scala]"},{"location":"LogStore/#source-scala_4","text":"write( path: Path, actions: Iterator[String], overwrite: Boolean = false): Unit write( path: String, actions: Iterator[String]): Unit Writes the actions out to the given path (with or without overwrite as indicated). Used when: Checkpoints is requested to < > ConvertToDeltaCommand is < > (and does < >) OptimisticTransactionImpl is requested to < > |=== [[implementations]] .LogStores (Direct Implementations and Extensions Only) [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | LogStore | Description | < > | [[HDFSLogStore]] | < > | [[HadoopFileSystemLogStore]] |=== == [[resolvePathOnPhysicalStorage]] resolvePathOnPhysicalStorage Method","title":"[source, scala]"},{"location":"LogStore/#source-scala_5","text":"","title":"[source, scala]"},{"location":"LogStore/#resolvepathonphysicalstoragepath-path-path","text":"resolvePathOnPhysicalStorage ...FIXME NOTE: resolvePathOnPhysicalStorage is used when...FIXME == [[apply]] Creating LogStore -- apply Utility","title":"resolvePathOnPhysicalStorage(path: Path): Path"},{"location":"LogStore/#source-scala_6","text":"apply( sc: SparkContext): LogStore apply( sparkConf: SparkConf, hadoopConf: Configuration): LogStore apply ...FIXME","title":"[source, scala]"},{"location":"LogStore/#note","text":"apply is used when: DeltaHistoryManager is requested to < > and < >","title":"[NOTE]"},{"location":"LogStore/#deltafileoperations-utility-is-used-to","text":"","title":"* DeltaFileOperations utility is used to &lt;&gt;"},{"location":"LogStoreProvider/","text":"== [[LogStoreProvider]] LogStoreProvider LogStoreProvider is an abstraction of < > of < >. [[logStoreClassConfKey]][[defaultLogStoreClass]][[spark.delta.logStore.class]] LogStoreProvider uses the spark.delta.logStore.class configuration property (default: < >) for the fully-qualified class name of the < > to < > (for a < >, a < >, and < >). == [[createLogStore]] Creating LogStore -- createLogStore Method [source, scala] \u00b6 createLogStore( spark: SparkSession): LogStore createLogStore( sparkConf: SparkConf, hadoopConf: Configuration): LogStore createLogStore ...FIXME [NOTE] \u00b6 createLogStore is used when: < > is created * < > utility is used (to create a < >) \u00b6","title":"LogStoreProvider"},{"location":"LogStoreProvider/#source-scala","text":"createLogStore( spark: SparkSession): LogStore createLogStore( sparkConf: SparkConf, hadoopConf: Configuration): LogStore createLogStore ...FIXME","title":"[source, scala]"},{"location":"LogStoreProvider/#note","text":"createLogStore is used when: < > is created","title":"[NOTE]"},{"location":"LogStoreProvider/#utility-is-used-to-create-a","text":"","title":"* &lt;&gt; utility is used (to create a &lt;&gt;)"},{"location":"Metadata/","text":"= Metadata Metadata is an < > that describes metadata (change) of a < > (indirectly via < >). [source,plaintext] \u00b6 import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, \"/tmp/delta/users\") scala> :type deltaLog.snapshot.metadata org.apache.spark.sql.delta.actions.Metadata Metadata contains all the non-data information ( metadata ) like < >, < >, < >, < >, < >, < > and < >. These can be changed (e.g., schema evolution). TIP: Use < > to review the metadata of a delta table. Metadata uses < > to uniquely identify a delta table. The ID is never going to change through the history of the table (unless the entire directory, along with the transaction log is deleted). It is known as tableId or < >. [NOTE] \u00b6 When I asked the question https://groups.google.com/forum/#!topic/delta-users/5OKEFvVKiew[tableId and reservoirId - Why two different names for metadata ID?] on delta-users mailing list, Tathagata Das wrote: Any reference to \"reservoir\" is just legacy code. In the early days of this project, the project was called \"Tahoe\" and each table is called a \"reservoir\" (Tahoe is one of the 2 nd deepest lake in US, and is a very large reservoir of water ;) ). So you may still find those two terms all around the codebase. In some cases, like DeltaSourceOffset, the term reservoirId is in the json that is written to the streaming checkpoint directory. So we cannot change that for backward compatibility. ==== Metadata can be < > in a OptimisticTransactionImpl.md[transaction] once (and only when created for an uninitialized table, when < > is -1 ). [source,scala] \u00b6 txn.metadata \u00b6 Metadata is < > when: DeltaLog is requested for the < > OptimisticTransactionImpl is requested for the < > ConvertToDeltaCommand is requested to < > ImplicitMetadataOperation is requested to < > == [[creating-instance]] Creating Metadata Instance Metadata takes the following to be created: [[id]] Table ID (default: a random UUID) [[name]] Name of the delta table (default: null ) [[description]] Description (default: null ) [[format]] Format [[schemaString]] Schema (default: null ) [[partitionColumns]] Partition columns (default: Nil ) [[configuration]] Configuration (default: empty ) [[createdTime]] Created time (in millis since the epoch) == [[wrap]] wrap Method [source, scala] \u00b6 wrap: SingleAction \u00b6 NOTE: wrap is part of the < > contract to wrap the action into a < >. wrap simply creates a new < > with the Metadata field set to this Metadata. == [[partitionSchema]] partitionSchema (Lazy) Property [source, scala] \u00b6 partitionSchema: StructType \u00b6 partitionSchema is the < > as StructFields (and defined in the < >). NOTE: partitionSchema throws an IllegalArgumentException for undefined fields that were used for the < > but not defined in the < >. NOTE: partitionSchema is used when...FIXME == [[dataSchema]] dataSchema (Lazy) Property [source, scala] \u00b6 dataSchema: StructType \u00b6 dataSchema ...FIXME NOTE: dataSchema is used when...FIXME == [[schema]] schema (Lazy) Property [source, scala] \u00b6 schema: StructType \u00b6 schema is a deserialized < > (from JSON format) to StructType . [NOTE] \u00b6 schema is used when: Metadata is requested for the schema of the < > and the < > DeltaLog is requested for an DeltaLog.md#createRelation[insertable HadoopFsRelation for batch queries] (for the data schema), to DeltaLog.md#upgradeProtocol[upgrade protocol], a DeltaLog.md#createDataFrame[DataFrame for given AddFiles] DeltaTableUtils utility is used to DeltaTableUtils.md#combineWithCatalogMetadata[combineWithCatalogMetadata] OptimisticTransactionImpl is requested to OptimisticTransactionImpl.md#verifyNewMetadata[verifyNewMetadata] * ...FIXME (there are other uses) \u00b6","title":"Metadata"},{"location":"Metadata/#sourceplaintext","text":"import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, \"/tmp/delta/users\") scala> :type deltaLog.snapshot.metadata org.apache.spark.sql.delta.actions.Metadata Metadata contains all the non-data information ( metadata ) like < >, < >, < >, < >, < >, < > and < >. These can be changed (e.g., schema evolution). TIP: Use < > to review the metadata of a delta table. Metadata uses < > to uniquely identify a delta table. The ID is never going to change through the history of the table (unless the entire directory, along with the transaction log is deleted). It is known as tableId or < >.","title":"[source,plaintext]"},{"location":"Metadata/#note","text":"When I asked the question https://groups.google.com/forum/#!topic/delta-users/5OKEFvVKiew[tableId and reservoirId - Why two different names for metadata ID?] on delta-users mailing list, Tathagata Das wrote: Any reference to \"reservoir\" is just legacy code. In the early days of this project, the project was called \"Tahoe\" and each table is called a \"reservoir\" (Tahoe is one of the 2 nd deepest lake in US, and is a very large reservoir of water ;) ). So you may still find those two terms all around the codebase. In some cases, like DeltaSourceOffset, the term reservoirId is in the json that is written to the streaming checkpoint directory. So we cannot change that for backward compatibility. ==== Metadata can be < > in a OptimisticTransactionImpl.md[transaction] once (and only when created for an uninitialized table, when < > is -1 ).","title":"[NOTE]"},{"location":"Metadata/#sourcescala","text":"","title":"[source,scala]"},{"location":"Metadata/#txnmetadata","text":"Metadata is < > when: DeltaLog is requested for the < > OptimisticTransactionImpl is requested for the < > ConvertToDeltaCommand is requested to < > ImplicitMetadataOperation is requested to < > == [[creating-instance]] Creating Metadata Instance Metadata takes the following to be created: [[id]] Table ID (default: a random UUID) [[name]] Name of the delta table (default: null ) [[description]] Description (default: null ) [[format]] Format [[schemaString]] Schema (default: null ) [[partitionColumns]] Partition columns (default: Nil ) [[configuration]] Configuration (default: empty ) [[createdTime]] Created time (in millis since the epoch) == [[wrap]] wrap Method","title":"txn.metadata"},{"location":"Metadata/#source-scala","text":"","title":"[source, scala]"},{"location":"Metadata/#wrap-singleaction","text":"NOTE: wrap is part of the < > contract to wrap the action into a < >. wrap simply creates a new < > with the Metadata field set to this Metadata. == [[partitionSchema]] partitionSchema (Lazy) Property","title":"wrap: SingleAction"},{"location":"Metadata/#source-scala_1","text":"","title":"[source, scala]"},{"location":"Metadata/#partitionschema-structtype","text":"partitionSchema is the < > as StructFields (and defined in the < >). NOTE: partitionSchema throws an IllegalArgumentException for undefined fields that were used for the < > but not defined in the < >. NOTE: partitionSchema is used when...FIXME == [[dataSchema]] dataSchema (Lazy) Property","title":"partitionSchema: StructType"},{"location":"Metadata/#source-scala_2","text":"","title":"[source, scala]"},{"location":"Metadata/#dataschema-structtype","text":"dataSchema ...FIXME NOTE: dataSchema is used when...FIXME == [[schema]] schema (Lazy) Property","title":"dataSchema: StructType"},{"location":"Metadata/#source-scala_3","text":"","title":"[source, scala]"},{"location":"Metadata/#schema-structtype","text":"schema is a deserialized < > (from JSON format) to StructType .","title":"schema: StructType"},{"location":"Metadata/#note_1","text":"schema is used when: Metadata is requested for the schema of the < > and the < > DeltaLog is requested for an DeltaLog.md#createRelation[insertable HadoopFsRelation for batch queries] (for the data schema), to DeltaLog.md#upgradeProtocol[upgrade protocol], a DeltaLog.md#createDataFrame[DataFrame for given AddFiles] DeltaTableUtils utility is used to DeltaTableUtils.md#combineWithCatalogMetadata[combineWithCatalogMetadata] OptimisticTransactionImpl is requested to OptimisticTransactionImpl.md#verifyNewMetadata[verifyNewMetadata]","title":"[NOTE]"},{"location":"Metadata/#fixme-there-are-other-uses","text":"","title":"* ...FIXME (there are other uses)"},{"location":"MetadataCleanup/","text":"MetadataCleanup \u00b6 MetadataCleanup is an abstraction of < > that can < > the < >. [[implementations]][[self]] NOTE: < > is the default and only known MetadataCleanup in Delta Lake. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.delta.MetadataCleanup logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.MetadataCleanup=ALL Refer to Logging .. \u00b6 == [[doLogCleanup]] doLogCleanup Method [source, scala] \u00b6 doLogCleanup(): Unit \u00b6 [NOTE] \u00b6 doLogCleanup is part of the < > to...FIXME. Interestingly, this MetadataCleanup and < > abstractions require to be used with < > only. \u00b6 doLogCleanup < > when the < > table property is enabled. == [[enableExpiredLogCleanup]] enableExpiredLogCleanup Table Property -- enableExpiredLogCleanup Method [source, scala] \u00b6 enableExpiredLogCleanup: Boolean \u00b6 enableExpiredLogCleanup gives the value of < > table property (< > the < >). NOTE: enableExpiredLogCleanup is used exclusively when MetadataCleanup is requested to < >. == [[deltaRetentionMillis]] logRetentionDuration Table Property -- deltaRetentionMillis Method [source, scala] \u00b6 deltaRetentionMillis: Long \u00b6 deltaRetentionMillis gives the value of < > table property (< > the < >). NOTE: deltaRetentionMillis is used when...FIXME == [[cleanUpExpiredLogs]] cleanUpExpiredLogs Internal Method [source, scala] \u00b6 cleanUpExpiredLogs(): Unit \u00b6 cleanUpExpiredLogs calculates a so-called fileCutOffTime based on the < > and the < > table property. cleanUpExpiredLogs prints out the following INFO message to the logs: Starting the deletion of log files older than [date] cleanUpExpiredLogs < > (based on the fileCutOffTime ) and deletes the files (using Hadoop's link:++ https://hadoop.apache.org/docs/r2.6.5/api/org/apache/hadoop/fs/FileSystem.html#delete(org.apache.hadoop.fs.Path,%20boolean)++[FileSystem.delete ] non-recursively). In the end, cleanUpExpiredLogs prints out the following INFO message to the logs: Deleted numDeleted log files older than [date] NOTE: cleanUpExpiredLogs is used exclusively when MetadataCleanup is requested to < >. == [[listExpiredDeltaLogs]] Finding Expired Delta Logs -- listExpiredDeltaLogs Internal Method [source, scala] \u00b6 listExpiredDeltaLogs( fileCutOffTime: Long): Iterator[FileStatus] listExpiredDeltaLogs ...FIXME requests the < > for the < > that are (lexicographically) greater or equal to the 0 th checkpoint file (per < > format) of the < > and < > files in the < > (of the < >). In the end, listExpiredDeltaLogs creates a BufferingLogDeletionIterator that...FIXME NOTE: listExpiredDeltaLogs is used exclusively when MetadataCleanup is requested to < >.","title":"MetadataCleanup"},{"location":"MetadataCleanup/#metadatacleanup","text":"MetadataCleanup is an abstraction of < > that can < > the < >. [[implementations]][[self]] NOTE: < > is the default and only known MetadataCleanup in Delta Lake. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.delta.MetadataCleanup logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.MetadataCleanup=ALL","title":"MetadataCleanup"},{"location":"MetadataCleanup/#refer-to-logging","text":"== [[doLogCleanup]] doLogCleanup Method","title":"Refer to Logging.."},{"location":"MetadataCleanup/#source-scala","text":"","title":"[source, scala]"},{"location":"MetadataCleanup/#dologcleanup-unit","text":"","title":"doLogCleanup(): Unit"},{"location":"MetadataCleanup/#note","text":"doLogCleanup is part of the < > to...FIXME.","title":"[NOTE]"},{"location":"MetadataCleanup/#interestingly-this-metadatacleanup-and-abstractions-require-to-be-used-with-only","text":"doLogCleanup < > when the < > table property is enabled. == [[enableExpiredLogCleanup]] enableExpiredLogCleanup Table Property -- enableExpiredLogCleanup Method","title":"Interestingly, this MetadataCleanup and &lt;&gt; abstractions require to be used with &lt;&gt; only."},{"location":"MetadataCleanup/#source-scala_1","text":"","title":"[source, scala]"},{"location":"MetadataCleanup/#enableexpiredlogcleanup-boolean","text":"enableExpiredLogCleanup gives the value of < > table property (< > the < >). NOTE: enableExpiredLogCleanup is used exclusively when MetadataCleanup is requested to < >. == [[deltaRetentionMillis]] logRetentionDuration Table Property -- deltaRetentionMillis Method","title":"enableExpiredLogCleanup: Boolean"},{"location":"MetadataCleanup/#source-scala_2","text":"","title":"[source, scala]"},{"location":"MetadataCleanup/#deltaretentionmillis-long","text":"deltaRetentionMillis gives the value of < > table property (< > the < >). NOTE: deltaRetentionMillis is used when...FIXME == [[cleanUpExpiredLogs]] cleanUpExpiredLogs Internal Method","title":"deltaRetentionMillis: Long"},{"location":"MetadataCleanup/#source-scala_3","text":"","title":"[source, scala]"},{"location":"MetadataCleanup/#cleanupexpiredlogs-unit","text":"cleanUpExpiredLogs calculates a so-called fileCutOffTime based on the < > and the < > table property. cleanUpExpiredLogs prints out the following INFO message to the logs: Starting the deletion of log files older than [date] cleanUpExpiredLogs < > (based on the fileCutOffTime ) and deletes the files (using Hadoop's link:++ https://hadoop.apache.org/docs/r2.6.5/api/org/apache/hadoop/fs/FileSystem.html#delete(org.apache.hadoop.fs.Path,%20boolean)++[FileSystem.delete ] non-recursively). In the end, cleanUpExpiredLogs prints out the following INFO message to the logs: Deleted numDeleted log files older than [date] NOTE: cleanUpExpiredLogs is used exclusively when MetadataCleanup is requested to < >. == [[listExpiredDeltaLogs]] Finding Expired Delta Logs -- listExpiredDeltaLogs Internal Method","title":"cleanUpExpiredLogs(): Unit"},{"location":"MetadataCleanup/#source-scala_4","text":"listExpiredDeltaLogs( fileCutOffTime: Long): Iterator[FileStatus] listExpiredDeltaLogs ...FIXME requests the < > for the < > that are (lexicographically) greater or equal to the 0 th checkpoint file (per < > format) of the < > and < > files in the < > (of the < >). In the end, listExpiredDeltaLogs creates a BufferingLogDeletionIterator that...FIXME NOTE: listExpiredDeltaLogs is used exclusively when MetadataCleanup is requested to < >.","title":"[source, scala]"},{"location":"Operation/","text":"Operation \u00b6 Operation is an abstraction of operations that can be executed on Delta tables. Operation is described by a name and parameters (that are simply used to create a CommitInfo for OptimisticTransactionImpl when committed and, as a way to bypass a transaction, ConvertToDeltaCommand ). Operation may have performance metrics . Contract \u00b6 parameters \u00b6 parameters : Map [ String , Any ] Parameters of the operation (to create a CommitInfo with the JSON-encoded values ) Used when Operation is requested for parameters with the values in JSON format Implementations \u00b6 Sealed Abstract Class Operation is a Scala sealed abstract class which means that all of the implementations are in the same compilation unit (a single file). AddColumns ChangeColumn ComputeStats Convert CreateTable Delete FileNotificationRetention Fsck ManualUpdate Optimize ReplaceColumns ReplaceTable ResetZCubeInfo SetTableProperties StreamingUpdate Truncate UnsetTableProperties Update UpdateColumnMetadata UpdateSchema UpgradeProtocol Write Merge \u00b6 Recorded when a merge operation is committed to a Delta table (when MergeIntoCommand is executed) Creating Instance \u00b6 Operation takes the following to be created: Name Abstract Class Operation is an abstract class and cannot be created directly. It is created indirectly for the concrete Operations . Serializing Parameter Values (to JSON Format) \u00b6 jsonEncodedValues : Map [ String , String ] jsonEncodedValues converts the values of the parameters to JSON format. jsonEncodedValues is used when: OptimisticTransactionImpl is requested to commit ConvertToDeltaCommand command is requested to streamWrite operationMetrics Registry \u00b6 operationMetrics : Set [ String ] operationMetrics is empty by default (and is expected to be overriden by concrete operations ) operationMetrics is used when Operation is requested to transformMetrics . transformMetrics Method \u00b6 transformMetrics ( metrics : Map [ String , SQLMetric ]) : Map [ String , String ] transformMetrics returns a collection of performance metrics ( SQLMetric ) and their values (as a text) that are defined as the operationMetrics . transformMetrics is used when SQLMetricsReporting is requested to getMetricsForOperation .","title":"Operation"},{"location":"Operation/#operation","text":"Operation is an abstraction of operations that can be executed on Delta tables. Operation is described by a name and parameters (that are simply used to create a CommitInfo for OptimisticTransactionImpl when committed and, as a way to bypass a transaction, ConvertToDeltaCommand ). Operation may have performance metrics .","title":"Operation"},{"location":"Operation/#contract","text":"","title":"Contract"},{"location":"Operation/#parameters","text":"parameters : Map [ String , Any ] Parameters of the operation (to create a CommitInfo with the JSON-encoded values ) Used when Operation is requested for parameters with the values in JSON format","title":" parameters"},{"location":"Operation/#implementations","text":"Sealed Abstract Class Operation is a Scala sealed abstract class which means that all of the implementations are in the same compilation unit (a single file). AddColumns ChangeColumn ComputeStats Convert CreateTable Delete FileNotificationRetention Fsck ManualUpdate Optimize ReplaceColumns ReplaceTable ResetZCubeInfo SetTableProperties StreamingUpdate Truncate UnsetTableProperties Update UpdateColumnMetadata UpdateSchema UpgradeProtocol Write","title":"Implementations"},{"location":"Operation/#merge","text":"Recorded when a merge operation is committed to a Delta table (when MergeIntoCommand is executed)","title":"Merge"},{"location":"Operation/#creating-instance","text":"Operation takes the following to be created: Name Abstract Class Operation is an abstract class and cannot be created directly. It is created indirectly for the concrete Operations .","title":"Creating Instance"},{"location":"Operation/#serializing-parameter-values-to-json-format","text":"jsonEncodedValues : Map [ String , String ] jsonEncodedValues converts the values of the parameters to JSON format. jsonEncodedValues is used when: OptimisticTransactionImpl is requested to commit ConvertToDeltaCommand command is requested to streamWrite","title":" Serializing Parameter Values (to JSON Format)"},{"location":"Operation/#operationmetrics-registry","text":"operationMetrics : Set [ String ] operationMetrics is empty by default (and is expected to be overriden by concrete operations ) operationMetrics is used when Operation is requested to transformMetrics .","title":" operationMetrics Registry"},{"location":"Operation/#transformmetrics-method","text":"transformMetrics ( metrics : Map [ String , SQLMetric ]) : Map [ String , String ] transformMetrics returns a collection of performance metrics ( SQLMetric ) and their values (as a text) that are defined as the operationMetrics . transformMetrics is used when SQLMetricsReporting is requested to getMetricsForOperation .","title":" transformMetrics Method"},{"location":"OptimisticTransaction/","text":"OptimisticTransaction \u00b6 OptimisticTransaction is an OptimisticTransactionImpl.md[] (which seems more of a class name change than anything more important). OptimisticTransaction is < > for changes to a < > at a given < >. When OptimisticTransaction (as a < >) is attempted to be < > (that does < > internally), the < > (of the < >) is requested to < >, e.g. _delta_log/00000000000000000001.json for the attempt version 1 . Only when a FileAlreadyExistsException is thrown a commit is considered unsuccessful and < >. OptimisticTransaction can be associated with a thread as an < >. == [[creating-instance]] Creating Instance OptimisticTransaction takes the following to be created: [[deltaLog]] DeltaLog.md[] [[snapshot]] Snapshot.md[] [[clock]] Clock NOTE: The < > and < > are part of the < > contract (which in turn inherits them as a TransactionalWrite.md[] and changes to val from def ). OptimisticTransaction is created when DeltaLog is used for the following: DeltaLog.md#startTransaction[Starting a new transaction] DeltaLog.md#withNewTransaction[Executing a single-threaded operation (in a new transaction)] (for < >, < >, < >, and < > commands as well as for < > for < >) == [[active]] Active Thread-Local OptimisticTransaction [source, scala] \u00b6 active: ThreadLocal[OptimisticTransaction] \u00b6 active is a Java https://docs.oracle.com/javase/8/docs/api/java/lang/ThreadLocal.html[ThreadLocal ] with the < > of the current thread. ThreadLocal provides thread-local variables. These variables differ from their normal counterparts in that each thread that accesses one (via its get or set method) has its own, independently initialized copy of the variable. ThreadLocal instances are typically private static fields in classes that wish to associate state with a thread (e.g., a user ID or Transaction ID). active is assigned to the current thread using < > utility and cleared in < >. active is available using < > utility. There can only be one active OptimisticTransaction (or an IllegalStateException is thrown). == [[utilities]] Utilities === [[setActive]] setActive [source, scala] \u00b6 setActive( txn: OptimisticTransaction): Unit setActive simply associates the given OptimisticTransaction as < > with the current thread. setActive throws an IllegalStateException if there is an active OptimisticTransaction already associated: Cannot set a new txn as active when one is already active setActive is used when DeltaLog is requested to < >. === [[clearActive]] clearActive [source, scala] \u00b6 clearActive(): Unit \u00b6 clearActive simply clears the < > transaction (so no transaction is associated with a thread). clearActive is used when DeltaLog is requested to < >. === [[getActive]] getActive [source, scala] \u00b6 getActive(): Option[OptimisticTransaction] \u00b6 getActive simply returns the < > transaction. getActive seems unused. == [[logging]] Logging Enable ALL logging level for org.apache.spark.sql.delta.OptimisticTransaction logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.sql.delta.OptimisticTransaction=ALL \u00b6 Refer to Logging . == [[demo]] Demo [source,scala] \u00b6 import org.apache.spark.sql.delta.DeltaLog val dir = \"/tmp/delta/users\" val log = DeltaLog.forTable(spark, dir) val txn = log.startTransaction() // ...changes to a delta table... val addFile = AddFile(\"foo\", Map.empty, 1L, System.currentTimeMillis(), dataChange = true) val removeFile = addFile.remove val actions = addFile :: removeFile :: Nil txn.commit(actions, op) // You could do the following instead deltaLog.withNewTransaction { txn => // ...transactional changes to a delta table }","title":"OptimisticTransaction"},{"location":"OptimisticTransaction/#optimistictransaction","text":"OptimisticTransaction is an OptimisticTransactionImpl.md[] (which seems more of a class name change than anything more important). OptimisticTransaction is < > for changes to a < > at a given < >. When OptimisticTransaction (as a < >) is attempted to be < > (that does < > internally), the < > (of the < >) is requested to < >, e.g. _delta_log/00000000000000000001.json for the attempt version 1 . Only when a FileAlreadyExistsException is thrown a commit is considered unsuccessful and < >. OptimisticTransaction can be associated with a thread as an < >. == [[creating-instance]] Creating Instance OptimisticTransaction takes the following to be created: [[deltaLog]] DeltaLog.md[] [[snapshot]] Snapshot.md[] [[clock]] Clock NOTE: The < > and < > are part of the < > contract (which in turn inherits them as a TransactionalWrite.md[] and changes to val from def ). OptimisticTransaction is created when DeltaLog is used for the following: DeltaLog.md#startTransaction[Starting a new transaction] DeltaLog.md#withNewTransaction[Executing a single-threaded operation (in a new transaction)] (for < >, < >, < >, and < > commands as well as for < > for < >) == [[active]] Active Thread-Local OptimisticTransaction","title":"OptimisticTransaction"},{"location":"OptimisticTransaction/#source-scala","text":"","title":"[source, scala]"},{"location":"OptimisticTransaction/#active-threadlocaloptimistictransaction","text":"active is a Java https://docs.oracle.com/javase/8/docs/api/java/lang/ThreadLocal.html[ThreadLocal ] with the < > of the current thread. ThreadLocal provides thread-local variables. These variables differ from their normal counterparts in that each thread that accesses one (via its get or set method) has its own, independently initialized copy of the variable. ThreadLocal instances are typically private static fields in classes that wish to associate state with a thread (e.g., a user ID or Transaction ID). active is assigned to the current thread using < > utility and cleared in < >. active is available using < > utility. There can only be one active OptimisticTransaction (or an IllegalStateException is thrown). == [[utilities]] Utilities === [[setActive]] setActive","title":"active: ThreadLocal[OptimisticTransaction]"},{"location":"OptimisticTransaction/#source-scala_1","text":"setActive( txn: OptimisticTransaction): Unit setActive simply associates the given OptimisticTransaction as < > with the current thread. setActive throws an IllegalStateException if there is an active OptimisticTransaction already associated: Cannot set a new txn as active when one is already active setActive is used when DeltaLog is requested to < >. === [[clearActive]] clearActive","title":"[source, scala]"},{"location":"OptimisticTransaction/#source-scala_2","text":"","title":"[source, scala]"},{"location":"OptimisticTransaction/#clearactive-unit","text":"clearActive simply clears the < > transaction (so no transaction is associated with a thread). clearActive is used when DeltaLog is requested to < >. === [[getActive]] getActive","title":"clearActive(): Unit"},{"location":"OptimisticTransaction/#source-scala_3","text":"","title":"[source, scala]"},{"location":"OptimisticTransaction/#getactive-optionoptimistictransaction","text":"getActive simply returns the < > transaction. getActive seems unused. == [[logging]] Logging Enable ALL logging level for org.apache.spark.sql.delta.OptimisticTransaction logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"getActive(): Option[OptimisticTransaction]"},{"location":"OptimisticTransaction/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"OptimisticTransaction/#log4jloggerorgapachesparksqldeltaoptimistictransactionall","text":"Refer to Logging . == [[demo]] Demo","title":"log4j.logger.org.apache.spark.sql.delta.OptimisticTransaction=ALL"},{"location":"OptimisticTransaction/#sourcescala","text":"import org.apache.spark.sql.delta.DeltaLog val dir = \"/tmp/delta/users\" val log = DeltaLog.forTable(spark, dir) val txn = log.startTransaction() // ...changes to a delta table... val addFile = AddFile(\"foo\", Map.empty, 1L, System.currentTimeMillis(), dataChange = true) val removeFile = addFile.remove val actions = addFile :: removeFile :: Nil txn.commit(actions, op) // You could do the following instead deltaLog.withNewTransaction { txn => // ...transactional changes to a delta table }","title":"[source,scala]"},{"location":"OptimisticTransactionImpl/","text":"OptimisticTransactionImpl \u00b6 OptimisticTransactionImpl is an < > of the TransactionalWrite.md[] abstraction for < > that can modify a < > (at a given < >) and can be < > eventually. In other words, OptimisticTransactionImpl is a set of Action.md[actions] as part of an Operation.md[]. == [[contract]] Contract === [[clock]] clock [source,scala] \u00b6 clock: Clock \u00b6 === [[deltaLog]] deltaLog [source,scala] \u00b6 deltaLog: DeltaLog \u00b6 DeltaLog.md[] (of the delta table) that this transaction is changing deltaLog is part of the TransactionalWrite.md#deltaLog[TransactionalWrite] contract and seems to change it to val (from def ). === [[snapshot]] snapshot [source,scala] \u00b6 snapshot: Snapshot \u00b6 Snapshot.md[] (of the < >) that this transaction is changing snapshot is part of the TransactionalWrite.md#deltaLog[TransactionalWrite] contract and seems to change it to val (from def ). == [[implementations]] Implementations OptimisticTransaction.md[] is the default and only known OptimisticTransactionImpl in Delta Lake. == [[metadata]] metadata Method [source, scala] \u00b6 metadata: Metadata \u00b6 metadata is either the < > (if defined) or the < >. metadata is part of the TransactionalWrite.md#metadata[TransactionalWrite] abstraction. == [[readVersion]] readVersion Method [source, scala] \u00b6 readVersion: Long \u00b6 readVersion simply requests the < > for the < >. readVersion is used when: OptimisticTransactionImpl is requested for < >, to < > and < > ConvertToDeltaCommand is requested to < > WriteIntoDelta is requested to < > ImplicitMetadataOperation is requested to < > == [[updateMetadata]] Updating Metadata [source, scala] \u00b6 updateMetadata( metadata: Metadata): Unit updateMetadata updates the < > internal property based on the < >: For -1 , updateMetadata updates the < > of the given metadata with a < > based on the SQLConf (of the active SparkSession ), the < > of the given metadata and a new < > For other versions, updateMetadata leaves the given < > unchanged [[updateMetadata-AssertionError-hasWritten]] updateMetadata throws an AssertionError when the < > flag is enabled ( true ): Cannot update the metadata in a transaction that has already written data. updateMetadata throws an AssertionError when the < > is not empty: Cannot change the metadata more than once in a transaction. updateMetadata is used when: < > is executed (and requested to < >) ImplicitMetadataOperation is requested to < > == [[filterFiles]] Files To Scan Matching Given Predicates [source, scala] \u00b6 filterFiles(): Seq[AddFile] // <1> filterFiles( filters: Seq[Expression]): Seq[AddFile] <1> Uses true literal to mean that all files match filterFiles gives the AddFile.md[files] to scan based on the given predicates (filter expressions). Internally, filterFiles requests the < > for the PartitionFiltering.md#filesForScan[filesForScan] (for no projection attributes and the given filters). filterFiles finds the DeltaTableUtils.md#isPredicatePartitionColumnsOnly[partition predicates] among the given filters (and the Metadata.md#partitionColumns[partition columns] of the < >). filterFiles registers ( adds ) the partition predicates (in the < > internal registry) and the files to scan (in the < > internal registry). filterFiles is used when: WriteIntoDelta is requested to WriteIntoDelta.md#write[write] DeltaSink is requested to DeltaSink.md#addBatch[add a streaming micro-batch] (with Complete output mode to mark all to be removed) DeleteCommand.md[DeleteCommand], MergeIntoCommand.md[MergeIntoCommand] and UpdateCommand.md[UpdateCommand] are executed == [[readWholeTable]] readWholeTable Method [source, scala] \u00b6 readWholeTable(): Unit \u00b6 readWholeTable simply adds True literal to the < > internal registry. readWholeTable is used when DeltaSink is requested to DeltaSink.md#addBatch[add a streaming micro-batch] (and the batch reads the same Delta table as this sink is going to write to). Committing Transaction \u00b6 commit ( actions : Seq [ Action ], op : DeltaOperations.Operation ) : Long commit commits the transaction (with the Action s and a given Operation ) [[commit-prepareCommit]] commit firstly < > (that gives the final actions to commit that may be different from the given < >). [[commit-isolationLevelToUse]] commit determines the isolation level for this commit by checking whether any < > (in the given < >) has the < > flag on ( true ). With no data changed, commit uses SnapshotIsolation else Serializable . [[commit-isBlindAppend]] commit...FIXME [[commit-commitInfo]] commit...FIXME [[commit-registerPostCommitHook]] commit < > the < > post-commit hook when there is a < > among the actions and the < > table property (< > the < >) is enabled ( true ). NOTE: < > table property defaults to false . [[commit-commitVersion]] commit < > with the next version, the actions, attempt number 0 , and the select isolation level. commit prints out the following INFO message to the logs: Committed delta #[commitVersion] to [logPath] [[commit-postCommit]] commit < > (with the version committed and the actions). [[commit-runPostCommitHooks]] In the end, commit < > and returns the version of the successful commit. == [[prepareCommit]] Preparing Commit [source, scala] \u00b6 prepareCommit( actions: Seq[Action], op: DeltaOperations.Operation): Seq[Action] prepareCommit adds the < > action (if available) to the given < >. prepareCommit < > if there was one. prepareCommit...FIXME prepareCommit requests the < > to < >. prepareCommit...FIXME prepareCommit throws an AssertionError when the number of metadata changes in the transaction (by means of < > actions) is above 1 : Cannot change the metadata more than once in a transaction. prepareCommit throws an AssertionError when the < > internal flag is turned on ( true ): Transaction already committed. prepareCommit is used when OptimisticTransactionImpl is requested to < > (at the beginning). == [[postCommit]] Performing Post-Commit Operations [source, scala] \u00b6 postCommit( commitVersion: Long, commitActions: Seq[Action]): Unit postCommit...FIXME postCommit is used when OptimisticTransactionImpl is requested to < > (at the end). == [[commitInfo]] CommitInfo OptimisticTransactionImpl creates a CommitInfo.md[] when requested to < > with DeltaSQLConf.md#commitInfo.enabled[spark.databricks.delta.commitInfo.enabled] configuration enabled. OptimisticTransactionImpl uses the CommitInfo to recordDeltaEvent (as a CommitStats). == [[registerPostCommitHook]] Registering Post-Commit Hook [source, scala] \u00b6 registerPostCommitHook( hook: PostCommitHook): Unit registerPostCommitHook registers ( adds ) the given < > to the < > internal registry. NOTE: registerPostCommitHook adds the hook only once. registerPostCommitHook is used when OptimisticTransactionImpl is requested to < > (to register the < > post-commit hook). == [[runPostCommitHooks]] Running Post-Commit Hooks [source, scala] \u00b6 runPostCommitHooks( version: Long, committedActions: Seq[Action]): Unit runPostCommitHooks simply < > every < > registered (in the < > internal registry). runPostCommitHooks < > (making all follow-up operations non-transactional). NOTE: Hooks may create new transactions. For any non-fatal exception, runPostCommitHooks prints out the following ERROR message to the logs, records the delta event, and requests the post-commit hook to < >. Error when executing post-commit hook [name] for commit [version] runPostCommitHooks throws an AssertionError when < > flag is turned off ( false ): Can't call post commit hooks before committing runPostCommitHooks is used when OptimisticTransactionImpl is requested to < >. == [[doCommit]] Attempting Commit [source, scala] \u00b6 doCommit( attemptVersion: Long, actions: Seq[Action], attemptNumber: Int): Long doCommit returns the given attemptVersion as the commit version if successful or < >. Internally, doCommit prints out the following DEBUG message to the logs: Attempting to commit version [attemptVersion] with [size] actions with [isolationLevel] isolation level [[doCommit-write]] doCommit requests the < > (of the < >) to < > the given < > (serialized to < >) to a < > (e.g. 00000000000000000001.json ) in the < > (of the < >) with the attemptVersion version. NOTE: < > must throw a java.nio.file.FileAlreadyExistsException exception if the delta file already exists. Any FileAlreadyExistsExceptions are caught by < > itself to < >. [[doCommit-postCommitSnapshot]] doCommit requests the < > to < >. [[doCommit-IllegalStateException]] doCommit throws an IllegalStateException if the version of the snapshot after update is smaller than the given attemptVersion version. The committed version is [attemptVersion] but the current version is [version]. [[doCommit-stats]] doCommit records a new CommitStats and returns the given attemptVersion as the commit version. [[doCommit-FileAlreadyExistsException]] doCommit catches FileAlreadyExistsExceptions and < >. doCommit is used when OptimisticTransactionImpl is requested to < > (and < >). == [[checkAndRetry]] Retrying Commit [source, scala] \u00b6 checkAndRetry( checkVersion: Long, actions: Seq[Action], attemptNumber: Int): Long checkAndRetry...FIXME checkAndRetry is used when OptimisticTransactionImpl is requested to < > (and < > that failed with an FileAlreadyExistsException). == [[verifyNewMetadata]] verifyNewMetadata Method [source, scala] \u00b6 verifyNewMetadata( metadata: Metadata): Unit verifyNewMetadata...FIXME verifyNewMetadata is used when OptimisticTransactionImpl is requested to < > and < >. == [[txnVersion]] Looking Up Transaction Version For Given (Streaming Query) ID [source, scala] \u00b6 txnVersion( id: String): Long txnVersion simply registers ( adds ) the given ID in the < > internal registry. In the end, txnVersion requests the < > for the < > or assumes -1 . txnVersion is used when DeltaSink is requested to < >. getOperationMetrics Method \u00b6 getOperationMetrics ( op : Operation ) : Option [ Map [ String , String ]] getOperationMetrics ...FIXME getOperationMetrics is used when OptimisticTransactionImpl is requested to commit . == [[getUserMetadata]] User-Defined Metadata [source,scala] \u00b6 getUserMetadata( op: Operation): Option[String] getUserMetadata returns the Operation.md#userMetadata[userMetadata] of the given Operation.md[] (if defined) or the value of DeltaSQLConf.md#DELTA_USER_METADATA[spark.databricks.delta.commitInfo.userMetadata] configuration property. getUserMetadata is used when OptimisticTransactionImpl is requested to < > (and DeltaSQLConf.md#DELTA_COMMIT_INFO_ENABLED[spark.databricks.delta.commitInfo.enabled] configuration property is enabled). == [[getPrettyPartitionMessage]] getPrettyPartitionMessage Method [source,scala] \u00b6 getPrettyPartitionMessage( partitionValues: Map[String, String]): String getPrettyPartitionMessage...FIXME getPrettyPartitionMessage is used when...FIXME == [[getNextAttemptVersion]] getNextAttemptVersion Internal Method [source,scala] \u00b6 getNextAttemptVersion( previousAttemptVersion: Long): Long getNextAttemptVersion...FIXME getNextAttemptVersion is used when OptimisticTransactionImpl is requested to < >. == [[internal-registries]] Internal Registries === [[postCommitHooks]] Post-Commit Hooks [source, scala] \u00b6 postCommitHooks: ArrayBuffer[PostCommitHook] \u00b6 OptimisticTransactionImpl manages PostCommitHook.md[]s that will be < > right after a < > is successful. Post-commit hooks can be < >, but only the < > post-commit hook is supported (when...FIXME). === [[newMetadata]] newMetadata [source, scala] \u00b6 newMetadata: Option[Metadata] \u00b6 OptimisticTransactionImpl uses the newMetadata internal registry for a new < > that should be committed with this transaction. newMetadata is initially undefined ( None ). It can be < > only once and before the transaction < >. newMetadata is used when < > (and < > for statistics). newMetadata is available using < > method. === [[readPredicates]] readPredicates [source,scala] \u00b6 readPredicates: ArrayBuffer[Expression] \u00b6 readPredicates holds predicate expressions for partitions the transaction is modifying. readPredicates is added a new predicate expression when < > and < >. readPredicates is used when < >. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | committed a| [[committed]] Flag that controls whether the transaction is < > or not (and prevents < > from being executed again) Default: false Enabled (set to true ) exclusively in < > | dependsOnFiles a| [[dependsOnFiles]] Flag that...FIXME Default: false Enabled (set to true ) in < >, < > Used in < > and < > | readFiles a| [[readFiles]] | readTxn a| [[readTxn]] Streaming query IDs that have been seen by this transaction A new queryId is added when OptimisticTransactionImpl is requested for < > Used when OptimisticTransactionImpl is requested to < > (to fail with a ConcurrentTransactionException for idempotent transactions that have conflicted) | snapshotMetadata a| [[snapshotMetadata]] < > of the < > |===","title":"OptimisticTransactionImpl"},{"location":"OptimisticTransactionImpl/#optimistictransactionimpl","text":"OptimisticTransactionImpl is an < > of the TransactionalWrite.md[] abstraction for < > that can modify a < > (at a given < >) and can be < > eventually. In other words, OptimisticTransactionImpl is a set of Action.md[actions] as part of an Operation.md[]. == [[contract]] Contract === [[clock]] clock","title":"OptimisticTransactionImpl"},{"location":"OptimisticTransactionImpl/#sourcescala","text":"","title":"[source,scala]"},{"location":"OptimisticTransactionImpl/#clock-clock","text":"=== [[deltaLog]] deltaLog","title":"clock: Clock"},{"location":"OptimisticTransactionImpl/#sourcescala_1","text":"","title":"[source,scala]"},{"location":"OptimisticTransactionImpl/#deltalog-deltalog","text":"DeltaLog.md[] (of the delta table) that this transaction is changing deltaLog is part of the TransactionalWrite.md#deltaLog[TransactionalWrite] contract and seems to change it to val (from def ). === [[snapshot]] snapshot","title":"deltaLog: DeltaLog"},{"location":"OptimisticTransactionImpl/#sourcescala_2","text":"","title":"[source,scala]"},{"location":"OptimisticTransactionImpl/#snapshot-snapshot","text":"Snapshot.md[] (of the < >) that this transaction is changing snapshot is part of the TransactionalWrite.md#deltaLog[TransactionalWrite] contract and seems to change it to val (from def ). == [[implementations]] Implementations OptimisticTransaction.md[] is the default and only known OptimisticTransactionImpl in Delta Lake. == [[metadata]] metadata Method","title":"snapshot: Snapshot"},{"location":"OptimisticTransactionImpl/#source-scala","text":"","title":"[source, scala]"},{"location":"OptimisticTransactionImpl/#metadata-metadata","text":"metadata is either the < > (if defined) or the < >. metadata is part of the TransactionalWrite.md#metadata[TransactionalWrite] abstraction. == [[readVersion]] readVersion Method","title":"metadata: Metadata"},{"location":"OptimisticTransactionImpl/#source-scala_1","text":"","title":"[source, scala]"},{"location":"OptimisticTransactionImpl/#readversion-long","text":"readVersion simply requests the < > for the < >. readVersion is used when: OptimisticTransactionImpl is requested for < >, to < > and < > ConvertToDeltaCommand is requested to < > WriteIntoDelta is requested to < > ImplicitMetadataOperation is requested to < > == [[updateMetadata]] Updating Metadata","title":"readVersion: Long"},{"location":"OptimisticTransactionImpl/#source-scala_2","text":"updateMetadata( metadata: Metadata): Unit updateMetadata updates the < > internal property based on the < >: For -1 , updateMetadata updates the < > of the given metadata with a < > based on the SQLConf (of the active SparkSession ), the < > of the given metadata and a new < > For other versions, updateMetadata leaves the given < > unchanged [[updateMetadata-AssertionError-hasWritten]] updateMetadata throws an AssertionError when the < > flag is enabled ( true ): Cannot update the metadata in a transaction that has already written data. updateMetadata throws an AssertionError when the < > is not empty: Cannot change the metadata more than once in a transaction. updateMetadata is used when: < > is executed (and requested to < >) ImplicitMetadataOperation is requested to < > == [[filterFiles]] Files To Scan Matching Given Predicates","title":"[source, scala]"},{"location":"OptimisticTransactionImpl/#source-scala_3","text":"filterFiles(): Seq[AddFile] // <1> filterFiles( filters: Seq[Expression]): Seq[AddFile] <1> Uses true literal to mean that all files match filterFiles gives the AddFile.md[files] to scan based on the given predicates (filter expressions). Internally, filterFiles requests the < > for the PartitionFiltering.md#filesForScan[filesForScan] (for no projection attributes and the given filters). filterFiles finds the DeltaTableUtils.md#isPredicatePartitionColumnsOnly[partition predicates] among the given filters (and the Metadata.md#partitionColumns[partition columns] of the < >). filterFiles registers ( adds ) the partition predicates (in the < > internal registry) and the files to scan (in the < > internal registry). filterFiles is used when: WriteIntoDelta is requested to WriteIntoDelta.md#write[write] DeltaSink is requested to DeltaSink.md#addBatch[add a streaming micro-batch] (with Complete output mode to mark all to be removed) DeleteCommand.md[DeleteCommand], MergeIntoCommand.md[MergeIntoCommand] and UpdateCommand.md[UpdateCommand] are executed == [[readWholeTable]] readWholeTable Method","title":"[source, scala]"},{"location":"OptimisticTransactionImpl/#source-scala_4","text":"","title":"[source, scala]"},{"location":"OptimisticTransactionImpl/#readwholetable-unit","text":"readWholeTable simply adds True literal to the < > internal registry. readWholeTable is used when DeltaSink is requested to DeltaSink.md#addBatch[add a streaming micro-batch] (and the batch reads the same Delta table as this sink is going to write to).","title":"readWholeTable(): Unit"},{"location":"OptimisticTransactionImpl/#committing-transaction","text":"commit ( actions : Seq [ Action ], op : DeltaOperations.Operation ) : Long commit commits the transaction (with the Action s and a given Operation ) [[commit-prepareCommit]] commit firstly < > (that gives the final actions to commit that may be different from the given < >). [[commit-isolationLevelToUse]] commit determines the isolation level for this commit by checking whether any < > (in the given < >) has the < > flag on ( true ). With no data changed, commit uses SnapshotIsolation else Serializable . [[commit-isBlindAppend]] commit...FIXME [[commit-commitInfo]] commit...FIXME [[commit-registerPostCommitHook]] commit < > the < > post-commit hook when there is a < > among the actions and the < > table property (< > the < >) is enabled ( true ). NOTE: < > table property defaults to false . [[commit-commitVersion]] commit < > with the next version, the actions, attempt number 0 , and the select isolation level. commit prints out the following INFO message to the logs: Committed delta #[commitVersion] to [logPath] [[commit-postCommit]] commit < > (with the version committed and the actions). [[commit-runPostCommitHooks]] In the end, commit < > and returns the version of the successful commit. == [[prepareCommit]] Preparing Commit","title":" Committing Transaction"},{"location":"OptimisticTransactionImpl/#source-scala_5","text":"prepareCommit( actions: Seq[Action], op: DeltaOperations.Operation): Seq[Action] prepareCommit adds the < > action (if available) to the given < >. prepareCommit < > if there was one. prepareCommit...FIXME prepareCommit requests the < > to < >. prepareCommit...FIXME prepareCommit throws an AssertionError when the number of metadata changes in the transaction (by means of < > actions) is above 1 : Cannot change the metadata more than once in a transaction. prepareCommit throws an AssertionError when the < > internal flag is turned on ( true ): Transaction already committed. prepareCommit is used when OptimisticTransactionImpl is requested to < > (at the beginning). == [[postCommit]] Performing Post-Commit Operations","title":"[source, scala]"},{"location":"OptimisticTransactionImpl/#source-scala_6","text":"postCommit( commitVersion: Long, commitActions: Seq[Action]): Unit postCommit...FIXME postCommit is used when OptimisticTransactionImpl is requested to < > (at the end). == [[commitInfo]] CommitInfo OptimisticTransactionImpl creates a CommitInfo.md[] when requested to < > with DeltaSQLConf.md#commitInfo.enabled[spark.databricks.delta.commitInfo.enabled] configuration enabled. OptimisticTransactionImpl uses the CommitInfo to recordDeltaEvent (as a CommitStats). == [[registerPostCommitHook]] Registering Post-Commit Hook","title":"[source, scala]"},{"location":"OptimisticTransactionImpl/#source-scala_7","text":"registerPostCommitHook( hook: PostCommitHook): Unit registerPostCommitHook registers ( adds ) the given < > to the < > internal registry. NOTE: registerPostCommitHook adds the hook only once. registerPostCommitHook is used when OptimisticTransactionImpl is requested to < > (to register the < > post-commit hook). == [[runPostCommitHooks]] Running Post-Commit Hooks","title":"[source, scala]"},{"location":"OptimisticTransactionImpl/#source-scala_8","text":"runPostCommitHooks( version: Long, committedActions: Seq[Action]): Unit runPostCommitHooks simply < > every < > registered (in the < > internal registry). runPostCommitHooks < > (making all follow-up operations non-transactional). NOTE: Hooks may create new transactions. For any non-fatal exception, runPostCommitHooks prints out the following ERROR message to the logs, records the delta event, and requests the post-commit hook to < >. Error when executing post-commit hook [name] for commit [version] runPostCommitHooks throws an AssertionError when < > flag is turned off ( false ): Can't call post commit hooks before committing runPostCommitHooks is used when OptimisticTransactionImpl is requested to < >. == [[doCommit]] Attempting Commit","title":"[source, scala]"},{"location":"OptimisticTransactionImpl/#source-scala_9","text":"doCommit( attemptVersion: Long, actions: Seq[Action], attemptNumber: Int): Long doCommit returns the given attemptVersion as the commit version if successful or < >. Internally, doCommit prints out the following DEBUG message to the logs: Attempting to commit version [attemptVersion] with [size] actions with [isolationLevel] isolation level [[doCommit-write]] doCommit requests the < > (of the < >) to < > the given < > (serialized to < >) to a < > (e.g. 00000000000000000001.json ) in the < > (of the < >) with the attemptVersion version. NOTE: < > must throw a java.nio.file.FileAlreadyExistsException exception if the delta file already exists. Any FileAlreadyExistsExceptions are caught by < > itself to < >. [[doCommit-postCommitSnapshot]] doCommit requests the < > to < >. [[doCommit-IllegalStateException]] doCommit throws an IllegalStateException if the version of the snapshot after update is smaller than the given attemptVersion version. The committed version is [attemptVersion] but the current version is [version]. [[doCommit-stats]] doCommit records a new CommitStats and returns the given attemptVersion as the commit version. [[doCommit-FileAlreadyExistsException]] doCommit catches FileAlreadyExistsExceptions and < >. doCommit is used when OptimisticTransactionImpl is requested to < > (and < >). == [[checkAndRetry]] Retrying Commit","title":"[source, scala]"},{"location":"OptimisticTransactionImpl/#source-scala_10","text":"checkAndRetry( checkVersion: Long, actions: Seq[Action], attemptNumber: Int): Long checkAndRetry...FIXME checkAndRetry is used when OptimisticTransactionImpl is requested to < > (and < > that failed with an FileAlreadyExistsException). == [[verifyNewMetadata]] verifyNewMetadata Method","title":"[source, scala]"},{"location":"OptimisticTransactionImpl/#source-scala_11","text":"verifyNewMetadata( metadata: Metadata): Unit verifyNewMetadata...FIXME verifyNewMetadata is used when OptimisticTransactionImpl is requested to < > and < >. == [[txnVersion]] Looking Up Transaction Version For Given (Streaming Query) ID","title":"[source, scala]"},{"location":"OptimisticTransactionImpl/#source-scala_12","text":"txnVersion( id: String): Long txnVersion simply registers ( adds ) the given ID in the < > internal registry. In the end, txnVersion requests the < > for the < > or assumes -1 . txnVersion is used when DeltaSink is requested to < >.","title":"[source, scala]"},{"location":"OptimisticTransactionImpl/#getoperationmetrics-method","text":"getOperationMetrics ( op : Operation ) : Option [ Map [ String , String ]] getOperationMetrics ...FIXME getOperationMetrics is used when OptimisticTransactionImpl is requested to commit . == [[getUserMetadata]] User-Defined Metadata","title":" getOperationMetrics Method"},{"location":"OptimisticTransactionImpl/#sourcescala_3","text":"getUserMetadata( op: Operation): Option[String] getUserMetadata returns the Operation.md#userMetadata[userMetadata] of the given Operation.md[] (if defined) or the value of DeltaSQLConf.md#DELTA_USER_METADATA[spark.databricks.delta.commitInfo.userMetadata] configuration property. getUserMetadata is used when OptimisticTransactionImpl is requested to < > (and DeltaSQLConf.md#DELTA_COMMIT_INFO_ENABLED[spark.databricks.delta.commitInfo.enabled] configuration property is enabled). == [[getPrettyPartitionMessage]] getPrettyPartitionMessage Method","title":"[source,scala]"},{"location":"OptimisticTransactionImpl/#sourcescala_4","text":"getPrettyPartitionMessage( partitionValues: Map[String, String]): String getPrettyPartitionMessage...FIXME getPrettyPartitionMessage is used when...FIXME == [[getNextAttemptVersion]] getNextAttemptVersion Internal Method","title":"[source,scala]"},{"location":"OptimisticTransactionImpl/#sourcescala_5","text":"getNextAttemptVersion( previousAttemptVersion: Long): Long getNextAttemptVersion...FIXME getNextAttemptVersion is used when OptimisticTransactionImpl is requested to < >. == [[internal-registries]] Internal Registries === [[postCommitHooks]] Post-Commit Hooks","title":"[source,scala]"},{"location":"OptimisticTransactionImpl/#source-scala_13","text":"","title":"[source, scala]"},{"location":"OptimisticTransactionImpl/#postcommithooks-arraybufferpostcommithook","text":"OptimisticTransactionImpl manages PostCommitHook.md[]s that will be < > right after a < > is successful. Post-commit hooks can be < >, but only the < > post-commit hook is supported (when...FIXME). === [[newMetadata]] newMetadata","title":"postCommitHooks: ArrayBuffer[PostCommitHook]"},{"location":"OptimisticTransactionImpl/#source-scala_14","text":"","title":"[source, scala]"},{"location":"OptimisticTransactionImpl/#newmetadata-optionmetadata","text":"OptimisticTransactionImpl uses the newMetadata internal registry for a new < > that should be committed with this transaction. newMetadata is initially undefined ( None ). It can be < > only once and before the transaction < >. newMetadata is used when < > (and < > for statistics). newMetadata is available using < > method. === [[readPredicates]] readPredicates","title":"newMetadata: Option[Metadata]"},{"location":"OptimisticTransactionImpl/#sourcescala_6","text":"","title":"[source,scala]"},{"location":"OptimisticTransactionImpl/#readpredicates-arraybufferexpression","text":"readPredicates holds predicate expressions for partitions the transaction is modifying. readPredicates is added a new predicate expression when < > and < >. readPredicates is used when < >. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | committed a| [[committed]] Flag that controls whether the transaction is < > or not (and prevents < > from being executed again) Default: false Enabled (set to true ) exclusively in < > | dependsOnFiles a| [[dependsOnFiles]] Flag that...FIXME Default: false Enabled (set to true ) in < >, < > Used in < > and < > | readFiles a| [[readFiles]] | readTxn a| [[readTxn]] Streaming query IDs that have been seen by this transaction A new queryId is added when OptimisticTransactionImpl is requested for < > Used when OptimisticTransactionImpl is requested to < > (to fail with a ConcurrentTransactionException for idempotent transactions that have conflicted) | snapshotMetadata a| [[snapshotMetadata]] < > of the < > |===","title":"readPredicates: ArrayBuffer[Expression]"},{"location":"PartitionFiltering/","text":"= [[PartitionFiltering]] PartitionFiltering -- Snapshots With Partition Filtering [[self]] PartitionFiltering is an abstraction of < > with < >. [[implementations]] NOTE: Snapshot.md[Snapshot] is the default and only known PartitionFiltering in Delta Lake. == [[filesForScan]] Files to Scan (Matching Projection Attributes and Predicates) -- filesForScan Method [source, scala] \u00b6 filesForScan( projection: Seq[Attribute], filters: Seq[Expression], keepStats: Boolean = false): DeltaScan filesForScan ...FIXME [NOTE] \u00b6 filesForScan is used when: OptimisticTransactionImpl is requested for the OptimisticTransactionImpl.md#filterFiles[files to scan matching given predicates] * TahoeLogFileIndex is requested for the TahoeLogFileIndex.md#matchingFiles[files matching predicates] and the TahoeLogFileIndex.md#inputFiles[input files] \u00b6","title":"PartitionFiltering"},{"location":"PartitionFiltering/#source-scala","text":"filesForScan( projection: Seq[Attribute], filters: Seq[Expression], keepStats: Boolean = false): DeltaScan filesForScan ...FIXME","title":"[source, scala]"},{"location":"PartitionFiltering/#note","text":"filesForScan is used when: OptimisticTransactionImpl is requested for the OptimisticTransactionImpl.md#filterFiles[files to scan matching given predicates]","title":"[NOTE]"},{"location":"PartitionFiltering/#tahoelogfileindex-is-requested-for-the-tahoelogfileindexmdmatchingfilesfiles-matching-predicates-and-the-tahoelogfileindexmdinputfilesinput-files","text":"","title":"* TahoeLogFileIndex is requested for the TahoeLogFileIndex.md#matchingFiles[files matching predicates] and the TahoeLogFileIndex.md#inputFiles[input files]"},{"location":"PostCommitHook/","text":"= PostCommitHook PostCommitHook is an < > of < > that have a < > and can be < > (when OptimisticTransactionImpl is < >). [[contract]] .PostCommitHook Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | handleError a| [[handleError]] [source, scala] \u00b6 handleError( error: Throwable, version: Long): Unit = {} Handles an error while < > Used when OptimisticTransactionImpl is requested to < > (when < >) | name a| [[name]] [source, scala] \u00b6 name: String \u00b6 User-friendly name of the hook for error reporting Used when: DeltaErrors utility is used to < > OptimisticTransactionImpl is requested to < > (when < >) GenerateSymlinkManifestImpl is requested to < > | run a| [[run]] [source, scala] \u00b6 run( spark: SparkSession, txn: OptimisticTransactionImpl, committedActions: Seq[Action]): Unit Executes the post-commit hook Used when OptimisticTransactionImpl is requested to < > (when < >) |=== [[implementations]] NOTE: < > is the default and only known PostCommitHook in Delta Lake.","title":"Post-Commit Hooks"},{"location":"PostCommitHook/#source-scala","text":"handleError( error: Throwable, version: Long): Unit = {} Handles an error while < > Used when OptimisticTransactionImpl is requested to < > (when < >) | name a| [[name]]","title":"[source, scala]"},{"location":"PostCommitHook/#source-scala_1","text":"","title":"[source, scala]"},{"location":"PostCommitHook/#name-string","text":"User-friendly name of the hook for error reporting Used when: DeltaErrors utility is used to < > OptimisticTransactionImpl is requested to < > (when < >) GenerateSymlinkManifestImpl is requested to < > | run a| [[run]]","title":"name: String"},{"location":"PostCommitHook/#source-scala_2","text":"run( spark: SparkSession, txn: OptimisticTransactionImpl, committedActions: Seq[Action]): Unit Executes the post-commit hook Used when OptimisticTransactionImpl is requested to < > (when < >) |=== [[implementations]] NOTE: < > is the default and only known PostCommitHook in Delta Lake.","title":"[source, scala]"},{"location":"PreprocessTableDelete/","text":"PreprocessTableDelete Logical Resolution Rule \u00b6 PreprocessTableDelete is a post-hoc logical resolution rule ( Rule[LogicalPlan] ) to < > in a query plan into DeleteCommand.md[]s. PreprocessTableDelete is installed (injected) into a SparkSession using DeltaSparkSessionExtension.md[]. == [[creating-instance]][[conf]] Creating Instance PreprocessTableDelete takes a single SQLConf to be created. PreprocessTableDelete is created when DeltaSparkSessionExtension is requested to DeltaSparkSessionExtension.md#apply[register Delta SQL support]. == [[apply]] Executing Rule [source, scala] \u00b6 apply( plan: LogicalPlan): LogicalPlan apply resolves ( replaces ) DeltaDelete logical commands (in a logical query plan) into corresponding DeleteCommand.md[]s. apply is part of the Spark SQL's Rule abstraction.","title":"PreprocessTableDelete"},{"location":"PreprocessTableDelete/#preprocesstabledelete-logical-resolution-rule","text":"PreprocessTableDelete is a post-hoc logical resolution rule ( Rule[LogicalPlan] ) to < > in a query plan into DeleteCommand.md[]s. PreprocessTableDelete is installed (injected) into a SparkSession using DeltaSparkSessionExtension.md[]. == [[creating-instance]][[conf]] Creating Instance PreprocessTableDelete takes a single SQLConf to be created. PreprocessTableDelete is created when DeltaSparkSessionExtension is requested to DeltaSparkSessionExtension.md#apply[register Delta SQL support]. == [[apply]] Executing Rule","title":"PreprocessTableDelete Logical Resolution Rule"},{"location":"PreprocessTableDelete/#source-scala","text":"apply( plan: LogicalPlan): LogicalPlan apply resolves ( replaces ) DeltaDelete logical commands (in a logical query plan) into corresponding DeleteCommand.md[]s. apply is part of the Spark SQL's Rule abstraction.","title":"[source, scala]"},{"location":"PreprocessTableMerge/","text":"PreprocessTableMerge Logical Resolution Rule \u00b6 PreprocessTableMerge is a post-hoc logical resolution rule ( Rule[LogicalPlan] ) to < > in a query plan into MergeIntoCommand.md[]s. PreprocessTableMerge is installed (injected) into a SparkSession using DeltaSparkSessionExtension.md[]. == [[creating-instance]][[conf]] Creating Instance PreprocessTableMerge takes a single SQLConf to be created. PreprocessTableMerge is created when: DeltaMergeBuilder is executed DeltaSparkSessionExtension is requested to DeltaSparkSessionExtension.md#apply[register Delta SQL support] == [[apply]] Executing Rule [source, scala] \u00b6 apply( plan: LogicalPlan): LogicalPlan apply resolves ( replaces ) DeltaMergeInto.md[] logical commands (in a logical query plan) into corresponding MergeIntoCommand.md[]s. apply is part of the Spark SQL's Rule abstraction.","title":"PreprocessTableMerge"},{"location":"PreprocessTableMerge/#preprocesstablemerge-logical-resolution-rule","text":"PreprocessTableMerge is a post-hoc logical resolution rule ( Rule[LogicalPlan] ) to < > in a query plan into MergeIntoCommand.md[]s. PreprocessTableMerge is installed (injected) into a SparkSession using DeltaSparkSessionExtension.md[]. == [[creating-instance]][[conf]] Creating Instance PreprocessTableMerge takes a single SQLConf to be created. PreprocessTableMerge is created when: DeltaMergeBuilder is executed DeltaSparkSessionExtension is requested to DeltaSparkSessionExtension.md#apply[register Delta SQL support] == [[apply]] Executing Rule","title":"PreprocessTableMerge Logical Resolution Rule"},{"location":"PreprocessTableMerge/#source-scala","text":"apply( plan: LogicalPlan): LogicalPlan apply resolves ( replaces ) DeltaMergeInto.md[] logical commands (in a logical query plan) into corresponding MergeIntoCommand.md[]s. apply is part of the Spark SQL's Rule abstraction.","title":"[source, scala]"},{"location":"PreprocessTableUpdate/","text":"PreprocessTableUpdate Logical Resolution Rule \u00b6 PreprocessTableUpdate is a post-hoc logical resolution rule ( Rule[LogicalPlan] ) to < > in a query plan into UpdateCommand.md[]s. PreprocessTableUpdate is installed (injected) into a SparkSession using DeltaSparkSessionExtension.md[]. == [[creating-instance]][[conf]] Creating Instance PreprocessTableUpdate takes a single SQLConf to be created. PreprocessTableUpdate is created when DeltaSparkSessionExtension is requested to DeltaSparkSessionExtension.md#apply[register Delta SQL support]. == [[apply]] Executing Rule [source, scala] \u00b6 apply( plan: LogicalPlan): LogicalPlan apply resolves ( replaces ) DeltaUpdateTable logical commands (in a logical query plan) into corresponding UpdateCommand.md[]s. apply is part of the Spark SQL's Rule abstraction.","title":"PreprocessTableUpdate"},{"location":"PreprocessTableUpdate/#preprocesstableupdate-logical-resolution-rule","text":"PreprocessTableUpdate is a post-hoc logical resolution rule ( Rule[LogicalPlan] ) to < > in a query plan into UpdateCommand.md[]s. PreprocessTableUpdate is installed (injected) into a SparkSession using DeltaSparkSessionExtension.md[]. == [[creating-instance]][[conf]] Creating Instance PreprocessTableUpdate takes a single SQLConf to be created. PreprocessTableUpdate is created when DeltaSparkSessionExtension is requested to DeltaSparkSessionExtension.md#apply[register Delta SQL support]. == [[apply]] Executing Rule","title":"PreprocessTableUpdate Logical Resolution Rule"},{"location":"PreprocessTableUpdate/#source-scala","text":"apply( plan: LogicalPlan): LogicalPlan apply resolves ( replaces ) DeltaUpdateTable logical commands (in a logical query plan) into corresponding UpdateCommand.md[]s. apply is part of the Spark SQL's Rule abstraction.","title":"[source, scala]"},{"location":"Protocol/","text":"= Protocol Protocol is...FIXME","title":"Protocol"},{"location":"RemoveFile/","text":"= RemoveFile RemoveFile is...FIXME","title":"RemoveFile"},{"location":"SQLMetricsReporting/","text":"SQLMetricsReporting \u00b6 SQLMetricsReporting is an extension for OptimisticTransactionImpl to track SQL metrics of Operations . Implementations \u00b6 OptimisticTransactionImpl operationSQLMetrics Registry \u00b6 operationSQLMetrics ( spark : SparkSession , metrics : Map [ String , SQLMetric ]) : Unit operationSQLMetrics ...FIXME operationSQLMetrics is used when...FIXME registerSQLMetrics \u00b6 registerSQLMetrics ( spark : SparkSession , metrics : Map [ String , SQLMetric ]) : Unit registerSQLMetrics ...FIXME registerSQLMetrics is used when...FIXME getMetricsForOperation \u00b6 getMetricsForOperation ( operation : Operation ) : Map [ String , String ] getMetricsForOperation ...FIXME getMetricsForOperation is used when OptimisticTransactionImpl is requested to getOperationMetrics .","title":"SQLMetricsReporting"},{"location":"SQLMetricsReporting/#sqlmetricsreporting","text":"SQLMetricsReporting is an extension for OptimisticTransactionImpl to track SQL metrics of Operations .","title":"SQLMetricsReporting"},{"location":"SQLMetricsReporting/#implementations","text":"OptimisticTransactionImpl","title":"Implementations"},{"location":"SQLMetricsReporting/#operationsqlmetrics-registry","text":"operationSQLMetrics ( spark : SparkSession , metrics : Map [ String , SQLMetric ]) : Unit operationSQLMetrics ...FIXME operationSQLMetrics is used when...FIXME","title":" operationSQLMetrics Registry"},{"location":"SQLMetricsReporting/#registersqlmetrics","text":"registerSQLMetrics ( spark : SparkSession , metrics : Map [ String , SQLMetric ]) : Unit registerSQLMetrics ...FIXME registerSQLMetrics is used when...FIXME","title":" registerSQLMetrics"},{"location":"SQLMetricsReporting/#getmetricsforoperation","text":"getMetricsForOperation ( operation : Operation ) : Map [ String , String ] getMetricsForOperation ...FIXME getMetricsForOperation is used when OptimisticTransactionImpl is requested to getOperationMetrics .","title":" getMetricsForOperation"},{"location":"SchemaUtils/","text":"= [[SchemaUtils]] SchemaUtils Utility SchemaUtils is...FIXME == [[mergeSchemas]] mergeSchemas Utility [source, scala] \u00b6 mergeSchemas( tableSchema: StructType, dataSchema: StructType): StructType mergeSchemas ...FIXME [NOTE] \u00b6 mergeSchemas is used when: ConvertToDeltaCommand is requested to ConvertToDeltaCommand.md#performConvert[performConvert] and ConvertToDeltaCommand.md#mergeSchemasInParallel[mergeSchemasInParallel] * ImplicitMetadataOperation is requested to ImplicitMetadataOperation.md#updateMetadata[update metadata] \u00b6","title":"SchemaUtils"},{"location":"SchemaUtils/#source-scala","text":"mergeSchemas( tableSchema: StructType, dataSchema: StructType): StructType mergeSchemas ...FIXME","title":"[source, scala]"},{"location":"SchemaUtils/#note","text":"mergeSchemas is used when: ConvertToDeltaCommand is requested to ConvertToDeltaCommand.md#performConvert[performConvert] and ConvertToDeltaCommand.md#mergeSchemasInParallel[mergeSchemasInParallel]","title":"[NOTE]"},{"location":"SchemaUtils/#implicitmetadataoperation-is-requested-to-implicitmetadataoperationmdupdatemetadataupdate-metadata","text":"","title":"* ImplicitMetadataOperation is requested to ImplicitMetadataOperation.md#updateMetadata[update metadata]"},{"location":"SetTransaction/","text":"= SetTransaction SetTransaction is an < > that denotes the committed < > for an < >. SetTransaction is < > when DeltaSink is requested to < > (for STREAMING UPDATE operation idempotence at query restart). == [[creating-instance]] Creating SetTransaction Instance SetTransaction takes the following to be created: [[appId]] Application ID (e.g. streaming query ID) [[version]] Version (e.g micro-batch ID) [[lastUpdated]] Last Updated (optional) (e.g. milliseconds since the epoch) == [[wrap]] wrap Method [source, scala] \u00b6 wrap: SingleAction \u00b6 NOTE: wrap is part of the < > contract to wrap the action into a < > for serialization. wrap simply creates a new < > with the txn field set to this SetTransaction .","title":"SetTransaction"},{"location":"SetTransaction/#source-scala","text":"","title":"[source, scala]"},{"location":"SetTransaction/#wrap-singleaction","text":"NOTE: wrap is part of the < > contract to wrap the action into a < > for serialization. wrap simply creates a new < > with the txn field set to this SetTransaction .","title":"wrap: SingleAction"},{"location":"SingleAction/","text":"= SingleAction SingleAction is...FIXME","title":"SingleAction"},{"location":"Snapshot/","text":"Snapshot \u00b6 Snapshot is an immutable snapshot of the < > of a < > at < >. Snapshot is < > when DeltaLog is requested for the < > or < >, and to < >. Snapshot can be requested for < >. [source, scala] \u00b6 scala> deltaLog.snapshot.allFiles.show(false) +-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+ |path |partitionValues|size|modificationTime|dataChange|stats|tags| +-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+ |part-00000-4050db39-e0f5-485d-ab3b-3ca72307f621-c000.snappy.parquet|[] |262 |1578083748000 |false |null |null| |part-00000-ba39f292-2970-4528-a40c-8f0aa5f796de-c000.snappy.parquet|[] |262 |1578083570000 |false |null |null| |part-00003-99f9d902-24a7-4f76-a15a-6971940bc245-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00007-03d987f1-5bb3-4b5b-8db9-97b6667107e2-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00011-a759a8c2-507d-46dd-9da7-dc722316214b-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00015-2e685d29-25ed-4262-90a7-5491847fd8d0-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00015-ee0ac1af-e1e0-4422-8245-12da91ced0a2-c000.snappy.parquet|[] |429 |1578083570000 |false |null |null| +-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+ Snapshot can be requested for < > (aka tombstones ). [source, scala] \u00b6 scala> deltaLog.snapshot.tombstones.show(false) +----+-----------------+----------+ |path|deletionTimestamp|dataChange| +----+-----------------+----------+ +----+-----------------+----------+ Snapshot uses the < > internal configuration property (default: 50 ) for the number of partition for < >. == [[creating-instance]] Creating Snapshot Instance Snapshot takes the following to be created: [[path]] Hadoop https://hadoop.apache.org/docs/r2.6.5/api/org/apache/hadoop/fs/Path.html[Path ] to the < > [[version]] Version [[previousSnapshot]] Previous snapshot ( Option[Dataset[SingleAction]] ) [[files]] Files ( Seq[Path] ) [[minFileRetentionTimestamp]] minFileRetentionTimestamp (that is exactly < >) [[deltaLog]] < > [[timestamp]] Timestamp [[lineageLength]] Length of the lineage (default: 1 ) Snapshot initializes the < >. While being created, Snapshot requests the < > to < > with the < >. == [[state]] state Method [source, scala] \u00b6 state: Dataset[SingleAction] \u00b6 state simply requests the < > to < >. [NOTE] \u00b6 state is used when: DeltaLog is requested to < > (and in turn < >) Checkpoints is requested to < > (and in turn < >) Snapshot is created, and requested for < > and < > * VacuumCommand is requested for < > \u00b6 == [[allFiles]] All AddFiles -- allFiles Method [source, scala] \u00b6 allFiles: Dataset[AddFile] \u00b6 allFiles simply takes the < > and selects AddFile.md[AddFiles] (adds where clause for add IS NOT NULL and select over the fields of AddFile.md[AddFiles]). NOTE: allFiles simply adds where and select clauses. No computation as it is (a description of) a distributed computation as a Dataset[AddFile] . [source, scala] \u00b6 import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, \"/tmp/delta/users\") val files = deltaLog.snapshot.allFiles scala> :type files org.apache.spark.sql.Dataset[org.apache.spark.sql.delta.actions.AddFile] scala> files.show +--------------------+---------------+----+----------------+----------+-----+----+ | path|partitionValues|size|modificationTime|dataChange|stats|tags| +--------------------+---------------+----+----------------+----------+-----+----+ |part-00000-68a7ce...| []| 875| 1579789902000| false| null|null| |part-00000-73e140...| []| 419| 1579723382000| false| null|null| |part-00000-7a01b0...| []| 875| 1579877119000| false| null|null| |part-00001-8a2ece...| []| 875| 1579789902000| false| null|null| |part-00002-0fc3da...| []| 866| 1579789902000| false| null|null| |part-00003-c0fc5f...| []| 884| 1579789902000| false| null|null| +--------------------+---------------+----+----------------+----------+-----+----+ [NOTE] \u00b6 allFiles is used when: PartitionFiltering is requested for the < > DeltaSourceSnapshot is requested for the < > (indexed AddFile.md[AddFiles]) GenerateSymlinkManifestImpl is requested to < > and < > * DeltaDataSource is requested for an < > \u00b6 == [[stateReconstruction]] stateReconstruction Internal Property [source, scala] \u00b6 stateReconstruction: Dataset[SingleAction] \u00b6 stateReconstruction is a dataset of < > (that is the < > part) of the < >. == [[emptyActions]] emptyActions Internal Method [source, scala] \u00b6 emptyActions: Dataset[SingleAction] \u00b6 emptyActions is an empty dataset of < > for < > and < >. == [[load]] load Internal Method [source, scala] \u00b6 load( files: Seq[DeltaLogFileIndex]): Dataset[SingleAction] load ...FIXME NOTE: load is used when Snapshot is created (and initializes < >). == [[transactions]] Transaction Version By App ID -- transactions Lookup Table [source, scala] \u00b6 transactions: Map[String, Long] \u00b6 transactions takes the < > actions (from the < > dataset) and makes them a lookup table of < > by < >. NOTE: transactions is a Scala lazy value and is not initialized until the first access. NOTE: transactions is used when OptimisticTransactionImpl is requested for the < >. == [[tombstones]] tombstones Method [source, scala] \u00b6 tombstones: Dataset[RemoveFile] \u00b6 tombstones ...FIXME NOTE: tombstones seems to be used for testing only. == [[redactedPath]] redactedPath Method [source, scala] \u00b6 redactedPath: String \u00b6 redactedPath ...FIXME NOTE: redactedPath is used...FIXME == [[numIndexedCols]] dataSkippingNumIndexedCols Table Property -- numIndexedCols Value [source, scala] \u00b6 numIndexedCols: Int \u00b6 numIndexedCols simply reads the < > table property < > the < >. NOTE: numIndexedCols seems unused. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | cachedState a| [[cachedState]] < > that is made up of the following: The < > part is the < > dataset of < > The < > in the format Delta Table State #version - [redactedPath] (with the < > and the < >) Used when Snapshot is requested for the < > (i.e. Dataset[SingleAction] ) | metadata a| [[metadata]] < > of the current < > of the < > | protocol a| [[protocol]] < > of the current < > of the < > | setTransactions a| [[setTransactions]] < > of the current < > of the < > |===","title":"Snapshot"},{"location":"Snapshot/#snapshot","text":"Snapshot is an immutable snapshot of the < > of a < > at < >. Snapshot is < > when DeltaLog is requested for the < > or < >, and to < >. Snapshot can be requested for < >.","title":"Snapshot"},{"location":"Snapshot/#source-scala","text":"scala> deltaLog.snapshot.allFiles.show(false) +-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+ |path |partitionValues|size|modificationTime|dataChange|stats|tags| +-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+ |part-00000-4050db39-e0f5-485d-ab3b-3ca72307f621-c000.snappy.parquet|[] |262 |1578083748000 |false |null |null| |part-00000-ba39f292-2970-4528-a40c-8f0aa5f796de-c000.snappy.parquet|[] |262 |1578083570000 |false |null |null| |part-00003-99f9d902-24a7-4f76-a15a-6971940bc245-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00007-03d987f1-5bb3-4b5b-8db9-97b6667107e2-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00011-a759a8c2-507d-46dd-9da7-dc722316214b-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00015-2e685d29-25ed-4262-90a7-5491847fd8d0-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00015-ee0ac1af-e1e0-4422-8245-12da91ced0a2-c000.snappy.parquet|[] |429 |1578083570000 |false |null |null| +-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+ Snapshot can be requested for < > (aka tombstones ).","title":"[source, scala]"},{"location":"Snapshot/#source-scala_1","text":"scala> deltaLog.snapshot.tombstones.show(false) +----+-----------------+----------+ |path|deletionTimestamp|dataChange| +----+-----------------+----------+ +----+-----------------+----------+ Snapshot uses the < > internal configuration property (default: 50 ) for the number of partition for < >. == [[creating-instance]] Creating Snapshot Instance Snapshot takes the following to be created: [[path]] Hadoop https://hadoop.apache.org/docs/r2.6.5/api/org/apache/hadoop/fs/Path.html[Path ] to the < > [[version]] Version [[previousSnapshot]] Previous snapshot ( Option[Dataset[SingleAction]] ) [[files]] Files ( Seq[Path] ) [[minFileRetentionTimestamp]] minFileRetentionTimestamp (that is exactly < >) [[deltaLog]] < > [[timestamp]] Timestamp [[lineageLength]] Length of the lineage (default: 1 ) Snapshot initializes the < >. While being created, Snapshot requests the < > to < > with the < >. == [[state]] state Method","title":"[source, scala]"},{"location":"Snapshot/#source-scala_2","text":"","title":"[source, scala]"},{"location":"Snapshot/#state-datasetsingleaction","text":"state simply requests the < > to < >.","title":"state: Dataset[SingleAction]"},{"location":"Snapshot/#note","text":"state is used when: DeltaLog is requested to < > (and in turn < >) Checkpoints is requested to < > (and in turn < >) Snapshot is created, and requested for < > and < >","title":"[NOTE]"},{"location":"Snapshot/#vacuumcommand-is-requested-for","text":"== [[allFiles]] All AddFiles -- allFiles Method","title":"* VacuumCommand is requested for &lt;&gt;"},{"location":"Snapshot/#source-scala_3","text":"","title":"[source, scala]"},{"location":"Snapshot/#allfiles-datasetaddfile","text":"allFiles simply takes the < > and selects AddFile.md[AddFiles] (adds where clause for add IS NOT NULL and select over the fields of AddFile.md[AddFiles]). NOTE: allFiles simply adds where and select clauses. No computation as it is (a description of) a distributed computation as a Dataset[AddFile] .","title":"allFiles: Dataset[AddFile]"},{"location":"Snapshot/#source-scala_4","text":"import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, \"/tmp/delta/users\") val files = deltaLog.snapshot.allFiles scala> :type files org.apache.spark.sql.Dataset[org.apache.spark.sql.delta.actions.AddFile] scala> files.show +--------------------+---------------+----+----------------+----------+-----+----+ | path|partitionValues|size|modificationTime|dataChange|stats|tags| +--------------------+---------------+----+----------------+----------+-----+----+ |part-00000-68a7ce...| []| 875| 1579789902000| false| null|null| |part-00000-73e140...| []| 419| 1579723382000| false| null|null| |part-00000-7a01b0...| []| 875| 1579877119000| false| null|null| |part-00001-8a2ece...| []| 875| 1579789902000| false| null|null| |part-00002-0fc3da...| []| 866| 1579789902000| false| null|null| |part-00003-c0fc5f...| []| 884| 1579789902000| false| null|null| +--------------------+---------------+----+----------------+----------+-----+----+","title":"[source, scala]"},{"location":"Snapshot/#note_1","text":"allFiles is used when: PartitionFiltering is requested for the < > DeltaSourceSnapshot is requested for the < > (indexed AddFile.md[AddFiles]) GenerateSymlinkManifestImpl is requested to < > and < >","title":"[NOTE]"},{"location":"Snapshot/#deltadatasource-is-requested-for-an","text":"== [[stateReconstruction]] stateReconstruction Internal Property","title":"* DeltaDataSource is requested for an &lt;&gt;"},{"location":"Snapshot/#source-scala_5","text":"","title":"[source, scala]"},{"location":"Snapshot/#statereconstruction-datasetsingleaction","text":"stateReconstruction is a dataset of < > (that is the < > part) of the < >. == [[emptyActions]] emptyActions Internal Method","title":"stateReconstruction: Dataset[SingleAction]"},{"location":"Snapshot/#source-scala_6","text":"","title":"[source, scala]"},{"location":"Snapshot/#emptyactions-datasetsingleaction","text":"emptyActions is an empty dataset of < > for < > and < >. == [[load]] load Internal Method","title":"emptyActions: Dataset[SingleAction]"},{"location":"Snapshot/#source-scala_7","text":"load( files: Seq[DeltaLogFileIndex]): Dataset[SingleAction] load ...FIXME NOTE: load is used when Snapshot is created (and initializes < >). == [[transactions]] Transaction Version By App ID -- transactions Lookup Table","title":"[source, scala]"},{"location":"Snapshot/#source-scala_8","text":"","title":"[source, scala]"},{"location":"Snapshot/#transactions-mapstring-long","text":"transactions takes the < > actions (from the < > dataset) and makes them a lookup table of < > by < >. NOTE: transactions is a Scala lazy value and is not initialized until the first access. NOTE: transactions is used when OptimisticTransactionImpl is requested for the < >. == [[tombstones]] tombstones Method","title":"transactions: Map[String, Long]"},{"location":"Snapshot/#source-scala_9","text":"","title":"[source, scala]"},{"location":"Snapshot/#tombstones-datasetremovefile","text":"tombstones ...FIXME NOTE: tombstones seems to be used for testing only. == [[redactedPath]] redactedPath Method","title":"tombstones: Dataset[RemoveFile]"},{"location":"Snapshot/#source-scala_10","text":"","title":"[source, scala]"},{"location":"Snapshot/#redactedpath-string","text":"redactedPath ...FIXME NOTE: redactedPath is used...FIXME == [[numIndexedCols]] dataSkippingNumIndexedCols Table Property -- numIndexedCols Value","title":"redactedPath: String"},{"location":"Snapshot/#source-scala_11","text":"","title":"[source, scala]"},{"location":"Snapshot/#numindexedcols-int","text":"numIndexedCols simply reads the < > table property < > the < >. NOTE: numIndexedCols seems unused. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | cachedState a| [[cachedState]] < > that is made up of the following: The < > part is the < > dataset of < > The < > in the format Delta Table State #version - [redactedPath] (with the < > and the < >) Used when Snapshot is requested for the < > (i.e. Dataset[SingleAction] ) | metadata a| [[metadata]] < > of the current < > of the < > | protocol a| [[protocol]] < > of the current < > of the < > | setTransactions a| [[setTransactions]] < > of the current < > of the < > |===","title":"numIndexedCols: Int"},{"location":"SnapshotIterator/","text":"= SnapshotIterator SnapshotIterator is...FIXME == [[iterator]] iterator Method [source, scala] \u00b6 iterator(): Iterator[IndexedFile] \u00b6 iterator ...FIXME NOTE: iterator is used exclusively when DeltaSource is requested for the < >.","title":"SnapshotIterator"},{"location":"SnapshotIterator/#source-scala","text":"","title":"[source, scala]"},{"location":"SnapshotIterator/#iterator-iteratorindexedfile","text":"iterator ...FIXME NOTE: iterator is used exclusively when DeltaSource is requested for the < >.","title":"iterator(): Iterator[IndexedFile]"},{"location":"SnapshotManagement/","text":"SnapshotManagement \u00b6 SnapshotManagement is...FIXME","title":"SnapshotManagement"},{"location":"SnapshotManagement/#snapshotmanagement","text":"SnapshotManagement is...FIXME","title":"SnapshotManagement"},{"location":"StagedDeltaTableV2/","text":"= StagedDeltaTableV2 StagedDeltaTableV2 is a StagedTable and a SupportsWrite (both from Spark SQL 3.0.0). == [[creating-instance]] Creating Instance StagedDeltaTableV2 takes the following to be created: [[ident]] Identifier [[schema]] Schema [[partitions]] Partitions ( Array[Transform] ) [[properties]] Properties [[operation]] Operation (one of Create, CreateOrReplace, Replace) StagedDeltaTableV2 is created when DeltaCatalog is requested to DeltaCatalog.md#stageReplace[stageReplace], DeltaCatalog.md#stageCreateOrReplace[stageCreateOrReplace] or DeltaCatalog.md#stageCreate[stageCreate]. == [[commitStagedChanges]] commitStagedChanges [source,scala] \u00b6 commitStagedChanges(): Unit \u00b6 commitStagedChanges...FIXME commitStagedChanges is part of the StagedTable (Spark SQL 3.0.0) abstraction. == [[abortStagedChanges]] abortStagedChanges [source,scala] \u00b6 abortStagedChanges(): Unit \u00b6 abortStagedChanges does nothing. abortStagedChanges is part of the StagedTable (Spark SQL 3.0.0) abstraction. == [[newWriteBuilder]] newWriteBuilder [source,scala] \u00b6 newWriteBuilder( info: LogicalWriteInfo): V1WriteBuilder newWriteBuilder...FIXME newWriteBuilder is part of the SupportsWrite (Spark SQL 3.0.0) abstraction.","title":"StagedDeltaTableV2"},{"location":"StagedDeltaTableV2/#sourcescala","text":"","title":"[source,scala]"},{"location":"StagedDeltaTableV2/#commitstagedchanges-unit","text":"commitStagedChanges...FIXME commitStagedChanges is part of the StagedTable (Spark SQL 3.0.0) abstraction. == [[abortStagedChanges]] abortStagedChanges","title":"commitStagedChanges(): Unit"},{"location":"StagedDeltaTableV2/#sourcescala_1","text":"","title":"[source,scala]"},{"location":"StagedDeltaTableV2/#abortstagedchanges-unit","text":"abortStagedChanges does nothing. abortStagedChanges is part of the StagedTable (Spark SQL 3.0.0) abstraction. == [[newWriteBuilder]] newWriteBuilder","title":"abortStagedChanges(): Unit"},{"location":"StagedDeltaTableV2/#sourcescala_2","text":"newWriteBuilder( info: LogicalWriteInfo): V1WriteBuilder newWriteBuilder...FIXME newWriteBuilder is part of the SupportsWrite (Spark SQL 3.0.0) abstraction.","title":"[source,scala]"},{"location":"StateCache/","text":"= StateCache StateCache is...FIXME == [[cacheDS]] Creating CachedDS Instance -- cacheDS Method [source, scala] \u00b6 cacheDS A : CachedDS[A] cacheDS simply creates a new < >. [NOTE] \u00b6 cacheDS is used when: < > is created (and creates a < >) * DeltaSourceSnapshot is requested to < > \u00b6","title":"StateCache"},{"location":"StateCache/#source-scala","text":"cacheDS A : CachedDS[A] cacheDS simply creates a new < >.","title":"[source, scala]"},{"location":"StateCache/#note","text":"cacheDS is used when: < > is created (and creates a < >)","title":"[NOTE]"},{"location":"StateCache/#deltasourcesnapshot-is-requested-to","text":"","title":"* DeltaSourceSnapshot is requested to &lt;&gt;"},{"location":"TahoeBatchFileIndex/","text":"= [[TahoeBatchFileIndex]] TahoeBatchFileIndex TahoeBatchFileIndex is a concrete < > for a given < > of a < >. TahoeBatchFileIndex is < > when: DeltaLog is requested for a < > (for < > and < >) < > and < > are executed DeltaCommand is requested for a < > (for < > and < >) == [[creating-instance]] Creating TahoeBatchFileIndex Instance TahoeBatchFileIndex takes the following to be created: [[spark]] SparkSession [[actionType]] Action type [[addFiles]] < > ( Seq[AddFile] ) [[deltaLog]] < > [[path]] Data directory of the delta table (as Hadoop https://hadoop.apache.org/docs/r2.6.5/api/org/apache/hadoop/fs/Path.html[Path ]) [[snapshot]] < > TahoeBatchFileIndex initializes the < >. == [[tableVersion]] tableVersion Method [source, scala] \u00b6 tableVersion: Long \u00b6 NOTE: tableVersion is part of the < > contract for the version of the delta table. tableVersion ...FIXME == [[matchingFiles]] matchingFiles Method [source, scala] \u00b6 matchingFiles( partitionFilters: Seq[Expression], dataFilters: Seq[Expression], keepStats: Boolean = false): Seq[AddFile] NOTE: matchingFiles is part of the < > for the matching (valid) files by the given filtering expressions. matchingFiles ...FIXME == [[inputFiles]] inputFiles Method [source, scala] \u00b6 inputFiles: Array[String] \u00b6 NOTE: inputFiles is part of the FileIndex contract to...FIXME inputFiles ...FIXME == [[partitionSchema]] Schema of Partition Columns -- partitionSchema Method [source, scala] \u00b6 partitionSchema: StructType \u00b6 NOTE: partitionSchema is part of the FileIndex contract (Spark SQL) to get the schema of the partition columns (if used). partitionSchema simply requests the < > for the < > that is in turn requested for the < >. == [[sizeInBytes]] sizeInBytes Property [source, scala] \u00b6 sizeInBytes: Long \u00b6 NOTE: sizeInBytes is part of the FileIndex contract (Spark SQL) for the table size (in bytes). sizeInBytes is simply a sum of the < > of all < >.","title":"TahoeBatchFileIndex"},{"location":"TahoeBatchFileIndex/#source-scala","text":"","title":"[source, scala]"},{"location":"TahoeBatchFileIndex/#tableversion-long","text":"NOTE: tableVersion is part of the < > contract for the version of the delta table. tableVersion ...FIXME == [[matchingFiles]] matchingFiles Method","title":"tableVersion: Long"},{"location":"TahoeBatchFileIndex/#source-scala_1","text":"matchingFiles( partitionFilters: Seq[Expression], dataFilters: Seq[Expression], keepStats: Boolean = false): Seq[AddFile] NOTE: matchingFiles is part of the < > for the matching (valid) files by the given filtering expressions. matchingFiles ...FIXME == [[inputFiles]] inputFiles Method","title":"[source, scala]"},{"location":"TahoeBatchFileIndex/#source-scala_2","text":"","title":"[source, scala]"},{"location":"TahoeBatchFileIndex/#inputfiles-arraystring","text":"NOTE: inputFiles is part of the FileIndex contract to...FIXME inputFiles ...FIXME == [[partitionSchema]] Schema of Partition Columns -- partitionSchema Method","title":"inputFiles: Array[String]"},{"location":"TahoeBatchFileIndex/#source-scala_3","text":"","title":"[source, scala]"},{"location":"TahoeBatchFileIndex/#partitionschema-structtype","text":"NOTE: partitionSchema is part of the FileIndex contract (Spark SQL) to get the schema of the partition columns (if used). partitionSchema simply requests the < > for the < > that is in turn requested for the < >. == [[sizeInBytes]] sizeInBytes Property","title":"partitionSchema: StructType"},{"location":"TahoeBatchFileIndex/#source-scala_4","text":"","title":"[source, scala]"},{"location":"TahoeBatchFileIndex/#sizeinbytes-long","text":"NOTE: sizeInBytes is part of the FileIndex contract (Spark SQL) for the table size (in bytes). sizeInBytes is simply a sum of the < > of all < >.","title":"sizeInBytes: Long"},{"location":"TahoeFileIndex/","text":"= TahoeFileIndex -- Indices Of Files Of Delta Table :navtitle: TahoeFileIndex TahoeFileIndex is an < > of the Spark SQL FileIndex contract for < > of delta tables that can < > to scan (based on < >). TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-FileIndex.html[FileIndex ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. NOTE: The aim of TahoeFileIndex is to reduce usage of very expensive disk access for file-related information using Hadoop https://hadoop.apache.org/docs/r2.6.5/api/org/apache/hadoop/fs/FileSystem.html[FileSystem ] API. [[contract]] .TahoeFileIndex Contract (Abstract Methods Only) [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | matchingFiles a| [[matchingFiles]] [source, scala] \u00b6 matchingFiles( partitionFilters: Seq[Expression], dataFilters: Seq[Expression], keepStats: Boolean = false): Seq[AddFile] Files (AddFile.md[AddFiles]) matching given partition and data predicates Used for < > |=== [[rootPaths]] When requested for the root input paths ( rootPaths ), TahoeFileIndex simply gives the < >. [[implementations]] .TahoeFileIndices [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | TahoeFileIndex | Description | < > | [[TahoeBatchFileIndex]] | < > | [[TahoeLogFileIndex]] |=== == [[creating-instance]] Creating TahoeFileIndex Instance TahoeFileIndex takes the following to be created: [[spark]] SparkSession [[deltaLog]] < > [[path]] Hadoop https://hadoop.apache.org/docs/r2.6.5/api/org/apache/hadoop/fs/Path.html[Path ] NOTE: TahoeFileIndex is a Scala abstract class and cannot be < > directly. It is created indirectly for the < >. == [[tableVersion]] Version of Delta Table -- tableVersion Method [source, scala] \u00b6 tableVersion: Long \u00b6 tableVersion is simply the < > of (the < > of) the < >. NOTE: tableVersion is used when TahoeFileIndex is requested for the < >. == [[listFiles]] Listing Data Files -- listFiles Method [source, scala] \u00b6 listFiles( partitionFilters: Seq[Expression], dataFilters: Seq[Expression]): Seq[PartitionDirectory] NOTE: listFiles is part of the FileIndex contract for the file names (grouped into partitions when the data is partitioned). listFiles ...FIXME == [[partitionSchema]] Partition Schema -- partitionSchema Method [source, scala] \u00b6 partitionSchema: StructType \u00b6 NOTE: partitionSchema is part of the FileIndex contract for the partition schema. partitionSchema simply requests the < > for the < > and then requests the Snapshot for < > that in turn is requested for the < >. == [[toString]] Human-Friendly Textual Representation -- toString Method [source, scala] \u00b6 toString: String \u00b6 NOTE: toString is part of the java.lang.Object contract for a string representation of the object. toString returns the following text (based on the < > and the < > truncated to 100 characters): Delta[version=[tableVersion], [truncatedPath]]","title":"TahoeFileIndex"},{"location":"TahoeFileIndex/#source-scala","text":"matchingFiles( partitionFilters: Seq[Expression], dataFilters: Seq[Expression], keepStats: Boolean = false): Seq[AddFile] Files (AddFile.md[AddFiles]) matching given partition and data predicates Used for < > |=== [[rootPaths]] When requested for the root input paths ( rootPaths ), TahoeFileIndex simply gives the < >. [[implementations]] .TahoeFileIndices [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | TahoeFileIndex | Description | < > | [[TahoeBatchFileIndex]] | < > | [[TahoeLogFileIndex]] |=== == [[creating-instance]] Creating TahoeFileIndex Instance TahoeFileIndex takes the following to be created: [[spark]] SparkSession [[deltaLog]] < > [[path]] Hadoop https://hadoop.apache.org/docs/r2.6.5/api/org/apache/hadoop/fs/Path.html[Path ] NOTE: TahoeFileIndex is a Scala abstract class and cannot be < > directly. It is created indirectly for the < >. == [[tableVersion]] Version of Delta Table -- tableVersion Method","title":"[source, scala]"},{"location":"TahoeFileIndex/#source-scala_1","text":"","title":"[source, scala]"},{"location":"TahoeFileIndex/#tableversion-long","text":"tableVersion is simply the < > of (the < > of) the < >. NOTE: tableVersion is used when TahoeFileIndex is requested for the < >. == [[listFiles]] Listing Data Files -- listFiles Method","title":"tableVersion: Long"},{"location":"TahoeFileIndex/#source-scala_2","text":"listFiles( partitionFilters: Seq[Expression], dataFilters: Seq[Expression]): Seq[PartitionDirectory] NOTE: listFiles is part of the FileIndex contract for the file names (grouped into partitions when the data is partitioned). listFiles ...FIXME == [[partitionSchema]] Partition Schema -- partitionSchema Method","title":"[source, scala]"},{"location":"TahoeFileIndex/#source-scala_3","text":"","title":"[source, scala]"},{"location":"TahoeFileIndex/#partitionschema-structtype","text":"NOTE: partitionSchema is part of the FileIndex contract for the partition schema. partitionSchema simply requests the < > for the < > and then requests the Snapshot for < > that in turn is requested for the < >. == [[toString]] Human-Friendly Textual Representation -- toString Method","title":"partitionSchema: StructType"},{"location":"TahoeFileIndex/#source-scala_4","text":"","title":"[source, scala]"},{"location":"TahoeFileIndex/#tostring-string","text":"NOTE: toString is part of the java.lang.Object contract for a string representation of the object. toString returns the following text (based on the < > and the < > truncated to 100 characters): Delta[version=[tableVersion], [truncatedPath]]","title":"toString: String"},{"location":"TahoeLogFileIndex/","text":"= [[TahoeLogFileIndex]] TahoeLogFileIndex TahoeLogFileIndex is a concrete < >. TahoeLogFileIndex is < > when DeltaLog is requested for a < > (when DeltaDataSource is requested for one as a < > and a < >). val q = spark.read.format(\"delta\").load(\"/tmp/delta/users\") val plan = q.queryExecution.executedPlan import org.apache.spark.sql.execution.FileSourceScanExec val scan = plan.collect { case e: FileSourceScanExec => e }.head import org.apache.spark.sql.delta.files.TahoeLogFileIndex val index = scan.relation.location.asInstanceOf[TahoeLogFileIndex] scala> println(index) Delta[version=1, file:/tmp/delta/users] == [[creating-instance]] Creating TahoeLogFileIndex Instance TahoeLogFileIndex takes the following to be created: [[spark]] SparkSession [[deltaLog]] < > [[dataPath]] Data directory of the delta table (as Hadoop https://hadoop.apache.org/docs/r2.6.5/api/org/apache/hadoop/fs/Path.html[Path ]) [[partitionFilters]] Partition filters (default: empty ) (as Catalyst expressions, i.e. Seq[Expression] ) [[versionToUse]] Snapshot version (default: undefined ) ( Option[Long] ) TahoeLogFileIndex initializes the < >. == [[partitionSchema]] Schema of Partition Columns -- partitionSchema Method [source, scala] \u00b6 partitionSchema: StructType \u00b6 NOTE: partitionSchema is part of the FileIndex contract (Spark SQL) to get the schema of the partition columns (if used). partitionSchema ...FIXME == [[matchingFiles]] matchingFiles Method [source, scala] \u00b6 matchingFiles( partitionFilters: Seq[Expression], dataFilters: Seq[Expression], keepStats: Boolean = false): Seq[AddFile] NOTE: matchingFiles is part of the < > for the AddFile.md[files] matching given predicates. matchingFiles < > (with stalenessAcceptable flag off) and requests it for the < > (for the index's < >, the given partitionFilters and dataFilters ). NOTE: < > and < > are similar. Both < > (of the delta table), but they use different filtering expressions and return value types. == [[inputFiles]] inputFiles Method [source, scala] \u00b6 inputFiles: Array[String] \u00b6 NOTE: inputFiles is part of the FileIndex contract to...FIXME inputFiles < > (with stalenessAcceptable flag off) and requests it for the < > (for the index's < >). NOTE: < > and < > are similar. Both < > (of the delta table), but they use different filtering expressions and return value types. == [[getSnapshot]] Historical Or Latest Snapshot -- getSnapshot Method [source, scala] \u00b6 getSnapshot( stalenessAcceptable: Boolean): Snapshot getSnapshot returns a < > that is either the < > (for the < > if defined) or requests the < > to < > (and give one). NOTE: getSnapshot is used when TahoeLogFileIndex is requested for the < > and the < >. == [[sizeInBytes]] sizeInBytes Property [source, scala] \u00b6 sizeInBytes: Long \u00b6 NOTE: sizeInBytes is part of the FileIndex contract for the table size (in bytes). sizeInBytes ...FIXME == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | historicalSnapshotOpt a| [[historicalSnapshotOpt]] Historical snapshot , i.e. < > for the < > (if defined) Used when TahoeLogFileIndex is requested for the < > and the < > |===","title":"TahoeLogFileIndex"},{"location":"TahoeLogFileIndex/#source-scala","text":"","title":"[source, scala]"},{"location":"TahoeLogFileIndex/#partitionschema-structtype","text":"NOTE: partitionSchema is part of the FileIndex contract (Spark SQL) to get the schema of the partition columns (if used). partitionSchema ...FIXME == [[matchingFiles]] matchingFiles Method","title":"partitionSchema: StructType"},{"location":"TahoeLogFileIndex/#source-scala_1","text":"matchingFiles( partitionFilters: Seq[Expression], dataFilters: Seq[Expression], keepStats: Boolean = false): Seq[AddFile] NOTE: matchingFiles is part of the < > for the AddFile.md[files] matching given predicates. matchingFiles < > (with stalenessAcceptable flag off) and requests it for the < > (for the index's < >, the given partitionFilters and dataFilters ). NOTE: < > and < > are similar. Both < > (of the delta table), but they use different filtering expressions and return value types. == [[inputFiles]] inputFiles Method","title":"[source, scala]"},{"location":"TahoeLogFileIndex/#source-scala_2","text":"","title":"[source, scala]"},{"location":"TahoeLogFileIndex/#inputfiles-arraystring","text":"NOTE: inputFiles is part of the FileIndex contract to...FIXME inputFiles < > (with stalenessAcceptable flag off) and requests it for the < > (for the index's < >). NOTE: < > and < > are similar. Both < > (of the delta table), but they use different filtering expressions and return value types. == [[getSnapshot]] Historical Or Latest Snapshot -- getSnapshot Method","title":"inputFiles: Array[String]"},{"location":"TahoeLogFileIndex/#source-scala_3","text":"getSnapshot( stalenessAcceptable: Boolean): Snapshot getSnapshot returns a < > that is either the < > (for the < > if defined) or requests the < > to < > (and give one). NOTE: getSnapshot is used when TahoeLogFileIndex is requested for the < > and the < >. == [[sizeInBytes]] sizeInBytes Property","title":"[source, scala]"},{"location":"TahoeLogFileIndex/#source-scala_4","text":"","title":"[source, scala]"},{"location":"TahoeLogFileIndex/#sizeinbytes-long","text":"NOTE: sizeInBytes is part of the FileIndex contract for the table size (in bytes). sizeInBytes ...FIXME == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | historicalSnapshotOpt a| [[historicalSnapshotOpt]] Historical snapshot , i.e. < > for the < > (if defined) Used when TahoeLogFileIndex is requested for the < > and the < > |===","title":"sizeInBytes: Long"},{"location":"TransactionalWrite/","text":"TransactionalWrite \u00b6 TransactionalWrite is an < > of < > that can < > to a < >. == [[contract]] Contract === [[deltaLog]] deltaLog [source,scala] \u00b6 deltaLog: DeltaLog \u00b6 DeltaLog.md[] (of a delta table) that this transaction is changing Used when: OptimisticTransactionImpl is requested to < >, < > (after < >), < >, and < > (and execute < >) < > and < > are executed DeltaCommand is requested to < > DeltaLog is requested to < > TransactionalWrite is requested to < > === [[metadata]] metadata [source, scala] \u00b6 metadata: Metadata \u00b6 Metadata.md[] (of the < >) that this transaction is changing === [[protocol]] protocol [source, scala] \u00b6 protocol: Protocol \u00b6 Protocol.md[] (of the < >) that this transaction is changing Used when AlterTableSetPropertiesDeltaCommand.md[] is executed (to DeltaConfigs.md#verifyProtocolVersionRequirements[verifyProtocolVersionRequirements]) === [[snapshot]] snapshot [source, scala] \u00b6 snapshot: Snapshot \u00b6 Snapshot.md[] (of the < >) that this transaction is < > == [[implementations]][[self]] Implementations OptimisticTransaction.md[] is the default and only known TransactionalWrite in Delta Lake (indirectly as a OptimisticTransactionImpl.md[]). Writing Data Out (Result Of Structured Query) \u00b6 writeFiles ( data : Dataset [ _ ]) : Seq [ AddFile ] writeFiles ( data : Dataset [ _ ], writeOptions : Option [ DeltaOptions ]) : Seq [ AddFile ] writeFiles ( data : Dataset [ _ ], isOptimize : Boolean ) : Seq [ AddFile ] writeFiles ( data : Dataset [ _ ], writeOptions : Option [ DeltaOptions ], isOptimize : Boolean ) : Seq [ AddFile ] writeFiles creates a DeltaInvariantCheckerExec and a DelayedCommitProtocol to write out files to the data path (of the DeltaLog ). Note writeFiles uses Spark SQL's FileFormatWriter utility to write out a result of a streaming query. Read up on FileFormatWriter in The Internals of Spark SQL online book. writeFiles is executed within SQLExecution.withNewExecutionId . Note writeFiles can be tracked using web UI or SQLAppStatusListener (using SparkListenerSQLExecutionStart and SparkListenerSQLExecutionEnd events). In the end, writeFiles returns the addedStatuses of the DelayedCommitProtocol committer. Internally, writeFiles turns the hasWritten flag on ( true ). NOTE: After writeFiles , no metadata updates in the transaction are permitted. writeFiles normalize the given data dataset (based on the partitionColumns of the Metadata ). writeFiles getPartitioningColumns based on the partitionSchema of the Metadata . writeFiles creates a DelayedCommitProtocol committer for the data path of the DeltaLog . writeFiles gets the invariants from the schema of the Metadata . writeFiles requests a new Execution ID (that is used to track all Spark jobs of FileFormatWriter.write in Spark SQL) with a physical query plan of a new DeltaInvariantCheckerExec unary physical operator (with the executed plan of the normalized query execution as the child operator). writeFiles is used when: DeleteCommand , MergeIntoCommand , UpdateCommand , and WriteIntoDelta commands are executed DeltaSink is requested to add a streaming micro-batch == [[getCommitter]] Creating Committer [source, scala] \u00b6 getCommitter( outputPath: Path): DelayedCommitProtocol getCommitter creates a new < > with the delta job ID and the given outputPath (and no random prefix). getCommitter is used when TransactionalWrite is requested to < >. == [[makeOutputNullable]] makeOutputNullable Method [source, scala] \u00b6 makeOutputNullable( output: Seq[Attribute]): Seq[Attribute] makeOutputNullable...FIXME makeOutputNullable is used when...FIXME == [[normalizeData]] normalizeData Method [source, scala] \u00b6 normalizeData( data: Dataset[_], partitionCols: Seq[String]): (QueryExecution, Seq[Attribute]) normalizeData...FIXME normalizeData is used when...FIXME == [[getPartitioningColumns]] getPartitioningColumns Method [source, scala] \u00b6 getPartitioningColumns( partitionSchema: StructType, output: Seq[Attribute], colsDropped: Boolean): Seq[Attribute] getPartitioningColumns...FIXME getPartitioningColumns is used when...FIXME == [[hasWritten]] hasWritten Flag [source, scala] \u00b6 hasWritten: Boolean = false \u00b6 TransactionalWrite uses the hasWritten internal registry to prevent OptimisticTransactionImpl from < > after < >. hasWritten is initially turned off ( false ). It can be turned on ( true ) when TransactionalWrite is requested to < >.","title":"TransactionalWrite"},{"location":"TransactionalWrite/#transactionalwrite","text":"TransactionalWrite is an < > of < > that can < > to a < >. == [[contract]] Contract === [[deltaLog]] deltaLog","title":"TransactionalWrite"},{"location":"TransactionalWrite/#sourcescala","text":"","title":"[source,scala]"},{"location":"TransactionalWrite/#deltalog-deltalog","text":"DeltaLog.md[] (of a delta table) that this transaction is changing Used when: OptimisticTransactionImpl is requested to < >, < > (after < >), < >, and < > (and execute < >) < > and < > are executed DeltaCommand is requested to < > DeltaLog is requested to < > TransactionalWrite is requested to < > === [[metadata]] metadata","title":"deltaLog: DeltaLog"},{"location":"TransactionalWrite/#source-scala","text":"","title":"[source, scala]"},{"location":"TransactionalWrite/#metadata-metadata","text":"Metadata.md[] (of the < >) that this transaction is changing === [[protocol]] protocol","title":"metadata: Metadata"},{"location":"TransactionalWrite/#source-scala_1","text":"","title":"[source, scala]"},{"location":"TransactionalWrite/#protocol-protocol","text":"Protocol.md[] (of the < >) that this transaction is changing Used when AlterTableSetPropertiesDeltaCommand.md[] is executed (to DeltaConfigs.md#verifyProtocolVersionRequirements[verifyProtocolVersionRequirements]) === [[snapshot]] snapshot","title":"protocol: Protocol"},{"location":"TransactionalWrite/#source-scala_2","text":"","title":"[source, scala]"},{"location":"TransactionalWrite/#snapshot-snapshot","text":"Snapshot.md[] (of the < >) that this transaction is < > == [[implementations]][[self]] Implementations OptimisticTransaction.md[] is the default and only known TransactionalWrite in Delta Lake (indirectly as a OptimisticTransactionImpl.md[]).","title":"snapshot: Snapshot"},{"location":"TransactionalWrite/#writing-data-out-result-of-structured-query","text":"writeFiles ( data : Dataset [ _ ]) : Seq [ AddFile ] writeFiles ( data : Dataset [ _ ], writeOptions : Option [ DeltaOptions ]) : Seq [ AddFile ] writeFiles ( data : Dataset [ _ ], isOptimize : Boolean ) : Seq [ AddFile ] writeFiles ( data : Dataset [ _ ], writeOptions : Option [ DeltaOptions ], isOptimize : Boolean ) : Seq [ AddFile ] writeFiles creates a DeltaInvariantCheckerExec and a DelayedCommitProtocol to write out files to the data path (of the DeltaLog ). Note writeFiles uses Spark SQL's FileFormatWriter utility to write out a result of a streaming query. Read up on FileFormatWriter in The Internals of Spark SQL online book. writeFiles is executed within SQLExecution.withNewExecutionId . Note writeFiles can be tracked using web UI or SQLAppStatusListener (using SparkListenerSQLExecutionStart and SparkListenerSQLExecutionEnd events). In the end, writeFiles returns the addedStatuses of the DelayedCommitProtocol committer. Internally, writeFiles turns the hasWritten flag on ( true ). NOTE: After writeFiles , no metadata updates in the transaction are permitted. writeFiles normalize the given data dataset (based on the partitionColumns of the Metadata ). writeFiles getPartitioningColumns based on the partitionSchema of the Metadata . writeFiles creates a DelayedCommitProtocol committer for the data path of the DeltaLog . writeFiles gets the invariants from the schema of the Metadata . writeFiles requests a new Execution ID (that is used to track all Spark jobs of FileFormatWriter.write in Spark SQL) with a physical query plan of a new DeltaInvariantCheckerExec unary physical operator (with the executed plan of the normalized query execution as the child operator). writeFiles is used when: DeleteCommand , MergeIntoCommand , UpdateCommand , and WriteIntoDelta commands are executed DeltaSink is requested to add a streaming micro-batch == [[getCommitter]] Creating Committer","title":" Writing Data Out (Result Of Structured Query)"},{"location":"TransactionalWrite/#source-scala_3","text":"getCommitter( outputPath: Path): DelayedCommitProtocol getCommitter creates a new < > with the delta job ID and the given outputPath (and no random prefix). getCommitter is used when TransactionalWrite is requested to < >. == [[makeOutputNullable]] makeOutputNullable Method","title":"[source, scala]"},{"location":"TransactionalWrite/#source-scala_4","text":"makeOutputNullable( output: Seq[Attribute]): Seq[Attribute] makeOutputNullable...FIXME makeOutputNullable is used when...FIXME == [[normalizeData]] normalizeData Method","title":"[source, scala]"},{"location":"TransactionalWrite/#source-scala_5","text":"normalizeData( data: Dataset[_], partitionCols: Seq[String]): (QueryExecution, Seq[Attribute]) normalizeData...FIXME normalizeData is used when...FIXME == [[getPartitioningColumns]] getPartitioningColumns Method","title":"[source, scala]"},{"location":"TransactionalWrite/#source-scala_6","text":"getPartitioningColumns( partitionSchema: StructType, output: Seq[Attribute], colsDropped: Boolean): Seq[Attribute] getPartitioningColumns...FIXME getPartitioningColumns is used when...FIXME == [[hasWritten]] hasWritten Flag","title":"[source, scala]"},{"location":"TransactionalWrite/#source-scala_7","text":"","title":"[source, scala]"},{"location":"TransactionalWrite/#haswritten-boolean-false","text":"TransactionalWrite uses the hasWritten internal registry to prevent OptimisticTransactionImpl from < > after < >. hasWritten is initially turned off ( false ). It can be turned on ( true ) when TransactionalWrite is requested to < >.","title":"hasWritten: Boolean = false"},{"location":"VerifyChecksum/","text":"= VerifyChecksum VerifyChecksum is...FIXME == [[validateChecksum]] validateChecksum Method [source, scala] \u00b6 validateChecksum(snapshot: Snapshot): Unit \u00b6 validateChecksum ...FIXME NOTE: validateChecksum is used when...FIXME","title":"VerifyChecksum"},{"location":"VerifyChecksum/#source-scala","text":"","title":"[source, scala]"},{"location":"VerifyChecksum/#validatechecksumsnapshot-snapshot-unit","text":"validateChecksum ...FIXME NOTE: validateChecksum is used when...FIXME","title":"validateChecksum(snapshot: Snapshot): Unit"},{"location":"installation/","text":"Installation \u00b6 Delta Lake is a Spark data source and as such installation boils down to using spark-submit's --packages command-line option. Delta Lake also requires DeltaSparkSessionExtension and DeltaCatalog to be registered (using respective configuration properties). Spark SQL Application \u00b6 import org.apache.spark.sql.SparkSession val spark = SparkSession . builder () . appName ( \"...\" ) . config ( \"spark.sql.extensions\" , \"io.delta.sql.DeltaSparkSessionExtension\" ) . config ( \"spark.sql.catalog.spark_catalog\" , \"org.apache.spark.sql.delta.catalog.DeltaCatalog\" ) . getOrCreate Spark Shell \u00b6 ./bin/spark-shell \\ --packages io.delta:delta-core_2.12:{{ delta.version }} \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog","title":"Installation"},{"location":"installation/#installation","text":"Delta Lake is a Spark data source and as such installation boils down to using spark-submit's --packages command-line option. Delta Lake also requires DeltaSparkSessionExtension and DeltaCatalog to be registered (using respective configuration properties).","title":"Installation"},{"location":"installation/#spark-sql-application","text":"import org.apache.spark.sql.SparkSession val spark = SparkSession . builder () . appName ( \"...\" ) . config ( \"spark.sql.extensions\" , \"io.delta.sql.DeltaSparkSessionExtension\" ) . config ( \"spark.sql.catalog.spark_catalog\" , \"org.apache.spark.sql.delta.catalog.DeltaCatalog\" ) . getOrCreate","title":" Spark SQL Application"},{"location":"installation/#spark-shell","text":"./bin/spark-shell \\ --packages io.delta:delta-core_2.12:{{ delta.version }} \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog","title":" Spark Shell"},{"location":"others/","text":"= The Others (Contenders) As it happens in the open source software world, Delta Lake is not alone. The following is a list of some other open source projects that seems to compete or cover the same use cases. == Apache Iceberg https://iceberg.incubator.apache.org/[Apache Iceberg] is an open table format for huge analytic datasets. Iceberg adds tables to Presto and Spark that use a high-performance format that works just like a SQL table. (video) https://databricks.com/session_eu19/acid-orc-iceberg-and-delta-lake-an-overview-of-table-formats-for-large-scale-storage-and-analytics[ACID ORC, Iceberg, and Delta Lake\u2014An Overview of Table Formats for Large Scale Storage and Analytics] (video) https://youtu.be/D0vd325CqoM[Introducing Iceberg Tables designed for object stores] (video) https://youtu.be/z7p_m17BXs8[Introducing Apache Iceberg: Tables Designed for Object Stores] (video) https://youtu.be/nWwQMlrjhy0[Iceberg : a fast table format for S3] == Apache Hudi https://hudi.incubator.apache.org/[Apache Hudi] ingests and manages storage of large analytical datasets over DFS (HDFS or cloud stores) and provides three logical views for query access. (video) https://youtu.be/7Wudjc-v7CA[Hoodie : An Open Source Incremental Processing Framework From Uber] (video) https://youtu.be/1w3IpavhSWA[Powering Uber's global network analytics pipelines in real-time with Apache Hudi]","title":"The Others (Contenders)"},{"location":"overview/","text":"Overview \u00b6 Delta Lake is an open-source storage management system (storage layer) that brings ACID transactions and time travel to Apache Spark and big data workloads. Important As of 0.7.0 Delta Lake requires Spark 3 (starting from the first 3.0.0 release). Delta Lake introduces a concept of DeltaTable that is simply a parquet table with a transactional log . Changes to (the state of) a delta table are reflected as actions and persisted to the transactional log (in JSON format ). Delta Lake uses OptimisticTransaction for transactional writes . A commit is successful when the transaction can write the actions to a delta file (in the transactional log ). In case the delta file for the commit version already exists, the transaction is retried . Structured queries can write (transactionally) to a delta table using the following interfaces: WriteIntoDelta command for batch queries (Spark SQL) DeltaSink for streaming queries (Spark Structured Streaming) More importantly, multiple queries can write to the same delta table simultaneously (at exactly the same time). Delta Lake provides DeltaTable API to programmatically access Delta tables. A delta table can be created based on a parquet table or from scratch . Delta Lake supports batch and streaming queries (Spark SQL and Structured Streaming, respectively) using delta format. In order to fine tune queries over data in Delta Lake use options . Among the options path option is mandatory. Delta Lake supports reading and writing in batch queries: Batch reads (as a RelationProvider ) Batch writes (as a CreatableRelationProvider ) Delta Lake supports reading and writing in streaming queries: Stream reads (as a Source ) Stream writes (as a Sink ) Delta Lake uses LogStore abstraction to read and write physical log files and checkpoints (using Hadoop FileSystem API ).","title":"Overview"},{"location":"overview/#overview","text":"Delta Lake is an open-source storage management system (storage layer) that brings ACID transactions and time travel to Apache Spark and big data workloads. Important As of 0.7.0 Delta Lake requires Spark 3 (starting from the first 3.0.0 release). Delta Lake introduces a concept of DeltaTable that is simply a parquet table with a transactional log . Changes to (the state of) a delta table are reflected as actions and persisted to the transactional log (in JSON format ). Delta Lake uses OptimisticTransaction for transactional writes . A commit is successful when the transaction can write the actions to a delta file (in the transactional log ). In case the delta file for the commit version already exists, the transaction is retried . Structured queries can write (transactionally) to a delta table using the following interfaces: WriteIntoDelta command for batch queries (Spark SQL) DeltaSink for streaming queries (Spark Structured Streaming) More importantly, multiple queries can write to the same delta table simultaneously (at exactly the same time). Delta Lake provides DeltaTable API to programmatically access Delta tables. A delta table can be created based on a parquet table or from scratch . Delta Lake supports batch and streaming queries (Spark SQL and Structured Streaming, respectively) using delta format. In order to fine tune queries over data in Delta Lake use options . Among the options path option is mandatory. Delta Lake supports reading and writing in batch queries: Batch reads (as a RelationProvider ) Batch writes (as a CreatableRelationProvider ) Delta Lake supports reading and writing in streaming queries: Stream reads (as a Source ) Stream writes (as a Sink ) Delta Lake uses LogStore abstraction to read and write physical log files and checkpoints (using Hadoop FileSystem API ).","title":"Overview"},{"location":"spark-logging/","text":"= Logging Delta Lake uses http://logging.apache.org/log4j[log4j ] for logging (as does Apache Spark itself). == [[levels]] Logging Levels The valid logging levels are http://logging.apache.org/log4j/2.x/log4j-api/apidocs/index.html[log4j's Levels] (from most specific to least): OFF (most specific, no logging) FATAL (most specific, little data) ERROR WARN INFO DEBUG TRACE (least specific, a lot of data) ALL (least specific, all data) == [[log4j-properties]] conf/log4j.properties You can set up the default logging for Delta applications in conf/log4j.properties of the Spark installation. Use Spark's conf/log4j.properties.template as a starting point. == [[sbt]] sbt When running a Delta application from within sbt using run task, you can use the following build.sbt to configure logging levels: [source, scala] \u00b6 fork in run := true javaOptions in run ++= Seq( \"-Dlog4j.debug=true\", \"-Dlog4j.configuration=log4j.properties\") outputStrategy := Some(StdoutOutput) With the above configuration log4j.properties file should be on CLASSPATH which can be in src/main/resources directory (that is included in CLASSPATH by default). When run starts, you should see the following output in sbt: [spark-activator]> run [info] Running StreamingApp log4j: Trying to find [log4j.properties] using context classloader sun.misc.Launcher$AppClassLoader@1b6d3586. log4j: Using URL [file:.../classes/log4j.properties] for automatic log4j configuration. log4j: Reading configuration from URL file:.../classes/log4j.properties","title":"Logging"},{"location":"spark-logging/#source-scala","text":"fork in run := true javaOptions in run ++= Seq( \"-Dlog4j.debug=true\", \"-Dlog4j.configuration=log4j.properties\") outputStrategy := Some(StdoutOutput) With the above configuration log4j.properties file should be on CLASSPATH which can be in src/main/resources directory (that is included in CLASSPATH by default). When run starts, you should see the following output in sbt: [spark-activator]> run [info] Running StreamingApp log4j: Trying to find [log4j.properties] using context classloader sun.misc.Launcher$AppClassLoader@1b6d3586. log4j: Using URL [file:.../classes/log4j.properties] for automatic log4j configuration. log4j: Reading configuration from URL file:.../classes/log4j.properties","title":"[source, scala]"},{"location":"time-travel/","text":"Time Travel \u00b6 < > option may optionally specify time travel . The format is defined per the following regular expressions: .*@(\\\\d&#123;17})$$ ( TIMESTAMP_URI_FOR_TIME_TRAVEL ), e.g. @(yyyyMMddHHmmssSSS) .*@[vV](\\d+)$ ( VERSION_URI_FOR_TIME_TRAVEL ), e.g. @v123 The < > or < >...FIXME == [[DeltaTimeTravelSpec]] DeltaTimeTravelSpec -- Metadata of Time Travel Node DeltaTimeTravelSpec describes a time travel node: timestamp or version Optional creationSource","title":"Time Travel"},{"location":"time-travel/#time-travel","text":"< > option may optionally specify time travel . The format is defined per the following regular expressions: .*@(\\\\d&#123;17})$$ ( TIMESTAMP_URI_FOR_TIME_TRAVEL ), e.g. @(yyyyMMddHHmmssSSS) .*@[vV](\\d+)$ ( VERSION_URI_FOR_TIME_TRAVEL ), e.g. @v123 The < > or < >...FIXME == [[DeltaTimeTravelSpec]] DeltaTimeTravelSpec -- Metadata of Time Travel Node DeltaTimeTravelSpec describes a time travel node: timestamp or version Optional creationSource","title":"Time Travel"},{"location":"commands/AlterDeltaTableCommand/","text":"AlterDeltaTableCommand \u00b6 AlterDeltaTableCommand is an extension of the DeltaCommand abstraction for Delta commands that alter a DeltaTableV2 . Contract \u00b6 table \u00b6 table : DeltaTableV2 DeltaTableV2 Used when AlterDeltaTableCommand is requested to startTransaction Implementations \u00b6 AlterTableAddColumnsDeltaCommand AlterTableChangeColumnDeltaCommand AlterTableReplaceColumnsDeltaCommand AlterTableSetLocationDeltaCommand AlterTableSetPropertiesDeltaCommand AlterTableUnsetPropertiesDeltaCommand startTransaction \u00b6 startTransaction () : OptimisticTransaction startTransaction simply requests the DeltaTableV2 for the DeltaLog that in turn is requested to startTransaction .","title":"AlterDeltaTableCommand"},{"location":"commands/AlterDeltaTableCommand/#alterdeltatablecommand","text":"AlterDeltaTableCommand is an extension of the DeltaCommand abstraction for Delta commands that alter a DeltaTableV2 .","title":"AlterDeltaTableCommand"},{"location":"commands/AlterDeltaTableCommand/#contract","text":"","title":"Contract"},{"location":"commands/AlterDeltaTableCommand/#table","text":"table : DeltaTableV2 DeltaTableV2 Used when AlterDeltaTableCommand is requested to startTransaction","title":" table"},{"location":"commands/AlterDeltaTableCommand/#implementations","text":"AlterTableAddColumnsDeltaCommand AlterTableChangeColumnDeltaCommand AlterTableReplaceColumnsDeltaCommand AlterTableSetLocationDeltaCommand AlterTableSetPropertiesDeltaCommand AlterTableUnsetPropertiesDeltaCommand","title":"Implementations"},{"location":"commands/AlterDeltaTableCommand/#starttransaction","text":"startTransaction () : OptimisticTransaction startTransaction simply requests the DeltaTableV2 for the DeltaLog that in turn is requested to startTransaction .","title":" startTransaction"},{"location":"commands/AlterTableAddColumnsDeltaCommand/","text":"AlterTableAddColumnsDeltaCommand \u00b6 AlterTableAddColumnsDeltaCommand is...FIXME","title":"AlterTableAddColumnsDeltaCommand"},{"location":"commands/AlterTableAddColumnsDeltaCommand/#altertableaddcolumnsdeltacommand","text":"AlterTableAddColumnsDeltaCommand is...FIXME","title":"AlterTableAddColumnsDeltaCommand"},{"location":"commands/AlterTableChangeColumnDeltaCommand/","text":"AlterTableChangeColumnDeltaCommand \u00b6 AlterTableChangeColumnDeltaCommand is...FIXME","title":"AlterTableChangeColumnDeltaCommand"},{"location":"commands/AlterTableChangeColumnDeltaCommand/#altertablechangecolumndeltacommand","text":"AlterTableChangeColumnDeltaCommand is...FIXME","title":"AlterTableChangeColumnDeltaCommand"},{"location":"commands/AlterTableReplaceColumnsDeltaCommand/","text":"AlterTableReplaceColumnsDeltaCommand \u00b6 AlterTableReplaceColumnsDeltaCommand is...FIXME","title":"AlterTableReplaceColumnsDeltaCommand"},{"location":"commands/AlterTableReplaceColumnsDeltaCommand/#altertablereplacecolumnsdeltacommand","text":"AlterTableReplaceColumnsDeltaCommand is...FIXME","title":"AlterTableReplaceColumnsDeltaCommand"},{"location":"commands/AlterTableSetLocationDeltaCommand/","text":"AlterTableSetLocationDeltaCommand \u00b6 AlterTableSetLocationDeltaCommand is...FIXME","title":"AlterTableSetLocationDeltaCommand"},{"location":"commands/AlterTableSetLocationDeltaCommand/#altertablesetlocationdeltacommand","text":"AlterTableSetLocationDeltaCommand is...FIXME","title":"AlterTableSetLocationDeltaCommand"},{"location":"commands/AlterTableSetPropertiesDeltaCommand/","text":"AlterTableSetPropertiesDeltaCommand \u00b6 AlterTableSetPropertiesDeltaCommand is...FIXME","title":"AlterTableSetPropertiesDeltaCommand"},{"location":"commands/AlterTableSetPropertiesDeltaCommand/#altertablesetpropertiesdeltacommand","text":"AlterTableSetPropertiesDeltaCommand is...FIXME","title":"AlterTableSetPropertiesDeltaCommand"},{"location":"commands/AlterTableUnsetPropertiesDeltaCommand/","text":"AlterTableUnsetPropertiesDeltaCommand \u00b6 AlterTableUnsetPropertiesDeltaCommand is...FIXME","title":"AlterTableUnsetPropertiesDeltaCommand"},{"location":"commands/AlterTableUnsetPropertiesDeltaCommand/#altertableunsetpropertiesdeltacommand","text":"AlterTableUnsetPropertiesDeltaCommand is...FIXME","title":"AlterTableUnsetPropertiesDeltaCommand"},{"location":"commands/ConvertToDeltaCommand/","text":"ConvertToDeltaCommand \u00b6 ConvertToDeltaCommand is a DeltaCommand that converts a parquet table into delta format ( imports it into Delta). ConvertToDeltaCommand is a Spark SQL RunnableCommand (and executed eagerly on the driver for side-effects). ConvertToDeltaCommand requires that the partition schema matches the partitions of the parquet table ( or an AnalysisException is thrown ) ConvertToDeltaCommandBase is the base of ConvertToDeltaCommand -like commands with the only known implementation being ConvertToDeltaCommand itself. Creating Instance \u00b6 ConvertToDeltaCommand takes the following to be created: Parquet table ( TableIdentifier ) Partition schema ( Option[StructType] ) Delta Path ( Option[String] ) ConvertToDeltaCommand is created when: CONVERT TO DELTA statement is used (and DeltaSqlAstBuilder is requested to visitConvert ) DeltaTable.convertToDelta utility is used (and DeltaConvert utility is used to executeConvert ) Executing Command \u00b6 run ( spark : SparkSession ) : Seq [ Row ] run is part of the RunnableCommand contract. run < > from the < > (with the given SparkSession ). run makes sure that the (data source) provider (the database part of the < >) is either delta or parquet . For all other data source providers, run throws an AnalysisException : CONVERT TO DELTA only supports parquet tables, but you are trying to convert a [sourceName] source: [ident] For delta data source provider, run simply prints out the following message to standard output and returns. The table you are trying to convert is already a delta table For parquet data source provider, run uses DeltaLog utility to < >. run then requests DeltaLog to < > and < >. In the end, run < >. In case the < > of the new transaction is greater than -1 , run simply prints out the following message to standard output and returns. The table you are trying to convert is already a delta table Internal Helper Methods \u00b6 performConvert Method \u00b6 performConvert ( spark : SparkSession , txn : OptimisticTransaction , convertProperties : ConvertTarget ) : Seq [ Row ] performConvert makes sure that the directory exists (from the given ConvertProperties which is the table part of the < > of the command). performConvert requests the OptimisticTransaction for the < > that is then requested to < >. performConvert < > in the directory and leaves only files (by filtering out directories using WHERE clause). NOTE: performConvert uses Dataset API to build a distributed computation to query files. [[performConvert-cache]] performConvert caches the Dataset of file names. [[performConvert-schemaBatchSize]] performConvert uses < > configuration property for the number of files per batch for schema inference. performConvert < > for every batch of files and then < >. performConvert < > using the inferred table schema and the < > (if specified). performConvert creates a new < > using the table schema and the < > (if specified). performConvert requests the OptimisticTransaction to < >. [[performConvert-statsBatchSize]] performConvert uses < > configuration property for the number of files per batch for stats collection. performConvert < > (in the < > of the < > of the OptimisticTransaction ) for every file in a batch. [[performConvert-streamWrite]][[performConvert-unpersist]] In the end, performConvert < > (with the OptimisticTransaction , the AddFile s, and Operation.md#Convert[Convert] operation) and unpersists the Dataset of file names. streamWrite Method \u00b6 streamWrite ( spark : SparkSession , txn : OptimisticTransaction , addFiles : Iterator [ AddFile ], op : DeltaOperations.Operation , numFiles : Long ) : Long streamWrite ...FIXME createAddFile Method \u00b6 createAddFile ( file : SerializableFileStatus , basePath : Path , fs : FileSystem , conf : SQLConf ) : AddFile createAddFile creates an AddFile action. Internally, createAddFile ...FIXME createAddFile throws an AnalysisException if the number of fields in the given < > does not match the number of partitions found (at partition discovery phase): Expecting [size] partition column(s): [expectedCols], but found [size] partition column(s): [parsedCols] from parsing the file name: [path] mergeSchemasInParallel Method \u00b6 mergeSchemasInParallel ( sparkSession : SparkSession , filesToTouch : Seq [ FileStatus ], serializedConf : SerializableConfiguration ) : Option [ StructType ] mergeSchemasInParallel ...FIXME constructTableSchema Method \u00b6 constructTableSchema ( spark : SparkSession , dataSchema : StructType , partitionFields : Seq [ StructField ]) : StructType constructTableSchema ...FIXME","title":"ConvertToDeltaCommand"},{"location":"commands/ConvertToDeltaCommand/#converttodeltacommand","text":"ConvertToDeltaCommand is a DeltaCommand that converts a parquet table into delta format ( imports it into Delta). ConvertToDeltaCommand is a Spark SQL RunnableCommand (and executed eagerly on the driver for side-effects). ConvertToDeltaCommand requires that the partition schema matches the partitions of the parquet table ( or an AnalysisException is thrown ) ConvertToDeltaCommandBase is the base of ConvertToDeltaCommand -like commands with the only known implementation being ConvertToDeltaCommand itself.","title":"ConvertToDeltaCommand"},{"location":"commands/ConvertToDeltaCommand/#creating-instance","text":"ConvertToDeltaCommand takes the following to be created: Parquet table ( TableIdentifier ) Partition schema ( Option[StructType] ) Delta Path ( Option[String] ) ConvertToDeltaCommand is created when: CONVERT TO DELTA statement is used (and DeltaSqlAstBuilder is requested to visitConvert ) DeltaTable.convertToDelta utility is used (and DeltaConvert utility is used to executeConvert )","title":"Creating Instance"},{"location":"commands/ConvertToDeltaCommand/#executing-command","text":"run ( spark : SparkSession ) : Seq [ Row ] run is part of the RunnableCommand contract. run < > from the < > (with the given SparkSession ). run makes sure that the (data source) provider (the database part of the < >) is either delta or parquet . For all other data source providers, run throws an AnalysisException : CONVERT TO DELTA only supports parquet tables, but you are trying to convert a [sourceName] source: [ident] For delta data source provider, run simply prints out the following message to standard output and returns. The table you are trying to convert is already a delta table For parquet data source provider, run uses DeltaLog utility to < >. run then requests DeltaLog to < > and < >. In the end, run < >. In case the < > of the new transaction is greater than -1 , run simply prints out the following message to standard output and returns. The table you are trying to convert is already a delta table","title":" Executing Command"},{"location":"commands/ConvertToDeltaCommand/#internal-helper-methods","text":"","title":"Internal Helper Methods"},{"location":"commands/ConvertToDeltaCommand/#performconvert-method","text":"performConvert ( spark : SparkSession , txn : OptimisticTransaction , convertProperties : ConvertTarget ) : Seq [ Row ] performConvert makes sure that the directory exists (from the given ConvertProperties which is the table part of the < > of the command). performConvert requests the OptimisticTransaction for the < > that is then requested to < >. performConvert < > in the directory and leaves only files (by filtering out directories using WHERE clause). NOTE: performConvert uses Dataset API to build a distributed computation to query files. [[performConvert-cache]] performConvert caches the Dataset of file names. [[performConvert-schemaBatchSize]] performConvert uses < > configuration property for the number of files per batch for schema inference. performConvert < > for every batch of files and then < >. performConvert < > using the inferred table schema and the < > (if specified). performConvert creates a new < > using the table schema and the < > (if specified). performConvert requests the OptimisticTransaction to < >. [[performConvert-statsBatchSize]] performConvert uses < > configuration property for the number of files per batch for stats collection. performConvert < > (in the < > of the < > of the OptimisticTransaction ) for every file in a batch. [[performConvert-streamWrite]][[performConvert-unpersist]] In the end, performConvert < > (with the OptimisticTransaction , the AddFile s, and Operation.md#Convert[Convert] operation) and unpersists the Dataset of file names.","title":" performConvert Method"},{"location":"commands/ConvertToDeltaCommand/#streamwrite-method","text":"streamWrite ( spark : SparkSession , txn : OptimisticTransaction , addFiles : Iterator [ AddFile ], op : DeltaOperations.Operation , numFiles : Long ) : Long streamWrite ...FIXME","title":" streamWrite Method"},{"location":"commands/ConvertToDeltaCommand/#createaddfile-method","text":"createAddFile ( file : SerializableFileStatus , basePath : Path , fs : FileSystem , conf : SQLConf ) : AddFile createAddFile creates an AddFile action. Internally, createAddFile ...FIXME createAddFile throws an AnalysisException if the number of fields in the given < > does not match the number of partitions found (at partition discovery phase): Expecting [size] partition column(s): [expectedCols], but found [size] partition column(s): [parsedCols] from parsing the file name: [path]","title":" createAddFile Method"},{"location":"commands/ConvertToDeltaCommand/#mergeschemasinparallel-method","text":"mergeSchemasInParallel ( sparkSession : SparkSession , filesToTouch : Seq [ FileStatus ], serializedConf : SerializableConfiguration ) : Option [ StructType ] mergeSchemasInParallel ...FIXME","title":" mergeSchemasInParallel Method"},{"location":"commands/ConvertToDeltaCommand/#constructtableschema-method","text":"constructTableSchema ( spark : SparkSession , dataSchema : StructType , partitionFields : Seq [ StructField ]) : StructType constructTableSchema ...FIXME","title":" constructTableSchema Method"},{"location":"commands/CreateDeltaTableCommand/","text":"CreateDeltaTableCommand \u00b6 CreateDeltaTableCommand is a Spark SQL RunnableCommand (and executed eagerly on the driver for side-effects). Creating Instance \u00b6 CreateDeltaTableCommand takes the following to be created: CatalogTable ( Spark SQL ) Existing CatalogTable (if available) SaveMode Optional Data Query ( LogicalPlan ) CreationMode (default: TableCreationModes.Create ) tableByPath flag (default: false ) CreateDeltaTableCommand is created when DeltaCatalog is requested to create a Delta table . Executing Command \u00b6 run ( sparkSession : SparkSession ) : Seq [ Row ] run creates a DeltaLog (for the given table based on a table location) and a DeltaOptions . run starts a transaction (on the DeltaLog ). run branches off based on the optional data query . For data query defined, run creates a WriteIntoDelta and requests it to write . Otherwise, run creates an empty table. Note run does a bit more, but I don't think it's of much interest. run commits the transaction . In the end, run updateCatalog . run is part of the RunnableCommand abstraction. Updating Catalog \u00b6 updateCatalog ( spark : SparkSession , table : CatalogTable ) : Unit updateCatalog uses the given SparkSession to access SessionCatalog to createTable or alterTable when the tableByPath flag is off. Otherwise, updateCatalog does nothing.","title":"CreateDeltaTableCommand"},{"location":"commands/CreateDeltaTableCommand/#createdeltatablecommand","text":"CreateDeltaTableCommand is a Spark SQL RunnableCommand (and executed eagerly on the driver for side-effects).","title":"CreateDeltaTableCommand"},{"location":"commands/CreateDeltaTableCommand/#creating-instance","text":"CreateDeltaTableCommand takes the following to be created: CatalogTable ( Spark SQL ) Existing CatalogTable (if available) SaveMode Optional Data Query ( LogicalPlan ) CreationMode (default: TableCreationModes.Create ) tableByPath flag (default: false ) CreateDeltaTableCommand is created when DeltaCatalog is requested to create a Delta table .","title":"Creating Instance"},{"location":"commands/CreateDeltaTableCommand/#executing-command","text":"run ( sparkSession : SparkSession ) : Seq [ Row ] run creates a DeltaLog (for the given table based on a table location) and a DeltaOptions . run starts a transaction (on the DeltaLog ). run branches off based on the optional data query . For data query defined, run creates a WriteIntoDelta and requests it to write . Otherwise, run creates an empty table. Note run does a bit more, but I don't think it's of much interest. run commits the transaction . In the end, run updateCatalog . run is part of the RunnableCommand abstraction.","title":" Executing Command"},{"location":"commands/CreateDeltaTableCommand/#updating-catalog","text":"updateCatalog ( spark : SparkSession , table : CatalogTable ) : Unit updateCatalog uses the given SparkSession to access SessionCatalog to createTable or alterTable when the tableByPath flag is off. Otherwise, updateCatalog does nothing.","title":" Updating Catalog"},{"location":"commands/DeleteCommand/","text":"DeleteCommand \u00b6 DeleteCommand is a < > that < >. DeleteCommand is < > (using < > factory utility) and < > when < > operator is used (indirectly through DeltaTableOperations when requested to < >). == [[creating-instance]] Creating DeleteCommand Instance DeleteCommand takes the following to be created: [[tahoeFileIndex]] TahoeFileIndex [[target]] Target LogicalPlan [[condition]] Optional Catalyst expression == [[apply]] Creating DeleteCommand Instance -- apply Factory Utility [source, scala] \u00b6 apply(delete: Delete): DeleteCommand \u00b6 apply ...FIXME NOTE: apply is used when...FIXME == [[run]] Running Command -- run Method [source, scala] \u00b6 run(sparkSession: SparkSession): Seq[Row] \u00b6 NOTE: run is part of the RunnableCommand contract to...FIXME. run requests the < > for the < >. run requests the DeltaLog to < > for < >. In the end, run re-caches all cached plans (incl. this relation itself) by requesting the CacheManager to recache the < >. == [[performDelete]] performDelete Internal Method [source, scala] \u00b6 performDelete( sparkSession: SparkSession, deltaLog: DeltaLog, txn: OptimisticTransaction): Unit performDelete ...FIXME NOTE: performDelete is used exclusively when DeleteCommand is requested to < >.","title":"DeleteCommand"},{"location":"commands/DeleteCommand/#deletecommand","text":"DeleteCommand is a < > that < >. DeleteCommand is < > (using < > factory utility) and < > when < > operator is used (indirectly through DeltaTableOperations when requested to < >). == [[creating-instance]] Creating DeleteCommand Instance DeleteCommand takes the following to be created: [[tahoeFileIndex]] TahoeFileIndex [[target]] Target LogicalPlan [[condition]] Optional Catalyst expression == [[apply]] Creating DeleteCommand Instance -- apply Factory Utility","title":"DeleteCommand"},{"location":"commands/DeleteCommand/#source-scala","text":"","title":"[source, scala]"},{"location":"commands/DeleteCommand/#applydelete-delete-deletecommand","text":"apply ...FIXME NOTE: apply is used when...FIXME == [[run]] Running Command -- run Method","title":"apply(delete: Delete): DeleteCommand"},{"location":"commands/DeleteCommand/#source-scala_1","text":"","title":"[source, scala]"},{"location":"commands/DeleteCommand/#runsparksession-sparksession-seqrow","text":"NOTE: run is part of the RunnableCommand contract to...FIXME. run requests the < > for the < >. run requests the DeltaLog to < > for < >. In the end, run re-caches all cached plans (incl. this relation itself) by requesting the CacheManager to recache the < >. == [[performDelete]] performDelete Internal Method","title":"run(sparkSession: SparkSession): Seq[Row]"},{"location":"commands/DeleteCommand/#source-scala_2","text":"performDelete( sparkSession: SparkSession, deltaLog: DeltaLog, txn: OptimisticTransaction): Unit performDelete ...FIXME NOTE: performDelete is used exclusively when DeleteCommand is requested to < >.","title":"[source, scala]"},{"location":"commands/DeltaCommand/","text":"DeltaCommand \u00b6 DeltaCommand is a marker interface for commands to work with data in delta tables. Implementations \u00b6 AlterDeltaTableCommand ConvertToDeltaCommand DeleteCommand MergeIntoCommand UpdateCommand VacuumCommandImpl WriteIntoDelta parsePartitionPredicates Method \u00b6 parsePartitionPredicates ( spark : SparkSession , predicate : String ) : Seq [ Expression ] parsePartitionPredicates ...FIXME parsePartitionPredicates is used when...FIXME verifyPartitionPredicates Method \u00b6 verifyPartitionPredicates ( spark : SparkSession , partitionColumns : Seq [ String ], predicates : Seq [ Expression ]) : Unit verifyPartitionPredicates ...FIXME verifyPartitionPredicates is used when...FIXME generateCandidateFileMap Method \u00b6 generateCandidateFileMap ( basePath : Path , candidateFiles : Seq [ AddFile ]) : Map [ String , AddFile ] generateCandidateFileMap ...FIXME generateCandidateFileMap is used when...FIXME removeFilesFromPaths Method \u00b6 removeFilesFromPaths ( deltaLog : DeltaLog , nameToAddFileMap : Map [ String , AddFile ], filesToRewrite : Seq [ String ], operationTimestamp : Long ) : Seq [ RemoveFile ] removeFilesFromPaths ...FIXME removeFilesFromPaths is used when DeleteCommand and UpdateCommand commands are executed. Creating HadoopFsRelation (With TahoeBatchFileIndex) \u00b6 buildBaseRelation ( spark : SparkSession , txn : OptimisticTransaction , actionType : String , rootPath : Path , inputLeafFiles : Seq [ String ], nameToAddFileMap : Map [ String , AddFile ]) : HadoopFsRelation [[buildBaseRelation-scannedFiles]] buildBaseRelation converts the given inputLeafFiles to < > (with the given rootPath and nameToAddFileMap ). buildBaseRelation creates a < > for the actionType , the < > and the rootPath . In the end, buildBaseRelation creates a HadoopFsRelation with the TahoeBatchFileIndex (and the other properties based on the < > of the given < >). buildBaseRelation is used when DeleteCommand and UpdateCommand commands are executed (with delete and update action types, respectively). getTouchedFile Method \u00b6 getTouchedFile ( basePath : Path , filePath : String , nameToAddFileMap : Map [ String , AddFile ]) : AddFile getTouchedFile ...FIXME getTouchedFile is used when: DeltaCommand is requested to removeFilesFromPaths and create a HadoopFsRelation (for DeleteCommand and UpdateCommand commands) MergeIntoCommand is executed isCatalogTable Method \u00b6 isCatalogTable ( analyzer : Analyzer , tableIdent : TableIdentifier ) : Boolean isCatalogTable ...FIXME isCatalogTable is used when...FIXME isPathIdentifier Method \u00b6 isPathIdentifier ( tableIdent : TableIdentifier ) : Boolean isPathIdentifier ...FIXME isPathIdentifier is used when...FIXME","title":"DeltaCommand"},{"location":"commands/DeltaCommand/#deltacommand","text":"DeltaCommand is a marker interface for commands to work with data in delta tables.","title":"DeltaCommand"},{"location":"commands/DeltaCommand/#implementations","text":"AlterDeltaTableCommand ConvertToDeltaCommand DeleteCommand MergeIntoCommand UpdateCommand VacuumCommandImpl WriteIntoDelta","title":"Implementations"},{"location":"commands/DeltaCommand/#parsepartitionpredicates-method","text":"parsePartitionPredicates ( spark : SparkSession , predicate : String ) : Seq [ Expression ] parsePartitionPredicates ...FIXME parsePartitionPredicates is used when...FIXME","title":" parsePartitionPredicates Method"},{"location":"commands/DeltaCommand/#verifypartitionpredicates-method","text":"verifyPartitionPredicates ( spark : SparkSession , partitionColumns : Seq [ String ], predicates : Seq [ Expression ]) : Unit verifyPartitionPredicates ...FIXME verifyPartitionPredicates is used when...FIXME","title":" verifyPartitionPredicates Method"},{"location":"commands/DeltaCommand/#generatecandidatefilemap-method","text":"generateCandidateFileMap ( basePath : Path , candidateFiles : Seq [ AddFile ]) : Map [ String , AddFile ] generateCandidateFileMap ...FIXME generateCandidateFileMap is used when...FIXME","title":" generateCandidateFileMap Method"},{"location":"commands/DeltaCommand/#removefilesfrompaths-method","text":"removeFilesFromPaths ( deltaLog : DeltaLog , nameToAddFileMap : Map [ String , AddFile ], filesToRewrite : Seq [ String ], operationTimestamp : Long ) : Seq [ RemoveFile ] removeFilesFromPaths ...FIXME removeFilesFromPaths is used when DeleteCommand and UpdateCommand commands are executed.","title":" removeFilesFromPaths Method"},{"location":"commands/DeltaCommand/#creating-hadoopfsrelation-with-tahoebatchfileindex","text":"buildBaseRelation ( spark : SparkSession , txn : OptimisticTransaction , actionType : String , rootPath : Path , inputLeafFiles : Seq [ String ], nameToAddFileMap : Map [ String , AddFile ]) : HadoopFsRelation [[buildBaseRelation-scannedFiles]] buildBaseRelation converts the given inputLeafFiles to < > (with the given rootPath and nameToAddFileMap ). buildBaseRelation creates a < > for the actionType , the < > and the rootPath . In the end, buildBaseRelation creates a HadoopFsRelation with the TahoeBatchFileIndex (and the other properties based on the < > of the given < >). buildBaseRelation is used when DeleteCommand and UpdateCommand commands are executed (with delete and update action types, respectively).","title":" Creating HadoopFsRelation (With TahoeBatchFileIndex)"},{"location":"commands/DeltaCommand/#gettouchedfile-method","text":"getTouchedFile ( basePath : Path , filePath : String , nameToAddFileMap : Map [ String , AddFile ]) : AddFile getTouchedFile ...FIXME getTouchedFile is used when: DeltaCommand is requested to removeFilesFromPaths and create a HadoopFsRelation (for DeleteCommand and UpdateCommand commands) MergeIntoCommand is executed","title":" getTouchedFile Method"},{"location":"commands/DeltaCommand/#iscatalogtable-method","text":"isCatalogTable ( analyzer : Analyzer , tableIdent : TableIdentifier ) : Boolean isCatalogTable ...FIXME isCatalogTable is used when...FIXME","title":" isCatalogTable Method"},{"location":"commands/DeltaCommand/#ispathidentifier-method","text":"isPathIdentifier ( tableIdent : TableIdentifier ) : Boolean isPathIdentifier ...FIXME isPathIdentifier is used when...FIXME","title":" isPathIdentifier Method"},{"location":"commands/DeltaGenerateCommand/","text":"= DeltaGenerateCommand -- Executing Generation Functions On Delta Tables DeltaGenerateCommand is a concrete < > (and so can < >) that can < >. [[symlink_format_manifest]] DeltaGenerateCommand supports symlink_format_manifest only for the < >. [source,text] \u00b6 val generateQ = \"\"\"GENERATE symlink_format_manifest for table delta. /tmp/delta/t1 \"\"\" scala> sql(generateQ).foreach(_ => ()) [[modeNameToGenerationFunc]] DeltaGenerateCommand uses a lookup table for generation functions per mode: Uses < > for the only-supported < > DeltaGenerateCommand is < > when: DeltaSqlAstBuilder is requested to < > < > operator is used (that < >) == [[creating-instance]] Creating DeltaGenerateCommand Instance DeltaGenerateCommand takes the following to be created: [[modeName]] Mode (< > is the only supported mode) [[tableId]] Delta table ( TableIdentifier ) == [[run]] Running Command -- run Method [source, scala] \u00b6 run(sparkSession: SparkSession): Seq[Row] \u00b6 NOTE: run is part of the RunnableCommand contract to...FIXME. run < > for the < > (from the < >). run finds the generate function for the mode (in the < > registry) and applies ( executes ) it to the DeltaLog . run throws an AnalysisException when the < > of the < > of the DeltaLog is negative (less than 0 ): Delta table not found at [tablePath]. run throws an IllegalArgumentException for unsupported < >: Specified mode '[modeName]' is not supported. Supported modes are: symlink_format_manifest","title":"DeltaGenerateCommand"},{"location":"commands/DeltaGenerateCommand/#sourcetext","text":"val generateQ = \"\"\"GENERATE symlink_format_manifest for table delta. /tmp/delta/t1 \"\"\" scala> sql(generateQ).foreach(_ => ()) [[modeNameToGenerationFunc]] DeltaGenerateCommand uses a lookup table for generation functions per mode: Uses < > for the only-supported < > DeltaGenerateCommand is < > when: DeltaSqlAstBuilder is requested to < > < > operator is used (that < >) == [[creating-instance]] Creating DeltaGenerateCommand Instance DeltaGenerateCommand takes the following to be created: [[modeName]] Mode (< > is the only supported mode) [[tableId]] Delta table ( TableIdentifier ) == [[run]] Running Command -- run Method","title":"[source,text]"},{"location":"commands/DeltaGenerateCommand/#source-scala","text":"","title":"[source, scala]"},{"location":"commands/DeltaGenerateCommand/#runsparksession-sparksession-seqrow","text":"NOTE: run is part of the RunnableCommand contract to...FIXME. run < > for the < > (from the < >). run finds the generate function for the mode (in the < > registry) and applies ( executes ) it to the DeltaLog . run throws an AnalysisException when the < > of the < > of the DeltaLog is negative (less than 0 ): Delta table not found at [tablePath]. run throws an IllegalArgumentException for unsupported < >: Specified mode '[modeName]' is not supported. Supported modes are: symlink_format_manifest","title":"run(sparkSession: SparkSession): Seq[Row]"},{"location":"commands/DeltaMergeBuilder/","text":"DeltaMergeBuilder \u00b6 DeltaMergeBuilder is a builder interface to describe how to merge data from a source DataFrame into the target delta table (using whenMatched and whenNotMatched conditions). DeltaMergeBuilder is created using DeltaTable.merge operator. In the end, DeltaMergeBuilder is supposed to be executed to take action. DeltaMergeBuilder creates a DeltaMergeInto logical command that is resolved to a MergeIntoCommand runnable logical command (using PreprocessTableMerge logical resolution rule). Creating Instance \u00b6 DeltaMergeBuilder takes the following to be created: Target DeltaTable Source DataFrame Condition Column When Clauses DeltaMergeBuilder is created using DeltaTable.merge operator. Operators \u00b6 whenMatched \u00b6 whenMatched () : DeltaMergeMatchedActionBuilder whenMatched ( condition : Column ) : DeltaMergeMatchedActionBuilder whenMatched ( condition : String ) : DeltaMergeMatchedActionBuilder Creates a DeltaMergeMatchedActionBuilder (for the DeltaMergeBuilder and a condition) whenNotMatched \u00b6 whenNotMatched () : DeltaMergeNotMatchedActionBuilder whenNotMatched ( condition : Column ) : DeltaMergeNotMatchedActionBuilder whenNotMatched ( condition : String ) : DeltaMergeNotMatchedActionBuilder Creates a DeltaMergeNotMatchedActionBuilder (for the DeltaMergeBuilder and a condition) Executing Merge \u00b6 execute () : Unit execute creates a merge plan (that is DeltaMergeInto logical command) and resolves column references . execute runs PreprocessTableMerge logical resolution rule on the DeltaMergeInto logical command (that gives MergeIntoCommand runnable logical command). In the end, execute executes the MergeIntoCommand logical command. Creating Logical Plan for Merge \u00b6 mergePlan : DeltaMergeInto mergePlan creates a DeltaMergeInto logical command. mergePlan is used when DeltaMergeBuilder is requested to execute . Creating DeltaMergeBuilder \u00b6 apply ( targetTable : DeltaTable , source : DataFrame , onCondition : Column ) : DeltaMergeBuilder apply utility creates a new DeltaMergeBuilder for the given parameters and no DeltaMergeIntoClauses . apply is used for DeltaTable.merge operator. Adding DeltaMergeIntoClause \u00b6 withClause ( clause : DeltaMergeIntoClause ) : DeltaMergeBuilder withClause creates a new DeltaMergeBuilder (based on the existing properties, e.g. the DeltaTable ) with the given DeltaMergeIntoClause added to the existing DeltaMergeIntoClauses (to create a more refined DeltaMergeBuilder ). withClause is used when: DeltaMergeMatchedActionBuilder is requested to updateAll , delete and addUpdateClause DeltaMergeNotMatchedActionBuilder is requested to insertAll and addInsertClause","title":"DeltaMergeBuilder"},{"location":"commands/DeltaMergeBuilder/#deltamergebuilder","text":"DeltaMergeBuilder is a builder interface to describe how to merge data from a source DataFrame into the target delta table (using whenMatched and whenNotMatched conditions). DeltaMergeBuilder is created using DeltaTable.merge operator. In the end, DeltaMergeBuilder is supposed to be executed to take action. DeltaMergeBuilder creates a DeltaMergeInto logical command that is resolved to a MergeIntoCommand runnable logical command (using PreprocessTableMerge logical resolution rule).","title":"DeltaMergeBuilder"},{"location":"commands/DeltaMergeBuilder/#creating-instance","text":"DeltaMergeBuilder takes the following to be created: Target DeltaTable Source DataFrame Condition Column When Clauses DeltaMergeBuilder is created using DeltaTable.merge operator.","title":"Creating Instance"},{"location":"commands/DeltaMergeBuilder/#operators","text":"","title":"Operators"},{"location":"commands/DeltaMergeBuilder/#whenmatched","text":"whenMatched () : DeltaMergeMatchedActionBuilder whenMatched ( condition : Column ) : DeltaMergeMatchedActionBuilder whenMatched ( condition : String ) : DeltaMergeMatchedActionBuilder Creates a DeltaMergeMatchedActionBuilder (for the DeltaMergeBuilder and a condition)","title":" whenMatched"},{"location":"commands/DeltaMergeBuilder/#whennotmatched","text":"whenNotMatched () : DeltaMergeNotMatchedActionBuilder whenNotMatched ( condition : Column ) : DeltaMergeNotMatchedActionBuilder whenNotMatched ( condition : String ) : DeltaMergeNotMatchedActionBuilder Creates a DeltaMergeNotMatchedActionBuilder (for the DeltaMergeBuilder and a condition)","title":" whenNotMatched"},{"location":"commands/DeltaMergeBuilder/#executing-merge","text":"execute () : Unit execute creates a merge plan (that is DeltaMergeInto logical command) and resolves column references . execute runs PreprocessTableMerge logical resolution rule on the DeltaMergeInto logical command (that gives MergeIntoCommand runnable logical command). In the end, execute executes the MergeIntoCommand logical command.","title":" Executing Merge"},{"location":"commands/DeltaMergeBuilder/#creating-logical-plan-for-merge","text":"mergePlan : DeltaMergeInto mergePlan creates a DeltaMergeInto logical command. mergePlan is used when DeltaMergeBuilder is requested to execute .","title":" Creating Logical Plan for Merge"},{"location":"commands/DeltaMergeBuilder/#creating-deltamergebuilder","text":"apply ( targetTable : DeltaTable , source : DataFrame , onCondition : Column ) : DeltaMergeBuilder apply utility creates a new DeltaMergeBuilder for the given parameters and no DeltaMergeIntoClauses . apply is used for DeltaTable.merge operator.","title":" Creating DeltaMergeBuilder"},{"location":"commands/DeltaMergeBuilder/#adding-deltamergeintoclause","text":"withClause ( clause : DeltaMergeIntoClause ) : DeltaMergeBuilder withClause creates a new DeltaMergeBuilder (based on the existing properties, e.g. the DeltaTable ) with the given DeltaMergeIntoClause added to the existing DeltaMergeIntoClauses (to create a more refined DeltaMergeBuilder ). withClause is used when: DeltaMergeMatchedActionBuilder is requested to updateAll , delete and addUpdateClause DeltaMergeNotMatchedActionBuilder is requested to insertAll and addInsertClause","title":" Adding DeltaMergeIntoClause"},{"location":"commands/DeltaMergeInto/","text":"DeltaMergeInto Logical Command \u00b6 DeltaMergeInto is a logical command (Spark SQL's Command ). Creating Instance \u00b6 DeltaMergeInto takes the following to be created: Target LogicalPlan Source LogicalPlan Condition Expression Matched Clauses ( Seq[DeltaMergeIntoMatchedClause] ) Optional Non-Matched Clause ( Option[DeltaMergeIntoInsertClause] ) Optional Migrated Schema (default: undefined ) DeltaMergeInto is created (using apply and resolveReferences utilities) when: DeltaMergeBuilder is requested to execute DeltaAnalysis logical resolution rule is executed Utilities \u00b6 apply \u00b6 apply ( target : LogicalPlan , source : LogicalPlan , condition : Expression , whenClauses : Seq [ DeltaMergeIntoClause ]) : DeltaMergeInto apply ...FIXME apply is used when: DeltaMergeBuilder is requested to execute (when mergePlan ) DeltaAnalysis logical resolution rule is executed (and resolves MergeIntoTable logical command) resolveReferences \u00b6 resolveReferences ( merge : DeltaMergeInto , conf : SQLConf )( resolveExpr : ( Expression , LogicalPlan ) => Expression ) : DeltaMergeInto resolveReferences ...FIXME resolveReferences is used when: DeltaMergeBuilder is requested to execute DeltaAnalysis logical resolution rule is executed (and resolves MergeIntoTable logical command)","title":"DeltaMergeInto"},{"location":"commands/DeltaMergeInto/#deltamergeinto-logical-command","text":"DeltaMergeInto is a logical command (Spark SQL's Command ).","title":"DeltaMergeInto Logical Command"},{"location":"commands/DeltaMergeInto/#creating-instance","text":"DeltaMergeInto takes the following to be created: Target LogicalPlan Source LogicalPlan Condition Expression Matched Clauses ( Seq[DeltaMergeIntoMatchedClause] ) Optional Non-Matched Clause ( Option[DeltaMergeIntoInsertClause] ) Optional Migrated Schema (default: undefined ) DeltaMergeInto is created (using apply and resolveReferences utilities) when: DeltaMergeBuilder is requested to execute DeltaAnalysis logical resolution rule is executed","title":"Creating Instance"},{"location":"commands/DeltaMergeInto/#utilities","text":"","title":"Utilities"},{"location":"commands/DeltaMergeInto/#apply","text":"apply ( target : LogicalPlan , source : LogicalPlan , condition : Expression , whenClauses : Seq [ DeltaMergeIntoClause ]) : DeltaMergeInto apply ...FIXME apply is used when: DeltaMergeBuilder is requested to execute (when mergePlan ) DeltaAnalysis logical resolution rule is executed (and resolves MergeIntoTable logical command)","title":" apply"},{"location":"commands/DeltaMergeInto/#resolvereferences","text":"resolveReferences ( merge : DeltaMergeInto , conf : SQLConf )( resolveExpr : ( Expression , LogicalPlan ) => Expression ) : DeltaMergeInto resolveReferences ...FIXME resolveReferences is used when: DeltaMergeBuilder is requested to execute DeltaAnalysis logical resolution rule is executed (and resolves MergeIntoTable logical command)","title":" resolveReferences"},{"location":"commands/DeltaMergeIntoClause/","text":"DeltaMergeIntoClause \u00b6 DeltaMergeIntoClause is...FIXME","title":"DeltaMergeIntoClause"},{"location":"commands/DeltaMergeIntoClause/#deltamergeintoclause","text":"DeltaMergeIntoClause is...FIXME","title":"DeltaMergeIntoClause"},{"location":"commands/DeltaMergeMatchedActionBuilder/","text":"DeltaMergeMatchedActionBuilder \u00b6 DeltaMergeMatchedActionBuilder is a builder interface for DeltaMergeBuilder.whenMatched operator. Creating Instance \u00b6 DeltaMergeMatchedActionBuilder takes the following to be created: DeltaMergeBuilder Optional match condition DeltaMergeMatchedActionBuilder is created when DeltaMergeBuilder is requested to whenMatched (using apply factory method). Operators \u00b6 delete \u00b6 delete () : DeltaMergeBuilder Adds a DeltaMergeIntoDeleteClause (with the matchCondition ) to the DeltaMergeBuilder . update \u00b6 update ( set : Map [ String , Column ]) : DeltaMergeBuilder updateAll \u00b6 updateAll () : DeltaMergeBuilder updateExpr \u00b6 updateExpr ( set : Map [ String , String ]) : DeltaMergeBuilder Creating DeltaMergeMatchedActionBuilder \u00b6 apply ( mergeBuilder : DeltaMergeBuilder , matchCondition : Option [ Column ]) : DeltaMergeMatchedActionBuilder apply creates a DeltaMergeMatchedActionBuilder (for the given parameters). apply is used when DeltaMergeBuilder is requested to whenMatched .","title":"DeltaMergeMatchedActionBuilder"},{"location":"commands/DeltaMergeMatchedActionBuilder/#deltamergematchedactionbuilder","text":"DeltaMergeMatchedActionBuilder is a builder interface for DeltaMergeBuilder.whenMatched operator.","title":"DeltaMergeMatchedActionBuilder"},{"location":"commands/DeltaMergeMatchedActionBuilder/#creating-instance","text":"DeltaMergeMatchedActionBuilder takes the following to be created: DeltaMergeBuilder Optional match condition DeltaMergeMatchedActionBuilder is created when DeltaMergeBuilder is requested to whenMatched (using apply factory method).","title":"Creating Instance"},{"location":"commands/DeltaMergeMatchedActionBuilder/#operators","text":"","title":"Operators"},{"location":"commands/DeltaMergeMatchedActionBuilder/#delete","text":"delete () : DeltaMergeBuilder Adds a DeltaMergeIntoDeleteClause (with the matchCondition ) to the DeltaMergeBuilder .","title":" delete"},{"location":"commands/DeltaMergeMatchedActionBuilder/#update","text":"update ( set : Map [ String , Column ]) : DeltaMergeBuilder","title":" update"},{"location":"commands/DeltaMergeMatchedActionBuilder/#updateall","text":"updateAll () : DeltaMergeBuilder","title":" updateAll"},{"location":"commands/DeltaMergeMatchedActionBuilder/#updateexpr","text":"updateExpr ( set : Map [ String , String ]) : DeltaMergeBuilder","title":" updateExpr"},{"location":"commands/DeltaMergeMatchedActionBuilder/#creating-deltamergematchedactionbuilder","text":"apply ( mergeBuilder : DeltaMergeBuilder , matchCondition : Option [ Column ]) : DeltaMergeMatchedActionBuilder apply creates a DeltaMergeMatchedActionBuilder (for the given parameters). apply is used when DeltaMergeBuilder is requested to whenMatched .","title":" Creating DeltaMergeMatchedActionBuilder"},{"location":"commands/DeltaMergeNotMatchedActionBuilder/","text":"DeltaMergeNotMatchedActionBuilder \u00b6 DeltaMergeNotMatchedActionBuilder is...FIXME","title":"DeltaMergeNotMatchedActionBuilder"},{"location":"commands/DeltaMergeNotMatchedActionBuilder/#deltamergenotmatchedactionbuilder","text":"DeltaMergeNotMatchedActionBuilder is...FIXME","title":"DeltaMergeNotMatchedActionBuilder"},{"location":"commands/DescribeDeltaDetailCommand/","text":"= DescribeDeltaDetailCommand (And DescribeDeltaDetailCommandBase) DescribeDeltaDetailCommand represents DESCRIBE DETAIL SQL command at execution (and is < > when DeltaSqlAstBuilder is requested to < >). Like DESCRIBE DETAIL SQL command, DescribeDeltaDetailCommand accepts either a < > or a < > (e.g. '/tmp/delta/t1' or ++delta. /tmp/delta/t1 ++ ) (DESC | DESCRIBE) DETAIL (path | table) [[demo]] .DESCRIBE DETAIL SQL Command's Demo val q = sql(\"DESCRIBE DETAIL '/tmp/delta/users'\") scala> q.show +------+--------------------+----+-----------+--------------------+--------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+ |format| id|name|description| location| createdAt| lastModified|partitionColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion| +------+--------------------+----+-----------+--------------------+--------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+ | delta|3799b291-dbfa-4f8...|null| null|file:/tmp/delta/u...|2020-01-06 17:08:...|2020-01-06 17:12:28| [city, country]| 4| 2581| []| 1| 2| +------+--------------------+----+-----------+--------------------+--------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+ [[implementations]] DescribeDeltaDetailCommand is the default and only known < > in Delta Lake. [[DescribeDeltaDetailCommandBase]] DescribeDeltaDetailCommandBase is an extension of the RunnableCommand contract (from Spark SQL) for < >. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan-RunnableCommand.html[RunnableCommand ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. == [[creating-instance]] Creating DescribeDeltaDetailCommand Instance DescribeDeltaDetailCommand takes the following to be created: [[path]] Table path [[tableIdentifier]] Table identifier (e.g. t1 or ++delta. /tmp/delta/t1 ++ ) == [[run]] Running Command -- run Method [source, scala] \u00b6 run(sparkSession: SparkSession): Seq[Row] \u00b6 NOTE: run is part of the RunnableCommand contract to...FIXME. run ...FIXME","title":"DescribeDeltaDetailCommand"},{"location":"commands/DescribeDeltaDetailCommand/#source-scala","text":"","title":"[source, scala]"},{"location":"commands/DescribeDeltaDetailCommand/#runsparksession-sparksession-seqrow","text":"NOTE: run is part of the RunnableCommand contract to...FIXME. run ...FIXME","title":"run(sparkSession: SparkSession): Seq[Row]"},{"location":"commands/DescribeDeltaHistoryCommand/","text":"= DescribeDeltaHistoryCommand DescribeDeltaHistoryCommand is...FIXME","title":"DescribeDeltaHistoryCommand"},{"location":"commands/MergeIntoCommand/","text":"MergeIntoCommand \u00b6 MergeIntoCommand is a DeltaCommand that represents a DeltaMergeInto logical command. MergeIntoCommand is a logical command (Spark SQL's RunnableCommand ). Performance Metrics \u00b6 Name web UI numSourceRows number of source rows numTargetRowsCopied number of target rows rewritten unmodified numTargetRowsInserted number of inserted rows numTargetRowsUpdated number of updated rows numTargetRowsDeleted number of deleted rows numTargetFilesBeforeSkipping number of target files before skipping numTargetFilesAfterSkipping number of target files after skipping numTargetFilesRemoved number of files removed to target numTargetFilesAdded number of files added to target Creating Instance \u00b6 MergeIntoCommand takes the following to be created: Source Data ( LogicalPlan ) Target Data ( LogicalPlan ) TahoeFileIndex Condition Expression Matched Clauses ( Seq[DeltaMergeIntoMatchedClause] ) Optional Non-Matched Clause ( Option[DeltaMergeIntoInsertClause] ) Migrated Schema MergeIntoCommand is created when PreprocessTableMerge logical resolution rule is executed (on a DeltaMergeInto logical command). Executing Command \u00b6 run ( spark : SparkSession ) : Seq [ Row ] run requests the target DeltaLog to start a new transaction . With spark.databricks.delta.schema.autoMerge.enabled configuration property enabled, run updates the metadata (of the transaction). run determines Delta actions ( RemoveFile s and AddFile s). Describe deltaActions part With spark.databricks.delta.history.metricsEnabled configuration property enabled, run requests the current transaction to register SQL metrics for the Delta operation . run requests the current transaction to commit (with the Delta actions and Merge operation). run records the Delta event. run posts a SparkListenerDriverAccumUpdates Spark event (with the metrics). In the end, run requests the CacheManager to recacheByPlan . run is part of the RunnableCommand ( Spark SQL ) abstraction. Exceptions \u00b6 run throws an AnalysisException when the target schema is different than the delta table's (has changed after analysis phase): The schema of your Delta table has changed in an incompatible way since your DataFrame or DeltaTable object was created. Please redefine your DataFrame or DeltaTable object. Changes: [schemaDiff] This check can be turned off by setting the session configuration key spark.databricks.delta.checkLatestSchemaOnRead to false. writeAllChanges \u00b6 writeAllChanges ( spark : SparkSession , deltaTxn : OptimisticTransaction , filesToRewrite : Seq [ AddFile ]) : Seq [ AddFile ] writeAllChanges builds the target output columns (possibly with some null s for the target columns that are not in the current schema). writeAllChanges buildTargetPlanWithFiles . writeAllChanges determines a join type to use ( rightOuter or fullOuter ). writeAllChanges prints out the following DEBUG message to the logs: writeAllChanges using [joinType] join: source.output: [outputSet] target.output: [outputSet] condition: [condition] newTarget.output: [outputSet] writeAllChanges creates a joinedDF DataFrame that is a join of the DataFrames for the source and the new target logical plans with the given join condition and the join type . writeAllChanges creates a JoinedRowProcessor that is then used to map over partitions of the joined DataFrame . writeAllChanges prints out the following DEBUG message to the logs: writeAllChanges: join output plan: [outputDF.queryExecution] writeAllChanges requests the input OptimisticTransaction to writeFiles (possibly repartitioning by the partition columns if table is partitioned and spark.databricks.delta.merge.repartitionBeforeWrite.enabled configuration property is enabled). writeAllChanges is used when MergeIntoCommand is requested to run . findTouchedFiles \u00b6 findTouchedFiles ( deltaTxn : OptimisticTransaction , files : Seq [ AddFile ]) : LogicalPlan findTouchedFiles ...FIXME findTouchedFiles is used when MergeIntoCommand is requested to run . buildTargetPlanWithFiles \u00b6 buildTargetPlanWithFiles ( deltaTxn : OptimisticTransaction , files : Seq [ AddFile ]) : LogicalPlan buildTargetPlanWithFiles ...FIXME buildTargetPlanWithFiles is used when MergeIntoCommand is requested to run (via findTouchedFiles and writeAllChanges ).","title":"MergeIntoCommand"},{"location":"commands/MergeIntoCommand/#mergeintocommand","text":"MergeIntoCommand is a DeltaCommand that represents a DeltaMergeInto logical command. MergeIntoCommand is a logical command (Spark SQL's RunnableCommand ).","title":"MergeIntoCommand"},{"location":"commands/MergeIntoCommand/#performance-metrics","text":"Name web UI numSourceRows number of source rows numTargetRowsCopied number of target rows rewritten unmodified numTargetRowsInserted number of inserted rows numTargetRowsUpdated number of updated rows numTargetRowsDeleted number of deleted rows numTargetFilesBeforeSkipping number of target files before skipping numTargetFilesAfterSkipping number of target files after skipping numTargetFilesRemoved number of files removed to target numTargetFilesAdded number of files added to target","title":"Performance Metrics"},{"location":"commands/MergeIntoCommand/#creating-instance","text":"MergeIntoCommand takes the following to be created: Source Data ( LogicalPlan ) Target Data ( LogicalPlan ) TahoeFileIndex Condition Expression Matched Clauses ( Seq[DeltaMergeIntoMatchedClause] ) Optional Non-Matched Clause ( Option[DeltaMergeIntoInsertClause] ) Migrated Schema MergeIntoCommand is created when PreprocessTableMerge logical resolution rule is executed (on a DeltaMergeInto logical command).","title":"Creating Instance"},{"location":"commands/MergeIntoCommand/#executing-command","text":"run ( spark : SparkSession ) : Seq [ Row ] run requests the target DeltaLog to start a new transaction . With spark.databricks.delta.schema.autoMerge.enabled configuration property enabled, run updates the metadata (of the transaction). run determines Delta actions ( RemoveFile s and AddFile s). Describe deltaActions part With spark.databricks.delta.history.metricsEnabled configuration property enabled, run requests the current transaction to register SQL metrics for the Delta operation . run requests the current transaction to commit (with the Delta actions and Merge operation). run records the Delta event. run posts a SparkListenerDriverAccumUpdates Spark event (with the metrics). In the end, run requests the CacheManager to recacheByPlan . run is part of the RunnableCommand ( Spark SQL ) abstraction.","title":" Executing Command"},{"location":"commands/MergeIntoCommand/#exceptions","text":"run throws an AnalysisException when the target schema is different than the delta table's (has changed after analysis phase): The schema of your Delta table has changed in an incompatible way since your DataFrame or DeltaTable object was created. Please redefine your DataFrame or DeltaTable object. Changes: [schemaDiff] This check can be turned off by setting the session configuration key spark.databricks.delta.checkLatestSchemaOnRead to false.","title":" Exceptions"},{"location":"commands/MergeIntoCommand/#writeallchanges","text":"writeAllChanges ( spark : SparkSession , deltaTxn : OptimisticTransaction , filesToRewrite : Seq [ AddFile ]) : Seq [ AddFile ] writeAllChanges builds the target output columns (possibly with some null s for the target columns that are not in the current schema). writeAllChanges buildTargetPlanWithFiles . writeAllChanges determines a join type to use ( rightOuter or fullOuter ). writeAllChanges prints out the following DEBUG message to the logs: writeAllChanges using [joinType] join: source.output: [outputSet] target.output: [outputSet] condition: [condition] newTarget.output: [outputSet] writeAllChanges creates a joinedDF DataFrame that is a join of the DataFrames for the source and the new target logical plans with the given join condition and the join type . writeAllChanges creates a JoinedRowProcessor that is then used to map over partitions of the joined DataFrame . writeAllChanges prints out the following DEBUG message to the logs: writeAllChanges: join output plan: [outputDF.queryExecution] writeAllChanges requests the input OptimisticTransaction to writeFiles (possibly repartitioning by the partition columns if table is partitioned and spark.databricks.delta.merge.repartitionBeforeWrite.enabled configuration property is enabled). writeAllChanges is used when MergeIntoCommand is requested to run .","title":" writeAllChanges"},{"location":"commands/MergeIntoCommand/#findtouchedfiles","text":"findTouchedFiles ( deltaTxn : OptimisticTransaction , files : Seq [ AddFile ]) : LogicalPlan findTouchedFiles ...FIXME findTouchedFiles is used when MergeIntoCommand is requested to run .","title":" findTouchedFiles"},{"location":"commands/MergeIntoCommand/#buildtargetplanwithfiles","text":"buildTargetPlanWithFiles ( deltaTxn : OptimisticTransaction , files : Seq [ AddFile ]) : LogicalPlan buildTargetPlanWithFiles ...FIXME buildTargetPlanWithFiles is used when MergeIntoCommand is requested to run (via findTouchedFiles and writeAllChanges ).","title":" buildTargetPlanWithFiles"},{"location":"commands/UpdateCommand/","text":"UpdateCommand \u00b6 UpdateCommand is...FIXME == [[run]] Running Command -- run Method [source, scala] \u00b6 run(sparkSession: SparkSession): Seq[Row] \u00b6 NOTE: run is part of the RunnableCommand contract to...FIXME. run ...FIXME == [[performUpdate]] performUpdate Internal Method [source, scala] \u00b6 performUpdate( sparkSession: SparkSession, deltaLog: DeltaLog, txn: OptimisticTransaction): Unit performUpdate ...FIXME NOTE: performUpdate is used exclusively when UpdateCommand is requested to < >. == [[rewriteFiles]] rewriteFiles Internal Method [source, scala] \u00b6 rewriteFiles( spark: SparkSession, txn: OptimisticTransaction, rootPath: Path, inputLeafFiles: Seq[String], nameToAddFileMap: Map[String, AddFile], condition: Expression): Seq[AddFile] rewriteFiles ...FIXME NOTE: rewriteFiles is used exclusively when UpdateCommand is requested to < >.","title":"UpdateCommand"},{"location":"commands/UpdateCommand/#updatecommand","text":"UpdateCommand is...FIXME == [[run]] Running Command -- run Method","title":"UpdateCommand"},{"location":"commands/UpdateCommand/#source-scala","text":"","title":"[source, scala]"},{"location":"commands/UpdateCommand/#runsparksession-sparksession-seqrow","text":"NOTE: run is part of the RunnableCommand contract to...FIXME. run ...FIXME == [[performUpdate]] performUpdate Internal Method","title":"run(sparkSession: SparkSession): Seq[Row]"},{"location":"commands/UpdateCommand/#source-scala_1","text":"performUpdate( sparkSession: SparkSession, deltaLog: DeltaLog, txn: OptimisticTransaction): Unit performUpdate ...FIXME NOTE: performUpdate is used exclusively when UpdateCommand is requested to < >. == [[rewriteFiles]] rewriteFiles Internal Method","title":"[source, scala]"},{"location":"commands/UpdateCommand/#source-scala_2","text":"rewriteFiles( spark: SparkSession, txn: OptimisticTransaction, rootPath: Path, inputLeafFiles: Seq[String], nameToAddFileMap: Map[String, AddFile], condition: Expression): Seq[AddFile] rewriteFiles ...FIXME NOTE: rewriteFiles is used exclusively when UpdateCommand is requested to < >.","title":"[source, scala]"},{"location":"commands/VacuumCommand/","text":"VacuumCommand Utility \u2014 Garbage Collecting Delta Table \u00b6 VacuumCommand is a concrete VacuumCommandImpl for gc . Garbage Collecting Of Delta Table \u00b6 gc ( spark : SparkSession , deltaLog : DeltaLog , dryRun : Boolean = true , retentionHours : Option [ Double ] = None , clock : Clock = new SystemClock ) : DataFrame gc requests the given DeltaLog to < > (and give the latest < > of the delta table). [[gc-deleteBeforeTimestamp]] gc...FIXME (deleteBeforeTimestamp) gc prints out the following INFO message to the logs: Starting garbage collection (dryRun = [dryRun]) of untracked files older than [deleteBeforeTimestamp] in [path] [[gc-validFiles]] gc requests the Snapshot for the < > and defines a function for every action (in a partition) that does the following: . FIXME gc converts the mapped state dataset (of actions) into a DataFrame with a single path column. [[gc-allFilesAndDirs]] gc...FIXME gc caches the < > dataset. gc prints out the following INFO message to the logs: Deleting untracked files and empty directories in [path] gc...FIXME gc prints out the following message to standard output: Deleted [filesDeleted] files and directories in a total of [dirCounts] directories. gc...FIXME In the end, gc unpersists the < > dataset. [NOTE] \u00b6 gc is used when: DeltaTableOperations is requested to < > (for < > operator) * < > is executed (for delta-sql.md#VACUUM[VACUUM] SQL command) \u00b6 == [[checkRetentionPeriodSafety]] checkRetentionPeriodSafety Method [source, scala] \u00b6 checkRetentionPeriodSafety( spark: SparkSession, retentionMs: Option[Long], configuredRetention: Long): Unit checkRetentionPeriodSafety ...FIXME NOTE: checkRetentionPeriodSafety is used exclusively when VacuumCommand utility is requested to < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.sql.delta.commands.VacuumCommand logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.sql.delta.commands.VacuumCommand=ALL \u00b6 Refer to Logging .","title":"VacuumCommand"},{"location":"commands/VacuumCommand/#vacuumcommand-utility-garbage-collecting-delta-table","text":"VacuumCommand is a concrete VacuumCommandImpl for gc .","title":"VacuumCommand Utility &mdash; Garbage Collecting Delta Table"},{"location":"commands/VacuumCommand/#garbage-collecting-of-delta-table","text":"gc ( spark : SparkSession , deltaLog : DeltaLog , dryRun : Boolean = true , retentionHours : Option [ Double ] = None , clock : Clock = new SystemClock ) : DataFrame gc requests the given DeltaLog to < > (and give the latest < > of the delta table). [[gc-deleteBeforeTimestamp]] gc...FIXME (deleteBeforeTimestamp) gc prints out the following INFO message to the logs: Starting garbage collection (dryRun = [dryRun]) of untracked files older than [deleteBeforeTimestamp] in [path] [[gc-validFiles]] gc requests the Snapshot for the < > and defines a function for every action (in a partition) that does the following: . FIXME gc converts the mapped state dataset (of actions) into a DataFrame with a single path column. [[gc-allFilesAndDirs]] gc...FIXME gc caches the < > dataset. gc prints out the following INFO message to the logs: Deleting untracked files and empty directories in [path] gc...FIXME gc prints out the following message to standard output: Deleted [filesDeleted] files and directories in a total of [dirCounts] directories. gc...FIXME In the end, gc unpersists the < > dataset.","title":" Garbage Collecting Of Delta Table"},{"location":"commands/VacuumCommand/#note","text":"gc is used when: DeltaTableOperations is requested to < > (for < > operator)","title":"[NOTE]"},{"location":"commands/VacuumCommand/#is-executed-for-delta-sqlmdvacuumvacuum-sql-command","text":"== [[checkRetentionPeriodSafety]] checkRetentionPeriodSafety Method","title":"* &lt;&gt; is executed (for delta-sql.md#VACUUM[VACUUM] SQL command)"},{"location":"commands/VacuumCommand/#source-scala","text":"checkRetentionPeriodSafety( spark: SparkSession, retentionMs: Option[Long], configuredRetention: Long): Unit checkRetentionPeriodSafety ...FIXME NOTE: checkRetentionPeriodSafety is used exclusively when VacuumCommand utility is requested to < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.sql.delta.commands.VacuumCommand logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, scala]"},{"location":"commands/VacuumCommand/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"commands/VacuumCommand/#log4jloggerorgapachesparksqldeltacommandsvacuumcommandall","text":"Refer to Logging .","title":"log4j.logger.org.apache.spark.sql.delta.commands.VacuumCommand=ALL"},{"location":"commands/VacuumCommandImpl/","text":"VacuumCommandImpl \u00b6 VacuumCommandImpl is...FIXME == [[delete]] delete Method [source,scala] \u00b6 delete( diff: Dataset[String], fs: FileSystem): Long delete...FIXME delete is used when VacuumCommand utility is requested to VacuumCommand.md#gc[gc]","title":"VacuumCommandImpl"},{"location":"commands/VacuumCommandImpl/#vacuumcommandimpl","text":"VacuumCommandImpl is...FIXME == [[delete]] delete Method","title":"VacuumCommandImpl"},{"location":"commands/VacuumCommandImpl/#sourcescala","text":"delete( diff: Dataset[String], fs: FileSystem): Long delete...FIXME delete is used when VacuumCommand utility is requested to VacuumCommand.md#gc[gc]","title":"[source,scala]"},{"location":"commands/VacuumTableCommand/","text":"= VacuumTableCommand VacuumTableCommand is a logical command ( RunnableCommand ) for delta-sql.md#VACUUM[VACUUM] SQL command. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan-RunnableCommand.html[RunnableCommand ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. VacuumTableCommand is < > exclusively when DeltaSqlAstBuilder is requested to < >. VacuumTableCommand < > that either the < > or the < > is defined and it is the root directory of a delta table. Partition directories are not supported. [[output]] The output of VacuumTableCommand is a single path column (of type StringType ). == [[creating-instance]] Creating VacuumTableCommand Instance VacuumTableCommand takes the following to be created: [[path]] Path (optional) [[table]] TableIdentifier (optional) [[horizonHours]] Optional horizonHours [[dryRun]] dryRun flag == [[run]] Running Command -- run Method [source, scala] \u00b6 run(sparkSession: SparkSession): Seq[Row] \u00b6 NOTE: run is part of the RunnableCommand contract to...FIXME. run takes the path to vacuum (i.e. either the < > or the < >) and < >. run < > for the delta table and executes < > utility (passing in the DeltaLog instance, the < > and the < > options). run throws an AnalysisException when executed for a non-root directory of a delta table: Please provide the base path ([baseDeltaPath]) when Vacuuming Delta tables. Vacuuming specific partitions is currently not supported. run throws an AnalysisException when executed for a DeltaLog with the snapshot version being -1 : [deltaTableIdentifier] is not a Delta table. VACUUM is only supported for Delta tables.","title":"VacuumTableCommand"},{"location":"commands/VacuumTableCommand/#source-scala","text":"","title":"[source, scala]"},{"location":"commands/VacuumTableCommand/#runsparksession-sparksession-seqrow","text":"NOTE: run is part of the RunnableCommand contract to...FIXME. run takes the path to vacuum (i.e. either the < > or the < >) and < >. run < > for the delta table and executes < > utility (passing in the DeltaLog instance, the < > and the < > options). run throws an AnalysisException when executed for a non-root directory of a delta table: Please provide the base path ([baseDeltaPath]) when Vacuuming Delta tables. Vacuuming specific partitions is currently not supported. run throws an AnalysisException when executed for a DeltaLog with the snapshot version being -1 : [deltaTableIdentifier] is not a Delta table. VACUUM is only supported for Delta tables.","title":"run(sparkSession: SparkSession): Seq[Row]"},{"location":"commands/WriteIntoDelta/","text":"WriteIntoDelta Command \u00b6 WriteIntoDelta is a < > that can write < > transactionally into a < >. [[demo]] .Demo [source, scala] import org.apache.spark.sql.delta.commands.WriteIntoDelta import org.apache.spark.sql.delta.DeltaLog import org.apache.spark.sql.SaveMode import org.apache.spark.sql.delta.DeltaOptions val tableName = \"/tmp/delta/t1\" val data = spark.range(5).toDF val writeCmd = WriteIntoDelta( deltaLog = DeltaLog.forTable(spark, tableName), mode = SaveMode.Overwrite, options = new DeltaOptions(Map.empty[String, String], spark.sessionState.conf), partitionColumns = Seq.empty[String], configuration = Map.empty[String, String], data) // Review web UI @ http://localhost:4040 writeCmd.run(spark) \u00b6 [[ImplicitMetadataOperation]] WriteIntoDelta is an < > of a < >. [[RunnableCommand]] WriteIntoDelta is a logical command ( RunnableCommand ). TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan-RunnableCommand.html[RunnableCommand ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. WriteIntoDelta is < > when: DeltaLog is requested to < > (when DeltaDataSource is requested to create a relation as a < > or a < >) DeltaDataSource is requested to < > (as a < >) == [[creating-instance]] Creating WriteIntoDelta Instance WriteIntoDelta takes the following to be created: [[deltaLog]] < > [[mode]] SaveMode [[options]] < > [[partitionColumns]] Names of the partition columns ( Seq[String] ) [[configuration]] Configuration ( Map[String, String] ) [[data]] Data ( DataFrame ) == [[run]] Running Command -- run Method [source, scala] \u00b6 run( sparkSession: SparkSession): Seq[Row] NOTE: run is part of the RunnableCommand contract to run a command. run requests the < > to < >. run < > and requests the OptimisticTransaction to < >. == [[write]] write Method [source, scala] \u00b6 write( txn: OptimisticTransaction, sparkSession: SparkSession): Seq[Action] write checks out whether the write operation is to a delta table that already exists. If so (i.e. the < > of the transaction is above -1 ), write branches per the < >: For ErrorIfExists , write throws an AnalysisException : + [path] already exists. For Ignore , write does nothing For Overwrite , write requests the < > to < > write < >. write ...FIXME NOTE: write is used exclusively when WriteIntoDelta is requested to < >.","title":"WriteIntoDelta"},{"location":"commands/WriteIntoDelta/#writeintodelta-command","text":"WriteIntoDelta is a < > that can write < > transactionally into a < >. [[demo]] .Demo [source, scala] import org.apache.spark.sql.delta.commands.WriteIntoDelta import org.apache.spark.sql.delta.DeltaLog import org.apache.spark.sql.SaveMode import org.apache.spark.sql.delta.DeltaOptions val tableName = \"/tmp/delta/t1\" val data = spark.range(5).toDF val writeCmd = WriteIntoDelta( deltaLog = DeltaLog.forTable(spark, tableName), mode = SaveMode.Overwrite, options = new DeltaOptions(Map.empty[String, String], spark.sessionState.conf), partitionColumns = Seq.empty[String], configuration = Map.empty[String, String], data) // Review web UI @ http://localhost:4040","title":"WriteIntoDelta Command"},{"location":"commands/WriteIntoDelta/#writecmdrunspark","text":"[[ImplicitMetadataOperation]] WriteIntoDelta is an < > of a < >. [[RunnableCommand]] WriteIntoDelta is a logical command ( RunnableCommand ). TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan-RunnableCommand.html[RunnableCommand ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. WriteIntoDelta is < > when: DeltaLog is requested to < > (when DeltaDataSource is requested to create a relation as a < > or a < >) DeltaDataSource is requested to < > (as a < >) == [[creating-instance]] Creating WriteIntoDelta Instance WriteIntoDelta takes the following to be created: [[deltaLog]] < > [[mode]] SaveMode [[options]] < > [[partitionColumns]] Names of the partition columns ( Seq[String] ) [[configuration]] Configuration ( Map[String, String] ) [[data]] Data ( DataFrame ) == [[run]] Running Command -- run Method","title":"writeCmd.run(spark)"},{"location":"commands/WriteIntoDelta/#source-scala","text":"run( sparkSession: SparkSession): Seq[Row] NOTE: run is part of the RunnableCommand contract to run a command. run requests the < > to < >. run < > and requests the OptimisticTransaction to < >. == [[write]] write Method","title":"[source, scala]"},{"location":"commands/WriteIntoDelta/#source-scala_1","text":"write( txn: OptimisticTransaction, sparkSession: SparkSession): Seq[Action] write checks out whether the write operation is to a delta table that already exists. If so (i.e. the < > of the transaction is above -1 ), write branches per the < >: For ErrorIfExists , write throws an AnalysisException : + [path] already exists. For Ignore , write does nothing For Overwrite , write requests the < > to < > write < >. write ...FIXME NOTE: write is used exclusively when WriteIntoDelta is requested to < >.","title":"[source, scala]"},{"location":"commands/vacuum/","text":"= Vacuum Command Vacuum command does...FIXME (see < >) Vacuum command can be executed as delta-sql.md#VACUUM[VACUUM] SQL command or < > operator. /* spark-shell \\ --packages io.delta:delta-core_2.12:0.7.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension */ scala> sql(\"VACUUM delta.`/tmp/delta/t1`\").show Deleted 0 files and directories in a total of 2 directories. +------------------+ | path| +------------------+ |file:/tmp/delta/t1| +------------------+","title":"Vacuum"},{"location":"demo/Converting-Parquet-Dataset-Into-Delta-Format/","text":"Demo: Converting Parquet Dataset Into Delta Format \u00b6 /* spark-shell \\ --packages io.delta:delta-core_2.12:0.7.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.databricks.delta.snapshotPartitions=1 */ assert(spark.version.matches(\"2.4.[2-6]\"), \"Delta Lake supports Spark 2.4.2+\") import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) val deltaLake = \"/tmp/delta\" // Create parquet table val users = s\"$deltaLake/users\" import spark.implicits._ val data = Seq( (0L, \"Agata\", \"Warsaw\", \"Poland\"), (1L, \"Jacek\", \"Warsaw\", \"Poland\"), (2L, \"Bartosz\", \"Paris\", \"France\") ).toDF(\"id\", \"name\", \"city\", \"country\") data .write .format(\"parquet\") .partitionBy(\"city\", \"country\") .mode(\"overwrite\") .save(users) // TIP: Use git to version the users directory // to track the changes for import // CONVERT TO DELTA only supports parquet tables // TableIdentifier should be parquet.`users` // Use TableIdentifier to refer to the parquet table // The path itself would work too val tableId = s\"parquet.`$users`\" val partitionSchema = \"city STRING, country STRING\" // Import users table into Delta Lake // Well, convert the parquet table into delta table // Use web UI to monitor execution, e.g. http://localhost:4040 import io.delta.tables.DeltaTable val dt = DeltaTable.convertToDelta(spark, tableId, partitionSchema) assert(dt.isInstanceOf[DeltaTable]) // users table is now in delta format assert(DeltaTable.isDeltaTable(users))","title":"Converting Parquet Dataset Into Delta Format"},{"location":"demo/Converting-Parquet-Dataset-Into-Delta-Format/#demo-converting-parquet-dataset-into-delta-format","text":"/* spark-shell \\ --packages io.delta:delta-core_2.12:0.7.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.databricks.delta.snapshotPartitions=1 */ assert(spark.version.matches(\"2.4.[2-6]\"), \"Delta Lake supports Spark 2.4.2+\") import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) val deltaLake = \"/tmp/delta\" // Create parquet table val users = s\"$deltaLake/users\" import spark.implicits._ val data = Seq( (0L, \"Agata\", \"Warsaw\", \"Poland\"), (1L, \"Jacek\", \"Warsaw\", \"Poland\"), (2L, \"Bartosz\", \"Paris\", \"France\") ).toDF(\"id\", \"name\", \"city\", \"country\") data .write .format(\"parquet\") .partitionBy(\"city\", \"country\") .mode(\"overwrite\") .save(users) // TIP: Use git to version the users directory // to track the changes for import // CONVERT TO DELTA only supports parquet tables // TableIdentifier should be parquet.`users` // Use TableIdentifier to refer to the parquet table // The path itself would work too val tableId = s\"parquet.`$users`\" val partitionSchema = \"city STRING, country STRING\" // Import users table into Delta Lake // Well, convert the parquet table into delta table // Use web UI to monitor execution, e.g. http://localhost:4040 import io.delta.tables.DeltaTable val dt = DeltaTable.convertToDelta(spark, tableId, partitionSchema) assert(dt.isInstanceOf[DeltaTable]) // users table is now in delta format assert(DeltaTable.isDeltaTable(users))","title":"Demo: Converting Parquet Dataset Into Delta Format"},{"location":"demo/Debugging-Delta-Lake-Using-IntelliJ-IDEA/","text":"= Demo: Debugging Delta Lake Using IntelliJ IDEA Import Delta Lake's https://github.com/delta-io/delta[sources ] to IntelliJ IDEA. Configure a new Remote debug configuration in IntelliJ IDEA (e.g. Run > Debug > Edit Configurations...) and simply give it a name and save. TIP: Use Option+Ctrl+D to access Debug menu. .Remote JVM Configuration image::demo-remote-jvm.png[align=\"center\"] Run spark-shell as follows to enable remote JVM for debugging. [source] \u00b6 $ export SPARK_SUBMIT_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 $ spark-shell \\ --packages io.delta:delta-core_2.12:0.7.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.databricks.delta.snapshotPartitions=1","title":"Debugging Delta Lake Using IntelliJ IDEA"},{"location":"demo/Debugging-Delta-Lake-Using-IntelliJ-IDEA/#source","text":"$ export SPARK_SUBMIT_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 $ spark-shell \\ --packages io.delta:delta-core_2.12:0.7.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.databricks.delta.snapshotPartitions=1","title":"[source]"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/","text":"= Demo: DeltaTable, DeltaLog And Snapshots NOTE: Use Converting-Parquet-Dataset-Into-Delta-Format.md[Demo: Converting Parquet Dataset Into Delta Format] to create the delta table for the demo. [source, scala] \u00b6 import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) val dataPath = \"/tmp/delta/users\" import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, dataPath) // Review the cached RDD for the state (snapshot) // Use http://localhost:4040/storage/ assert(deltaLog.snapshot.version == 0) // Show the changes (actions) scala> deltaLog.snapshot.state.toDF.show +----+--------------------+------+--------------------+--------+----------+ | txn| add|remove| metaData|protocol|commitInfo| +----+--------------------+------+--------------------+--------+----------+ |null| null| null| null| [1, 2]| null| |null| null| null|[3799b291-dbfa-4f...| null| null| |null|[city=Paris/count...| null| null| null| null| |null|[city=Warsaw/coun...| null| null| null| null| |null|[city=Warsaw/coun...| null| null| null| null| +----+--------------------+------+--------------------+--------+----------+ val dt = DeltaTable.forPath(deltaLog.dataPath.toString) scala> dt.history.show +-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+ |version| timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend| +-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+ | 0|2020-01-06 17:08:02| null| null| CONVERT|[numFiles -> 3, p...|null| null| null| null| null| null| +-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+ // Show the data (for the changes) val users = dt.toDF scala> users.show +---+-------+------+-------+ | id| name| city|country| +---+-------+------+-------+ | 2|Bartosz| Paris| France| | 0| Agata|Warsaw| Poland| | 1| Jacek|Warsaw| Poland| +---+-------+------+-------+ // Add a new user val loic = Seq((3L, \"Loic\", \"Paris\", \"France\")).toDF(\"id\", \"name\", \"city\", \"country\") scala> loic.show +---+----+-----+-------+ | id|name| city|country| +---+----+-----+-------+ | 3|Loic|Paris| France| +---+----+-----+-------+ loic.write.format(\"delta\").mode(\"append\").save(deltaLog.dataPath.toString) // Review the cached RDD for the state (snapshot) // Use http://localhost:4040/storage/ assert(deltaLog.snapshot.version == 1) scala> users.show +---+-------+------+-------+ | id| name| city|country| +---+-------+------+-------+ | 2|Bartosz| Paris| France| | 0| Agata|Warsaw| Poland| | 1| Jacek|Warsaw| Poland| | 3| Loic| Paris| France| +---+-------+------+-------+ scala> dt.history.show +-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+ |version| timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend| +-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+ | 1|2020-01-06 17:12:28| null| null| WRITE|[mode -> Append, ...|null| null| null| 0| null| true| | 0|2020-01-06 17:08:02| null| null| CONVERT|[numFiles -> 3, p...|null| null| null| null| null| null| +-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+","title":"DeltaTable, DeltaLog And Snapshots"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/#source-scala","text":"import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) val dataPath = \"/tmp/delta/users\" import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, dataPath) // Review the cached RDD for the state (snapshot) // Use http://localhost:4040/storage/ assert(deltaLog.snapshot.version == 0) // Show the changes (actions) scala> deltaLog.snapshot.state.toDF.show +----+--------------------+------+--------------------+--------+----------+ | txn| add|remove| metaData|protocol|commitInfo| +----+--------------------+------+--------------------+--------+----------+ |null| null| null| null| [1, 2]| null| |null| null| null|[3799b291-dbfa-4f...| null| null| |null|[city=Paris/count...| null| null| null| null| |null|[city=Warsaw/coun...| null| null| null| null| |null|[city=Warsaw/coun...| null| null| null| null| +----+--------------------+------+--------------------+--------+----------+ val dt = DeltaTable.forPath(deltaLog.dataPath.toString) scala> dt.history.show +-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+ |version| timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend| +-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+ | 0|2020-01-06 17:08:02| null| null| CONVERT|[numFiles -> 3, p...|null| null| null| null| null| null| +-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+ // Show the data (for the changes) val users = dt.toDF scala> users.show +---+-------+------+-------+ | id| name| city|country| +---+-------+------+-------+ | 2|Bartosz| Paris| France| | 0| Agata|Warsaw| Poland| | 1| Jacek|Warsaw| Poland| +---+-------+------+-------+ // Add a new user val loic = Seq((3L, \"Loic\", \"Paris\", \"France\")).toDF(\"id\", \"name\", \"city\", \"country\") scala> loic.show +---+----+-----+-------+ | id|name| city|country| +---+----+-----+-------+ | 3|Loic|Paris| France| +---+----+-----+-------+ loic.write.format(\"delta\").mode(\"append\").save(deltaLog.dataPath.toString) // Review the cached RDD for the state (snapshot) // Use http://localhost:4040/storage/ assert(deltaLog.snapshot.version == 1) scala> users.show +---+-------+------+-------+ | id| name| city|country| +---+-------+------+-------+ | 2|Bartosz| Paris| France| | 0| Agata|Warsaw| Poland| | 1| Jacek|Warsaw| Poland| | 3| Loic| Paris| France| +---+-------+------+-------+ scala> dt.history.show +-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+ |version| timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend| +-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+ | 1|2020-01-06 17:12:28| null| null| WRITE|[mode -> Append, ...|null| null| null| 0| null| true| | 0|2020-01-06 17:08:02| null| null| CONVERT|[numFiles -> 3, p...|null| null| null| null| null| null| +-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+","title":"[source, scala]"},{"location":"demo/Observing-Transaction-Retries/","text":"= Demo: Observing Transaction Retries Enable ALL logging level for ROOT:OptimisticTransaction.md#logging[org.apache.spark.sql.delta.OptimisticTransaction] logger. You'll be looking for the following DEBUG message in the logs: Attempting to commit version [version] with 13 actions with Serializable isolation level Start with < > and place the following line breakpoints in OptimisticTransactionImpl : . In OptimisticTransactionImpl.doCommit when a transaction is about to deltaLog.store.write (line 388) . In OptimisticTransactionImpl.doCommit when a transaction is about to checkAndRetry after a FileAlreadyExistsException (line 433) . In OptimisticTransactionImpl.checkAndRetry when a transaction calculates nextAttemptVersion (line 453) In order to interfere with a transaction about to be committed, you will use ROOT:WriteIntoDelta.md[WriteIntoDelta] action (it is simple and does the work). Run the command (copy and paste the ROOT:WriteIntoDelta.md#demo[demo code] to spark-shell using paste mode). You should see the following messages in the logs: scala> writeCmd.run(spark) DeltaLog: DELTA: Updating the Delta table's state OptimisticTransaction: Attempting to commit version 6 with 13 actions with Serializable isolation level That's when you \"commit\" another transaction (to simulate two competing transactional writes). Simply create a delta file for the transaction. The commit version in the message above is 6 so the name of the delta file should be 00000000000000000006.json : $ touch /tmp/delta/t1/_delta_log/00000000000000000006.json F9 in IntelliJ IDEA to resume the WriteIntoDelta command. It should stop at checkAndRetry due to FileAlreadyExistsException . Press F9 twice to resume. You should see the following messages in the logs: OptimisticTransaction: No logical conflicts with deltas [6, 7), retrying. OptimisticTransaction: Attempting to commit version 7 with 13 actions with Serializable isolation level Rinse and repeat. You know the drill already. Happy debugging!","title":"Observing Transaction Retries"},{"location":"demo/Using-Delta-Lake-as-Streaming-Sink-in-Structured-Streaming/","text":"= Demo: Using Delta Lake as Streaming Sink in Structured Streaming /* ./bin/spark-shell \\ --packages io.delta:delta-core_2.12:0.7.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension */ assert(spark.isInstanceOf[org.apache.spark.sql.SparkSession]) assert(spark.version.matches(\"2.4.[2-4]\"), \"Delta Lake supports Spark 2.4.2+\") // Input data \"format\" case class User(id: Long, name: String, city: String) // Any streaming data source would work // Using memory data source // Gives control over the input stream implicit val ctx = spark.sqlContext import org.apache.spark.sql.execution.streaming.MemoryStream val usersIn = MemoryStream[User] val users = usersIn.toDF val deltaLake = \"/tmp/delta-lake\" val checkpointLocation = \"/tmp/delta-checkpointLocation\" val path = s\"$deltaLake/users\" val partitionBy = \"city\" // The streaming query that writes out to Delta Lake val sq = users .writeStream .format(\"delta\") .option(\"checkpointLocation\", checkpointLocation) .option(\"path\", path) .partitionBy(partitionBy) .start // TIP: You could use git to version the path directory // and track the changes of every micro-batch // TIP: Use web UI to monitor execution, e.g. http://localhost:4040 // FIXME: Use DESCRIBE HISTORY every micro-batch val batch1 = Seq( User(0, \"Agata\", \"Warsaw\"), User(1, \"Jacek\", \"Warsaw\")) val offset = usersIn.addData(batch1) sq.processAllAvailable() val history = s\"DESCRIBE HISTORY delta.`$path`\" val clmns = Seq($\"version\", $\"timestamp\", $\"operation\", $\"operationParameters\", $\"isBlindAppend\") val h = sql(history).select(clmns: _*).orderBy($\"version\".asc) scala> h.show(truncate = false) +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |version|timestamp |operation |operationParameters |isBlindAppend| +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |0 |2019-12-06 10:06:20|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 0]|true | +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ val batch2 = Seq( User(2, \"Bartek\", \"Paris\"), User(3, \"Jacek\", \"Paris\")) val offset = usersIn.addData(batch2) sq.processAllAvailable() // You have to execute the history SQL command again // It materializes immediately with whatever data is available at the time val h = sql(history).select(clmns: _*).orderBy($\"version\".asc) scala> h.show(truncate = false) +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |version|timestamp |operation |operationParameters |isBlindAppend| +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |0 |2019-12-06 10:06:20|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 0]|true | |1 |2019-12-06 10:13:27|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 1]|true | +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ val batch3 = Seq( User(4, \"Gorazd\", \"Ljubljana\")) val offset = usersIn.addData(batch3) sq.processAllAvailable() // Let's use DeltaTable API instead import io.delta.tables.DeltaTable val usersDT = DeltaTable.forPath(path) val h = usersDT.history.select(clmns: _*).orderBy($\"version\".asc) scala> h.show(truncate = false) +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |version|timestamp |operation |operationParameters |isBlindAppend| +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |0 |2019-12-06 10:06:20|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 0]|true | |1 |2019-12-06 10:13:27|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 1]|true | |2 |2019-12-06 10:20:56|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 2]|true | +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+","title":"Using Delta Lake as Streaming Sink in Structured Streaming"},{"location":"demo/merge-operation/","text":"Demo: Merge Operation \u00b6 // Create a delta table val path = \"/tmp/delta/demo\" val data = spark.range(5) data.write.format(\"delta\").save(path) // Manage the delta table import io.delta.tables.DeltaTable val target = DeltaTable.forPath(path) scala> :type target io.delta.tables.DeltaTable assert(target.history.count == 1, \"There must be version 0 only\") case class Person(id: Long, name: String) val source = Seq(Person(0, \"Zero\"), Person(1, \"One\")).toDF // Note the difference in schemas scala> target.toDF.printSchema root |-- id: long (nullable = true) scala> source.printSchema root |-- id: long (nullable = false) |-- name: string (nullable = true) // Not only do we update the matching rows // But also update the schema (schema evolution) val mergeBuilder = target.as(\"to\") .merge( source = source.as(\"from\"), condition = $\"to.id\" === $\"from.id\") scala> :type mergeBuilder io.delta.tables.DeltaMergeBuilder scala> mergeBuilder.execute org.apache.spark.sql.AnalysisException: There must be at least one WHEN clause in a MERGE query; at org.apache.spark.sql.catalyst.plans.logical.DeltaMergeInto$.apply(deltaMerge.scala:217) at io.delta.tables.DeltaMergeBuilder.mergePlan(DeltaMergeBuilder.scala:255) at io.delta.tables.DeltaMergeBuilder.$anonfun$execute$1(DeltaMergeBuilder.scala:228) at org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError(AnalysisHelper.scala:60) at org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError$(AnalysisHelper.scala:48) at io.delta.tables.DeltaMergeBuilder.improveUnsupportedOpError(DeltaMergeBuilder.scala:121) at io.delta.tables.DeltaMergeBuilder.execute(DeltaMergeBuilder.scala:225) ... 47 elided val mergeMatchedBuilder = mergeBuilder.whenMatched() scala> :type mergeMatchedBuilder io.delta.tables.DeltaMergeMatchedActionBuilder val mergeBuilderDeleteMatched = mergeMatchedBuilder.delete() scala> :type mergeBuilderDeleteMatched io.delta.tables.DeltaMergeBuilder mergeBuilderDeleteMatched.execute() assert(target.history.count == 2, \"There must be two versions only\")","title":"Merge Operation"},{"location":"demo/merge-operation/#demo-merge-operation","text":"// Create a delta table val path = \"/tmp/delta/demo\" val data = spark.range(5) data.write.format(\"delta\").save(path) // Manage the delta table import io.delta.tables.DeltaTable val target = DeltaTable.forPath(path) scala> :type target io.delta.tables.DeltaTable assert(target.history.count == 1, \"There must be version 0 only\") case class Person(id: Long, name: String) val source = Seq(Person(0, \"Zero\"), Person(1, \"One\")).toDF // Note the difference in schemas scala> target.toDF.printSchema root |-- id: long (nullable = true) scala> source.printSchema root |-- id: long (nullable = false) |-- name: string (nullable = true) // Not only do we update the matching rows // But also update the schema (schema evolution) val mergeBuilder = target.as(\"to\") .merge( source = source.as(\"from\"), condition = $\"to.id\" === $\"from.id\") scala> :type mergeBuilder io.delta.tables.DeltaMergeBuilder scala> mergeBuilder.execute org.apache.spark.sql.AnalysisException: There must be at least one WHEN clause in a MERGE query; at org.apache.spark.sql.catalyst.plans.logical.DeltaMergeInto$.apply(deltaMerge.scala:217) at io.delta.tables.DeltaMergeBuilder.mergePlan(DeltaMergeBuilder.scala:255) at io.delta.tables.DeltaMergeBuilder.$anonfun$execute$1(DeltaMergeBuilder.scala:228) at org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError(AnalysisHelper.scala:60) at org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError$(AnalysisHelper.scala:48) at io.delta.tables.DeltaMergeBuilder.improveUnsupportedOpError(DeltaMergeBuilder.scala:121) at io.delta.tables.DeltaMergeBuilder.execute(DeltaMergeBuilder.scala:225) ... 47 elided val mergeMatchedBuilder = mergeBuilder.whenMatched() scala> :type mergeMatchedBuilder io.delta.tables.DeltaMergeMatchedActionBuilder val mergeBuilderDeleteMatched = mergeMatchedBuilder.delete() scala> :type mergeBuilderDeleteMatched io.delta.tables.DeltaMergeBuilder mergeBuilderDeleteMatched.execute() assert(target.history.count == 2, \"There must be two versions only\")","title":"Demo: Merge Operation"},{"location":"demo/schema-evolution/","text":"= Demo: Schema Evolution :navtitle: Schema Evolution [source,plaintext] \u00b6 /* spark-shell \\ --packages io.delta:delta-core_2.12:0.7.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.databricks.delta.snapshotPartitions=1 */ assert(spark.version.matches(\"2.4.[2-6]\"), \"Delta Lake supports Spark 2.4.2+\") case class PersonV1(id: Long, name: String) import org.apache.spark.sql.Encoders val schemaV1 = Encoders.product[PersonV1].schema scala> schemaV1.printTreeString root |-- id: long (nullable = false) |-- name: string (nullable = true) val dataPath = \"/tmp/delta/people\" // Write data Seq(PersonV1(0, \"Zero\"), PersonV1(1, \"One\")) .toDF .write .format(\"delta\") .mode(\"overwrite\") .option(\"overwriteSchema\", \"true\") .save(dataPath) // Committed delta #0 to file:/tmp/delta/people/_delta_log import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, dataPath) assert(deltaLog.snapshot.version == 0) scala> deltaLog.snapshot.dataSchema.printTreeString root |-- id: long (nullable = true) |-- name: string (nullable = true) import io.delta.tables.DeltaTable val dt = DeltaTable.forPath(dataPath) scala> dt.toDF.show +---+----+ | id|name| +---+----+ | 0|Zero| | 1| One| +---+----+ val main = dt.as(\"main\") case class PersonV2(id: Long, name: String, newField: Boolean) val schemaV2 = Encoders.product[PersonV2].schema scala> schemaV2.printTreeString root |-- id: long (nullable = false) |-- name: string (nullable = true) |-- newField: boolean (nullable = false) val updates = Seq( PersonV2(0, \"ZERO\", newField = true), PersonV2(2, \"TWO\", newField = false)).toDF // Merge two datasets and create a new version // Schema evolution in play main.merge( source = updates.as(\"updates\"), condition = $\"main.id\" === $\"updates.id\") .whenMatched.updateAll .execute val latestPeople = spark .read .format(\"delta\") .load(dataPath) scala> latestPeople.show +---+----+ | id|name| +---+----+ | 0|ZERO| | 1| One| +---+----+","title":"Schema Evolution"},{"location":"demo/schema-evolution/#sourceplaintext","text":"/* spark-shell \\ --packages io.delta:delta-core_2.12:0.7.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.databricks.delta.snapshotPartitions=1 */ assert(spark.version.matches(\"2.4.[2-6]\"), \"Delta Lake supports Spark 2.4.2+\") case class PersonV1(id: Long, name: String) import org.apache.spark.sql.Encoders val schemaV1 = Encoders.product[PersonV1].schema scala> schemaV1.printTreeString root |-- id: long (nullable = false) |-- name: string (nullable = true) val dataPath = \"/tmp/delta/people\" // Write data Seq(PersonV1(0, \"Zero\"), PersonV1(1, \"One\")) .toDF .write .format(\"delta\") .mode(\"overwrite\") .option(\"overwriteSchema\", \"true\") .save(dataPath) // Committed delta #0 to file:/tmp/delta/people/_delta_log import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, dataPath) assert(deltaLog.snapshot.version == 0) scala> deltaLog.snapshot.dataSchema.printTreeString root |-- id: long (nullable = true) |-- name: string (nullable = true) import io.delta.tables.DeltaTable val dt = DeltaTable.forPath(dataPath) scala> dt.toDF.show +---+----+ | id|name| +---+----+ | 0|Zero| | 1| One| +---+----+ val main = dt.as(\"main\") case class PersonV2(id: Long, name: String, newField: Boolean) val schemaV2 = Encoders.product[PersonV2].schema scala> schemaV2.printTreeString root |-- id: long (nullable = false) |-- name: string (nullable = true) |-- newField: boolean (nullable = false) val updates = Seq( PersonV2(0, \"ZERO\", newField = true), PersonV2(2, \"TWO\", newField = false)).toDF // Merge two datasets and create a new version // Schema evolution in play main.merge( source = updates.as(\"updates\"), condition = $\"main.id\" === $\"updates.id\") .whenMatched.updateAll .execute val latestPeople = spark .read .format(\"delta\") .load(dataPath) scala> latestPeople.show +---+----+ | id|name| +---+----+ | 0|ZERO| | 1| One| +---+----+","title":"[source,plaintext]"},{"location":"demo/stream-processing-of-delta-table/","text":"= Demo: Stream Processing of Delta Table [source,plaintext] \u00b6 /* spark-shell \\ --packages io.delta:delta-core_2.12:0.7.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.databricks.delta.snapshotPartitions=1 */ assert(spark.version.matches(\"2.4.[2-4]\"), \"Delta Lake supports Spark 2.4.2+\") import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) val deltaTableDir = \"/tmp/delta/users\" val checkpointLocation = \"/tmp/checkpointLocation\" // Initialize the delta table // - No data // - Schema only case class Person(id: Long, name: String, city: String) spark.emptyDataset[Person].write.format(\"delta\").save(deltaTableDir) import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = spark .readStream .format(\"delta\") .option(\"maxFilesPerTrigger\", 1) // Maximum number of files to scan per micro-batch .load(deltaTableDir) .writeStream .format(\"console\") .option(\"truncate\", false) .option(\"checkpointLocation\", checkpointLocation) .trigger(Trigger.ProcessingTime(10.seconds)) // Useful for debugging .start // The streaming query over delta table // should display the 0 th version as Batch 0 Batch: 0 \u00b6 +---+----+----+ |id |name|city| +---+----+----+ +---+----+----+ // Let's write to the delta table val users = Seq( Person(0, \"Jacek\", \"Warsaw\"), Person(1, \"Agata\", \"Warsaw\"), Person(2, \"Jacek\", \"Paris\"), Person(3, \"Domas\", \"Vilnius\")).toDF // More partitions are more file added // And per maxFilesPerTrigger as 1 file addition per micro-batch // You should see more micro-batches scala> println(users.rdd.getNumPartitions) 4 // Change the default SaveMode.ErrorIfExists to more meaningful save mode import org.apache.spark.sql.SaveMode users .write .format(\"delta\") .mode(SaveMode.Append) // Appending rows .save(deltaTableDir) // Immediately after the above write finishes // New batches should be printed out to the console // Per the number of partitions of users dataset // And per maxFilesPerTrigger as 1 file addition // You should see as many micro-batches as files Batch: 1 \u00b6 +---+-----+------+ |id |name |city | +---+-----+------+ |0 |Jacek|Warsaw| +---+-----+------+ Batch: 2 \u00b6 +---+-----+------+ |id |name |city | +---+-----+------+ |1 |Agata|Warsaw| +---+-----+------+ Batch: 3 \u00b6 +---+-----+-----+ |id |name |city | +---+-----+-----+ |2 |Jacek|Paris| +---+-----+-----+ Batch: 4 \u00b6 +---+-----+-------+ |id |name |city | +---+-----+-------+ |3 |Domas|Vilnius| +---+-----+-------+","title":"Stream Processing of Delta Table"},{"location":"demo/stream-processing-of-delta-table/#sourceplaintext","text":"/* spark-shell \\ --packages io.delta:delta-core_2.12:0.7.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.databricks.delta.snapshotPartitions=1 */ assert(spark.version.matches(\"2.4.[2-4]\"), \"Delta Lake supports Spark 2.4.2+\") import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) val deltaTableDir = \"/tmp/delta/users\" val checkpointLocation = \"/tmp/checkpointLocation\" // Initialize the delta table // - No data // - Schema only case class Person(id: Long, name: String, city: String) spark.emptyDataset[Person].write.format(\"delta\").save(deltaTableDir) import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = spark .readStream .format(\"delta\") .option(\"maxFilesPerTrigger\", 1) // Maximum number of files to scan per micro-batch .load(deltaTableDir) .writeStream .format(\"console\") .option(\"truncate\", false) .option(\"checkpointLocation\", checkpointLocation) .trigger(Trigger.ProcessingTime(10.seconds)) // Useful for debugging .start // The streaming query over delta table // should display the 0 th version as Batch 0","title":"[source,plaintext]"},{"location":"demo/stream-processing-of-delta-table/#batch-0","text":"+---+----+----+ |id |name|city| +---+----+----+ +---+----+----+ // Let's write to the delta table val users = Seq( Person(0, \"Jacek\", \"Warsaw\"), Person(1, \"Agata\", \"Warsaw\"), Person(2, \"Jacek\", \"Paris\"), Person(3, \"Domas\", \"Vilnius\")).toDF // More partitions are more file added // And per maxFilesPerTrigger as 1 file addition per micro-batch // You should see more micro-batches scala> println(users.rdd.getNumPartitions) 4 // Change the default SaveMode.ErrorIfExists to more meaningful save mode import org.apache.spark.sql.SaveMode users .write .format(\"delta\") .mode(SaveMode.Append) // Appending rows .save(deltaTableDir) // Immediately after the above write finishes // New batches should be printed out to the console // Per the number of partitions of users dataset // And per maxFilesPerTrigger as 1 file addition // You should see as many micro-batches as files","title":"Batch: 0"},{"location":"demo/stream-processing-of-delta-table/#batch-1","text":"+---+-----+------+ |id |name |city | +---+-----+------+ |0 |Jacek|Warsaw| +---+-----+------+","title":"Batch: 1"},{"location":"demo/stream-processing-of-delta-table/#batch-2","text":"+---+-----+------+ |id |name |city | +---+-----+------+ |1 |Agata|Warsaw| +---+-----+------+","title":"Batch: 2"},{"location":"demo/stream-processing-of-delta-table/#batch-3","text":"+---+-----+-----+ |id |name |city | +---+-----+-----+ |2 |Jacek|Paris| +---+-----+-----+","title":"Batch: 3"},{"location":"demo/stream-processing-of-delta-table/#batch-4","text":"+---+-----+-------+ |id |name |city | +---+-----+-------+ |3 |Domas|Vilnius| +---+-----+-------+","title":"Batch: 4"},{"location":"demo/using-user-metadata-to-distinguish-commits-streaming-queries/","text":"= Demo: Using User Metadata to Distinguish Commits of Streaming Queries :navtitle: Using User Metadata to Distinguish Commits of Streaming Queries The demo shows how to differentiate commits of two streaming queries using ROOT:DeltaOptions.md#userMetadata[userMetadata] option. [source,plaintext] \u00b6 // FIXME \u00b6","title":"Using User Metadata to Distinguish Commits of Streaming Queries"},{"location":"demo/using-user-metadata-to-distinguish-commits-streaming-queries/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"demo/using-user-metadata-to-distinguish-commits-streaming-queries/#fixme","text":"","title":"// FIXME"},{"location":"sql/","text":"Delta SQL \u00b6 Delta Lake registers custom SQL statements (using DeltaSparkSessionExtension to inject DeltaSqlParser with DeltaSqlAstBuilder ). The SQL statements support table of the format delta.`path` (with backticks), e.g. delta.`/tmp/delta/t1` while path is between single quotes, e.g. '/tmp/delta/t1' . The SQL statements can also refer to a table registered in a metastore. Note SQL grammar is described using ANTLR in DeltaSqlBase.g4 . CONVERT TO DELTA \u00b6 CONVERT TO DELTA table (PARTITIONED BY '(' colTypeList ')')? Runs a ConvertToDeltaCommand DESCRIBE DETAIL \u00b6 (DESC | DESCRIBE) DETAIL (path | table) Runs a DescribeDeltaDetailCommand DESCRIBE HISTORY \u00b6 (DESC | DESCRIBE) HISTORY (path | table) (LIMIT limit)? Runs a DescribeDeltaHistoryCommand GENERATE \u00b6 GENERATE modeName FOR TABLE table Runs a DeltaGenerateCommand VACUUM \u00b6 VACUUM (path | table) (RETAIN number HOURS)? (DRY RUN)? Runs a VacuumTableCommand","title":"Delta SQL"},{"location":"sql/#delta-sql","text":"Delta Lake registers custom SQL statements (using DeltaSparkSessionExtension to inject DeltaSqlParser with DeltaSqlAstBuilder ). The SQL statements support table of the format delta.`path` (with backticks), e.g. delta.`/tmp/delta/t1` while path is between single quotes, e.g. '/tmp/delta/t1' . The SQL statements can also refer to a table registered in a metastore. Note SQL grammar is described using ANTLR in DeltaSqlBase.g4 .","title":"Delta SQL"},{"location":"sql/#convert-to-delta","text":"CONVERT TO DELTA table (PARTITIONED BY '(' colTypeList ')')? Runs a ConvertToDeltaCommand","title":" CONVERT TO DELTA"},{"location":"sql/#describe-detail","text":"(DESC | DESCRIBE) DETAIL (path | table) Runs a DescribeDeltaDetailCommand","title":" DESCRIBE DETAIL"},{"location":"sql/#describe-history","text":"(DESC | DESCRIBE) HISTORY (path | table) (LIMIT limit)? Runs a DescribeDeltaHistoryCommand","title":" DESCRIBE HISTORY"},{"location":"sql/#generate","text":"GENERATE modeName FOR TABLE table Runs a DeltaGenerateCommand","title":" GENERATE"},{"location":"sql/#vacuum","text":"VACUUM (path | table) (RETAIN number HOURS)? (DRY RUN)? Runs a VacuumTableCommand","title":" VACUUM"},{"location":"sql/DeltaSqlAstBuilder/","text":"DeltaSqlAstBuilder \u00b6 DeltaSqlAstBuilder is a command builder for the Delta SQL statements (described in DeltaSqlBase.g4 ANTLR grammar). DeltaSqlParser is used by DeltaSqlParser . SQL Statement Logical Command CONVERT TO DELTA ConvertToDeltaCommand DESCRIBE DETAIL DescribeDeltaDetailCommand DESCRIBE HISTORY DescribeDeltaHistoryCommand GENERATE DeltaGenerateCommand VACUUM VacuumTableCommand","title":"DeltaSqlAstBuilder"},{"location":"sql/DeltaSqlAstBuilder/#deltasqlastbuilder","text":"DeltaSqlAstBuilder is a command builder for the Delta SQL statements (described in DeltaSqlBase.g4 ANTLR grammar). DeltaSqlParser is used by DeltaSqlParser . SQL Statement Logical Command CONVERT TO DELTA ConvertToDeltaCommand DESCRIBE DETAIL DescribeDeltaDetailCommand DESCRIBE HISTORY DescribeDeltaHistoryCommand GENERATE DeltaGenerateCommand VACUUM VacuumTableCommand","title":"DeltaSqlAstBuilder"},{"location":"sql/DeltaSqlParser/","text":"DeltaSqlParser \u00b6 DeltaSqlParser is a SQL parser (Spark SQL's ParserInterface ) for Delta SQL . DeltaSqlParser is registered in a Spark SQL application using DeltaSparkSessionExtension . Creating Instance \u00b6 DeltaSqlParser takes the following to be created: ParserInterface (to fall back to for unsupported SQL) DeltaSqlParser is created when DeltaSparkSessionExtension is requested to register Delta SQL support . DeltaSqlAstBuilder \u00b6 DeltaSqlParser uses DeltaSqlAstBuilder to convert SQL statements to their runtime representation (as a LogicalPlan ). In case an AST could not be converted to a LogicalPlan , DeltaSqlAstBuilder requests the delegate ParserInterface to parse it.","title":"DeltaSqlParser"},{"location":"sql/DeltaSqlParser/#deltasqlparser","text":"DeltaSqlParser is a SQL parser (Spark SQL's ParserInterface ) for Delta SQL . DeltaSqlParser is registered in a Spark SQL application using DeltaSparkSessionExtension .","title":"DeltaSqlParser"},{"location":"sql/DeltaSqlParser/#creating-instance","text":"DeltaSqlParser takes the following to be created: ParserInterface (to fall back to for unsupported SQL) DeltaSqlParser is created when DeltaSparkSessionExtension is requested to register Delta SQL support .","title":"Creating Instance"},{"location":"sql/DeltaSqlParser/#deltasqlastbuilder","text":"DeltaSqlParser uses DeltaSqlAstBuilder to convert SQL statements to their runtime representation (as a LogicalPlan ). In case an AST could not be converted to a LogicalPlan , DeltaSqlAstBuilder requests the delegate ParserInterface to parse it.","title":" DeltaSqlAstBuilder"}]}